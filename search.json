[
  {
    "objectID": "sessions/2_nf_dev_intro.html",
    "href": "sessions/2_nf_dev_intro.html",
    "title": "Developing bioinformatics workflows with Nextflow",
    "section": "",
    "text": "This workshop is designed to provide participants with a fundamental understanding of developing bioinformatics pipelines using Nextflow. This workshop aims to provide participants with the necessary skills required to create a Nextflow pipeline from scratch or from the nf-core template.\n\nCourse Presenters\n\nSong Li, Bioinformatics Core Facility, Peter Mac\nRichard Lupat, Bioinformatics Core Facility, Peter Mac\n\n\n\nCourse Helpers\nTBC\n\n\nPrerequisites\n\nExperience with command line interface and cluster/slurm\nFamiliarity with the basic concept of workflows\nAttendance in the ‘Introduction to Nextflow and Running nf-core Workflows’ workshop, or an understanding of the Nextflow concepts outlined in the workshop material\n\n\n\nLearning Objectives:\nBy the end of this workshop, participants should be able to:\n\nDevelop a basic Nextflow workflow consisting of processes that use multiple scripting languages\nRead data of different types into a Nextflow workflow\nOutput Nextflow process results to a predefined directory\nRe-use and import processes, modules, and sub-workflows into a Nextflow workflow\nGain an understanding of Nextflow channel operators\nDevelop a basic Nextflow workflow with nf-core templates\nTroubleshoot known errors in workflow development\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP via Slack.\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\nTime\n\n\n\n\nSetup\nFollow these instructions to install VS Code and setup your workspace\n&lt; 10.00\n\n\nSession kick off\nSession kick off: Discuss learning outcomes and finalising workspace setup\n10.00 - 10.10\n\n\nWorking with nf-core Templates\nIntroduction to nf-core templates and its structure\n10.10 - 10.50\n\n\nBreak\nBreak\n10:50 - 11:00\n\n\nCreating workflow with nf-core Templates - NEED NEW NOTES\nCreating workflow with nf-core templates\n11.00 - 12.15\n\n\nLunch Break\nBreak\n12:15 - 13:00\n\n\nmetadata - NEED NEW NOTES\nWorking with Nextflow-Schema & Metadata\n13.00 - 13.30\n\n\nBest practise and Q&A\nBest practise, tips & tricks for running nextflow pipelines\n13.30 - 14:00\n\n\n\n\n\nCredits and acknowledgement\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Intro to Nextflow Workshop",
    "section": "",
    "text": "These workshops are designed to provide participants with a foundational understanding of Nextflow and nf-core pipelines. Participants are expected to have prior experience with the command-line interface and working with cluster systems like Slurm. The primary goal of the workshop is to equip researchers with the skills needed to use nextflow and nf-core pipelines for their research data.\n\nCourse Developers & Maintainers\nCollaborative workshop between Peter Mac, Melbourne Bioinformatics, and WEHI\n\nSong Li, Bioinformatics Core Facility, Peter Mac\nRichard Lupat, Bioinformatics Core Facility, Peter Mac\nMiriam Yeung, Cancer Genomics Translational Research Centre, Peter Mac\nSanduni Rajapaksa, Research Computing Facility, Peter Mac\n\n\n\nWorkshop Sessions\n\n\n\nLesson\nOverview\nDate\n\n\n\n\nIntroduction to Nextflow and running nf-core workflows\nIntroduction to Nextflow: Introduce nextflow’s core features and concepts + nf-core\n28th May 2025\n\n\nWorking with nf-core Templates\nIntroduction to developing bioinformatics workflow with Nextflow (Part2)\n29th May 2025\n\n\n\n\n\nCredits and acknowledgement\nThis workshop is adapted from Customising Nf-Core Workshop materials from Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/2.3_tips_and_tricks.html",
    "href": "workshops/2.3_tips_and_tricks.html",
    "title": "Best practise, tips and tricks",
    "section": "",
    "text": "2.3.1. Running Nextflow Pipelines on a HPC \nNextflow, by default, spawns parallel task executions wherever it is running. You can use Nextflow’s executors feature to run these tasks using an HPC job schedulers such as SLURM and PBS Pro. Use a custom configuration file to send all processes to the job scheduler as separate jobs and define essential resource requests like cpus, time, memory, and queue inside a process {} scope.\n\nRun all workflow tasks as separate jobs on HPC\nIn this custom configuration file we have sent all tasks that a workflow is running to a PBS Pro job scheduler and specified jobs to be run on the normal queue, each running for a max time of 3 hours with 1 cpu and 4 Gb of memory:\nprocess {\n  executor = 'slurm'\n  queue = 'prod_short'\n  cpus = 1\n  time = '2h'\n  memory = '4.GB'\n}\n\n\nRun processes with different resource profiles as HPC jobs\nAdjusting the custom configuration file above, we can use the withName {} process selector to specify process-specific resource requirements:\nprocess {\n  executor = 'slurm'\n    \n  withName processONE {\n    queue = 'prod_short'\n    cpus = 1\n    time = '2h'\n    memory = '4.GB'\n  }\n\n  withName processTWO {\n    queue = 'prod_med'\n    cpus = 2\n    time = '10h'\n    memory = '50.GB'\n  }\n}\n\n\nSpecify infrastructure-specific directives for your jobs\nAdjusting the custom configuration file above, we can define any native configuration options using the clusterOptions directive. We can use this to specify non-standard resources. Below we have specified which HPC project code to bill for all process jobs:\nYou can also setup a config to tailor specific to Peter Mac’s HPC partitions setup.\nexecutor {\n    queueSize         = 100\n    queueStatInterval = '1 min'\n    pollInterval      = '1 min'\n    submitRateLimit   = '20 min'\n}\n\nprocess {\n    executor = 'slurm'\n    cache    = 'lenient'\n    beforeScript = 'module load singularity'\n    stageInMode = 'symlink'\n    queue = { task.time &lt; 2.h ? 'prod_short' : task.time &lt; 24.h ? 'prod_med' : 'prod' } \n}\n\n\n\n\n\n\nChallenge\n\n\n\nRun the previous nf-core/rnaseq workflow using the process and executor scope above (in a config file), and send each task to slurm.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a nextflow.config file\nprocess.executor = 'slurm'\nRun the nfcore/rna-seq workflow again\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    -params-file workshop-params.yaml\n    -profile singularity \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \nDid you get the following error?\nsbatch: error: Batch job submission failed: Access/permission denied\nTry running the same workflow on login-node and observe the difference\n&gt;&gt;&gt; squeue -u rlupat -i 5\n\n          17429286      prod nf-NFCOR   rlupat  R       0:03      1 papr-res-compute01\n\n\n\n\n\n\n\n2.3.3. Clean your work directory\nYour work directory can get very big very quickly (especially if you are using full sized datasets). It is good practise to clean your work directory regularly. Rather than removing the work folder with all of it’s contents, the Nextflow clean function allows you to selectively remove data associated with specific runs.\nnextflow clean -help\nThe -after, -before, and -but options are all very useful to select specific runs to clean. The -dry-run option is also very useful to see which files will be removed if you were to -force the clean command.\n\n\n\n\n\n\nChallenge\n\n\n\nYou Nextflow to clean your work work directory of staged files but keep your execution logs.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the Nextflow clean command with the -k and -f options:\nnextflow clean -k -f\n\n\n\n\n\n\n2.3.4. Change default Nextflow cache strategy\nWorkflow execution is sometimes not resumed as expected. The default behaviour of Nextflow cache keys is to index the input files meta-data information. Reducing the cache stringency to lenient means the files cache keys are based only on filesize and path, and can help to avoid unexpectedly re-running certain processes when -resume is in use.\nTo apply lenient cache strategy to all of your runs, you could add to a custom configuration file:\nprocess {\n    cache = 'lenient'\n}\nYou can specify different cache stategies for different processes by using withName or withLabel. You can specify a particular cache strategy be applied to certain profiles within your institutional config, or to apply to all profiles described within that config by placing the above process code block outside the profiles scope.\n\n\n2.3.5. Access private GitHub repositories\nTo interact with private repositories on GitHub, you can provide Nextflow with access to GitHub by specifying your GitHub user name and a Personal Access Token in the scm configuration file inside your specified .nextflow/ directory:\nproviders {\n\n  github {\n    user = 'rlupat'\n    password = 'my-personal-access-token'\n  }\n\n}\n\n\n2.3.7. Additional resources \nHere are some useful resources to help you get started with running nf-core pipelines and developing Nextflow pipelines:\n\nNextflow tutorials\nnf-core pipeline tutorials\nNextflow patterns\nHPC tips and tricks\nNextflow coding best practice recommendations\nThe Nextflow blog\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/7.1_metadata_propagation.html",
    "href": "workshops/7.1_metadata_propagation.html",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain and understanding of how to manipulate and proprogate metadata"
  },
  {
    "objectID": "workshops/7.1_metadata_propagation.html#environment-setup",
    "href": "workshops/7.1_metadata_propagation.html#environment-setup",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\nThe training data can be cloned from:\ngit clone https://github.com/nextflow-io/training.git"
  },
  {
    "objectID": "workshops/7.1_metadata_propagation.html#metadata-parsing",
    "href": "workshops/7.1_metadata_propagation.html#metadata-parsing",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "7.1 Metadata Parsing",
    "text": "7.1 Metadata Parsing\nWe have covered a few different methods of metadata parsing.\n\n7.1.1 First Pass: .fromFilePairs\nA first pass attempt at pulling these files into Nextflow might use the fromFilePairs method:\nworkflow {\n    Channel.fromFilePairs(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz\")\n    .view\n}\nNextflow will pull out the first part of the fastq filename and returned us a channel of tuple elements where the first element is the filename-derived ID and the second element is a list of two fastq files.\nThe id is stored as a simple string. We’d like to move to using a map of key-value pairs because we have more than one piece of metadata to track. In this example, we have sample, replicate, tumor/normal, and treatment. We could add extra elements to the tuple, but this changes the ‘cardinality’ of the elements in the channel and adding extra elements would require updating all downstream processes. A map is a single object and is passed through Nextflow channels as one value, so adding extra metadata fields will not require us to change the cardinality of the downstream processes.\nThere are a couple of different ways we can pull out the metadata\nWe can use the tokenize method to split our id. To sanity-check, I just pipe the result directly into the view operator.\nworkflow {\n    Channel.fromFilePairs(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz\")\n    .map { id, reads -&gt;\n        tokens = id.tokenize(\"_\")\n    }\n    .view\n}\nIf we are confident about the stability of the naming scheme, we can destructure the list returned by tokenize and assign them to variables directly:\nmap { id, reads -&gt;\n    (sample, replicate, type) = id.tokenize(\"_\")\n    meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n\n\n\n\n\n\nNote\n\n\n\nMake sure that you're using a tuple with parentheses e.g. (one, two) rather than a List e.g. [one, two]\n\n\nIf we move back to the previous method, but decided that the ‘rep’ prefix on the replicate should be removed, we can use regular expressions to simply “subtract” pieces of a string. Here we remove a ‘rep’ prefix from the replicate variable if the prefix is present:\nmap { id, reads -&gt;\n    (sample, replicate, type) = id.tokenize(\"_\")\n    replicate -= ~/^rep/\n    meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\nBy setting up our the “meta”, in our tuple with the format above, allows us to access the values in “sample” throughout our modules/configs as ${meta.sample}."
  },
  {
    "objectID": "workshops/7.1_metadata_propagation.html#second-parse-.splitcsv",
    "href": "workshops/7.1_metadata_propagation.html#second-parse-.splitcsv",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "Second Parse: .splitCsv",
    "text": "Second Parse: .splitCsv\nWe have briefly touched on .splitCsv in the first week.\nAs a quick overview\nAssuming we have the samplesheet\nsample_name,fastq1,fastq2\ngut_sample,/.../training/nf-training/data/ggal/gut_1.fq,/.../training/nf-training/data/ggal/gut_2.fq\nliver_sample,/.../training/nf-training/data/ggal/liver_1.fq,/.../training/nf-training/data/ggal/liver_2.fq\nlung_sample,/.../training/nf-training/data/ggal/lung_1.fq,/.../training/nf-training/data/ggal/lung_2.fq\nWe can set up a workflow to read in these files as:\nparams.reads = \"/.../rnaseq_samplesheet.csv\"\n\nreads_ch = Channel.fromPath(params.reads)\nreads_ch.view()\nreads_ch = reads_ch.splitCsv(header:true)\nreads_ch.view()\n\n\n\n\n\n\nChallenge\n\n\n\nUsing .splitCsv and .map read in the samplesheet below: /home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/samplesheet.csv\nSet the meta to contain the following keys from the header id, repeat and type\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nparams.input = \"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/samplesheet.csv\"\n\nch_sheet = Channel.fromPath(params.input)\n\nch_sheet.splitCsv(header:true)\n    .map {\n        it -&gt;\n            [[it.id, it.repeat, it.type], it.fastq_1, it.fastq_2]\n    }.view()"
  },
  {
    "objectID": "workshops/7.1_metadata_propagation.html#manipulating-metadata-and-channels",
    "href": "workshops/7.1_metadata_propagation.html#manipulating-metadata-and-channels",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "7.2 Manipulating Metadata and Channels",
    "text": "7.2 Manipulating Metadata and Channels\nThere are a number of use cases where we will be interested in manipulating our metadata and channels.\nHere we will look at 2 use cases.\n\n7.2.1 Matching input channels\nAs we have seen in examples/challenges in the operators section, it is important to ensure that the format of the channels that you provide as inputs match the process definition.\nparams.reads = \"/home/Shared/For_NF_Workshop/training/nf-training/data/ggal/*_{1,2}.fq\"\n\nprocess printNumLines {\n    input:\n    path(reads)\n\n    output:\n    path(\"*txt\")\n\n    script:\n    \"\"\"\n    wc -l ${reads}\n    \"\"\"\n}\n\nworkflow {\n    ch_input = Channel.fromFilePairs(\"$params.reads\")\n    printNumLines( ch_input )\n}\nAs if the format does not match you will see and error similar to below:\n[myeung@papr-res-compute204 lesson7.1test]$ nextflow run test.nf \nN E X T F L O W  ~  version 23.04.1\nLaunching `test.nf` [agitated_faggin] DSL2 - revision: c210080493\n[-        ] process &gt; printNumLines -\nor if using nf-core template\nERROR ~ Error executing process &gt; 'PMCCCGTRC_UMIHYBCAP:UMIHYBCAP:PREPARE_GENOME:BEDTOOLS_SLOP'\n\nCaused by:\n  Not a valid path value type: java.util.LinkedHashMap ([id:genome_size])\n\n\nTip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`\n\n -- Check '.nextflow.log' file for details\nWhen encountering these errors there are two methods to correct this:\n\nChange the input definition in the process\nUse variations of the channel operators to correct the format of your channel\n\nThere are cases where changing the input definition is impractical (i.e. when using nf-core modules/subworkflows).\nLet’s take a look at some select modules.\nBEDTOOLS_SLOP\nBEDTOOLS_INTERSECT\n\n\n\n\n\n\nChallenge\n\n\n\nAssuming that you have the following inputs\nch_target = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals.bed\")\nch_bait = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals2.bed\").map { fn -&gt; [ [id: fn.baseName ], fn ] }\nch_sizes = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/genome.sizes\")\nWrite a mini workflow that:\n\nTakes the ch_target bedfile and extends the bed by 20bp on both sides using BEDTOOLS_SLOP (You can use the config definition below as a helper, or write your own as an additional challenge)\nTake the output from BEDTOOLS_SLOP and input this output with the ch_baits to BEDTOOLS_INTERSECT\n\nHINT: The modules can be imported from this location: /home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools\nHINT: You will need need the following operators to achieve this .map and .combine\n\n\n\n\n\n\n\n\nConfig\n\n\n\n\n\n\nprocess {\n    withName: 'BEDTOOLS_SLOP' {\n        ext.args = \"-b 20\"\n        ext.prefix = \"extended.bed\"\n    }\n\n    withName: 'BEDTOOLS_INTERSECT' {\n        ext.prefix = \"intersect.bed\"\n    }\n}\n:::\n\n:::{.callout-caution collapse=\"true\"}\n## **Solution**\n```default\ninclude { BEDTOOLS_SLOP } from '/home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools/slop/main'\ninclude { BEDTOOLS_INTERSECT } from '/home/Shared/For_NF_Workshop/training/pmcc-test/modules/nf-core/bedtools/intersect/main'\n\n\nch_target = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals.bed\")\nch_bait = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/intervals2.bed\").map { fn -&gt; [ [id: fn.baseName ], fn ] }\nch_sizes = Channel.fromPath(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/grouping/data/genome.sizes\")\n\nworkflow {\n    BEDTOOLS_SLOP ( ch_target.map{ fn -&gt; [ [id:fn.baseName], fn ]}, ch_sizes)\n\n    target_bait_bed = BEDTOOLS_SLOP.out.bed.combine( ch_bait )\n    BEDTOOLS_INTERSECT( target_bait_bed, ch_sizes.map{ fn -&gt; [ [id: fn.baseName], fn]} )\n}\nnextflow run nfcoretest.nf -profile singularity -c test2.config --outdir nfcoretest"
  },
  {
    "objectID": "workshops/7.1_metadata_propagation.html#grouping-with-metadata",
    "href": "workshops/7.1_metadata_propagation.html#grouping-with-metadata",
    "title": "Nextflow Development - Metadata Proprogation",
    "section": "7.3 Grouping with Metadata",
    "text": "7.3 Grouping with Metadata\nEarlier we introduced the function groupTuple\n\nch_reads = Channel.fromFilePairs(\"/home/Shared/For_NF_Workshop/training/nf-training-advanced/metadata/data/reads/*/*_R{1,2}.fastq.gz\")\n    .map { id, reads -&gt;\n        (sample, replicate, type) = id.tokenize(\"_\")\n        replicate -= ~/^rep/\n        meta = [sample:sample, replicate:replicate, type:type]\n    [meta, reads]\n}\n\n## Assume that we want to drop replicate from the meta and combine fastqs\n\nch_reads.map {\n    meta, reads -&gt; \n        [ meta - meta.subMap('replicate') + [data_type: 'fastq'], reads ]\n    }\n    .groupTuple().view()"
  },
  {
    "objectID": "workshops/1.2_intro_nf_core.html",
    "href": "workshops/1.2_intro_nf_core.html",
    "title": "Introduction to nf-core",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the core features of nf-core.\nLearn the terminology used by nf-core.\nUse Nextflow to pull and run the nf-core/testpipeline workflow\n\n\n\nIntroduction to nf-core: Introduce nf-core features and concepts, structures, tools, and example nf-core pipelines\n\n1.2.1. What is nf-core?\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.\nThe community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276–278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won’t be left in the dark.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.\n\nCloud-ready\n\nnf-core workflows are tested on AWS\n\n\n\n\n1.2.2. Executing an nf-core workflow\nThe nf-core website has a full list of workflows and asssociated documentation tno be explored.\nEach workflow has a dedicated page that includes expansive documentation that is split into 7 sections:\n\nIntroduction\n\nAn introduction and overview of the workflow\n\nResults\n\nExample output files generated from the full test dataset\n\nUsage docs\n\nDescriptions of how to execute the workflow\n\nParameters\n\nGrouped workflow parameters with descriptions\n\nOutput docs\n\nDescriptions and examples of the expected output files\n\nReleases & Statistics\n\nWorkflow version history and statistics\n\n\nAs nf-core is a community development project the code for a pipeline can be changed at any time. To ensure that you have locked in a specific version of a pipeline you can use Nextflow’s built-in functionality to pull a workflow. The Nextflow pull command can download and cache workflows from GitHub repositories:\nnextflow pull nf-core/&lt;pipeline&gt;\nNextflow run will also automatically pull the workflow if it was not already available locally:\nnextflow run nf-core/&lt;pipeline&gt;\nNextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the -revision or -r flag.\nFor this section of the workshop we will be using the nf-core/testpipeline as an example.\nAs we will be running some bioinformatics tools, we will need to make sure of the following:\n\nWe are not running on login node\nsingularity module is loaded (module load singularity/3.7.3)\n\n\n\n\n\n\n\nSetup an interactive session\n\n\n\nsrun --pty -p prod_short --mem 20GB --cpus-per-task 2 -t 0-2:00 /bin/bash\n\nEnsure the required modules are loaded\nmodule list\nCurrently Loaded Modulefiles:\n  1) java/jdk-17.0.6      2) nextflow/23.04.1     3) squashfs-tools/4.5   4) singularity/3.7.3\n\n\n\nWe will also create a separate output directory for this section.\ncd /scratch/users/&lt;your-username&gt;/nfWorkshop; mkdir ./lesson1.2 && cd $_\nThe base command we will be using for this section is:\nnextflow run nf-core/testpipeline -profile test,singularity --outdir my_results\n\n\n1.2.3. Workflow structure\nnf-core workflows start from a common template and follow the same structure. Although you won’t need to edit code in the workflow project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution.\nLet’s take a look at the code for the nf-core/rnaseq pipeline.\nNextflow DSL2 workflows are built up of subworkflows and modules that are stored as separate .nf files.\nMost nf-core workflows consist of a single workflow file (there are a few exceptions). This is the main &lt;workflow&gt;.nf file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules.\nA subworkflow is a groups of modules that are used in combination with each other and have a common purpose. Subworkflows improve workflow readability and help with the reuse of modules within a workflow. The nf-core community also shares subworkflows in the nf-core subworkflows GitHub repository. Local subworkflows are workflow specific that are not shared in the nf-core subworkflows repository.\nLet’s take a look at the BAM_STATS_SAMTOOLS subworkflow.\nThis subworkflow is comprised of the following modules: - SAMTOOLS_STATS - SAMTOOLS_IDXSTATS, and - SAMTOOLS_FLAGSTAT\nA module is a wrapper for a process, most modules will execute a single tool and contain the following definitions: - inputs - outputs, and - script block.\nLike subworkflows, modules can also be shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility. Local modules are workflow specific that are not shared in the nf-core modules repository.\n\n\n1.2.4. Viewing parameters\nEvery nf-core workflow has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will have additional text to help you understand when and how a parameter should be used.\n\nParameters and their descriptions can also be viewed in the command line using the run command with the --help parameter:\nnextflow run nf-core/&lt;workflow&gt; --help\n\n\n\n\n\n\nChallenge\n\n\n\nView the parameters for the nf-core/testpipeline workflow using the command line:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe nf-core/testpipeline workflow parameters can be printed using the run command and the --help option:\nnextflow run nf-core/testpipeline --help\n\n\n\n\n\n1.2.5. Parameters in the command line\nParameters can be customized using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash (--):\nnextflow run nf-core/&lt;workflow&gt; --&lt;parameter&gt;\n\n\n\n\n\n\nTip\n\n\n\nNextflow options are prefixed with a single dash (-) and workflow parameters are prefixed with a double dash (--).\n\n\nDepending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag:\nnextflow run nf-core/&lt;workflow&gt; --&lt;parameter&gt; string\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the nf-core/testpipeline workflow the name of your favorite animal using the multiqc_title parameter using a command line flag:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAdd the --multiqc_title flag to your command and execute it. Use the -resume option to save time:\nnextflow run nf-core/testpipeline -profile test,singularity --multiqc_title koala --outdir my_results -resume\n\n\n\nIn this example, you can check your parameter has been applied by listing the files created in the results folder (my_results):\nls my_results/multiqc/\n\n\n1.2.6. Configuration files\nConfiguration files are .config files that can contain various workflow properties. Custom paths passed in the command-line using the -c option:\nnextflow run nf-core/&lt;workflow&gt; -profile test,docker -c &lt;path/to/custom.config&gt;\nMultiple custom .config files can be included at execution by separating them with a comma (,).\nCustom configuration files follow the same structure as the configuration file included in the workflow directory. Configuration properties are organized into scopes by grouping the properties in the same scope using the curly brackets notation. For example:\nalpha {\n     x = 1\n     y = 'string value..'\n}\nScopes allow you to quickly configure settings required to deploy a workflow on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a workflow on a HPC cluster. Similarly, the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here.\n\n\n\n\n\n\nChallenge\n\n\n\nGive the MultiQC report for the nf-core/testpipeline workflow the name of your favorite color using the multiqc_title parameter in a custom my_custom.config file:\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCreate a custom my_custom.config file that contains your favourite colour, e.g., blue:\nparams {\n    multiqc_title = \"blue\"\n}\nInclude the custom .config file in your execution command with the -c option:\nnextflow run nf-core/testpipeline --outdir my_results -profile test,singularity -resume -c my_custom.config\nCheck that it has been applied:\nls my_results/multiqc/\nWhy did this fail?\nYou can not use the params scope in custom configuration files. Parameters can only be configured using the -params-file option and the command line. While parameter is listed as a parameter on the STDOUT, it was not applied to the executed command.\nWe will revisit this at the end of the module\n\n\n\n\n\n1.2.7 Parameter files\nParameter files are used to define params options for a pipeline, generally written in the YAML format. They are added to a pipeline with the flag --params-file\nExample YAML:\n\"&lt;parameter1_name&gt;\": 1,\n\"&lt;parameter2_name&gt;\": \"&lt;string&gt;\",\n\"&lt;parameter3_name&gt;\": true\n\n\n\n\n\n\nChallenge\n\n\n\nBased on the failed application of the parameter multiqc_title create a my_params.yml setting multiqc_title to your favourite colour. Then re-run the pipeline with the your my_params.yml\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSet up my_params.yml\nmultiqc_title: \"black\"\nnextflow run nf-core/testpipeline -profile test,singularity --params-file my_params.yml --outdir Lesson1_2\n\n\n\n\n\n1.2.8. Default configuration files\nAll parameters will have a default setting that is defined using the nextflow.config file in the workflow project directory. By default, most parameters are set to null or false and are only activated by a profile or configuration file.\nThere are also several includeConfig statements in the nextflow.config file that are used to load additional .config files from the conf/ folder. Each additional .config file contains categorized configuration information for your workflow execution, some of which can be optionally included:\n\nbase.config\n\nIncluded by the workflow by default.\nGenerous resource allocations using labels.\nDoes not specify any method for software management and expects software to be available (or specified elsewhere).\n\nigenomes.config\n\nIncluded by the workflow by default.\nDefault configuration to access reference files stored on AWS iGenomes.\n\nmodules.config\n\nIncluded by the workflow by default.\nModule-specific configuration options (both mandatory and optional).\n\n\nNotably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a workflow by using the -profile command option:\nnextflow run nf-core/&lt;workflow&gt; -profile &lt;profile&gt;\nProfiles used by nf-core workflows include:\n\nSoftware management profiles\n\nProfiles for the management of software using software management tools, e.g., docker, singularity, and conda.\n\nTest profiles\n\nProfiles to execute the workflow with a standardized set of test data and parameters, e.g., test and test_full.\n\n\nMultiple profiles can be specified in a comma-separated (,) list when you execute your command. The order of profiles is important as they will be read from left to right:\nnextflow run nf-core/&lt;workflow&gt; -profile test,singularity\nnf-core workflows are required to define software containers and conda environments that can be activated using profiles.\n\n\n\n\n\n\nTip\n\n\n\nIf you’re computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core workflow with the test profile and the respective software management profile ‘out of the box’. The test data profile will pull small test files directly from the nf-core/test-data GitHub repository and run it on your local system. The test profile is an important control to check the workflow is working as expected and is a great way to trial a workflow. Some workflows have multiple test profiles for you to test.\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nNextflow can be used to pull nf-core workflows.\nnf-core workflows follow similar structures\nnf-core workflows are configured using parameters and profiles\n\n\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/4.1_modules.html",
    "href": "workshops/4.1_modules.html",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of Nextflow modules and subworkflows\nGain an understanding of Nextflow workflow structures\nExplore some groovy functions and libraries\nSetup config, profile, and some test data"
  },
  {
    "objectID": "workshops/4.1_modules.html#environment-setup",
    "href": "workshops/4.1_modules.html#environment-setup",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here."
  },
  {
    "objectID": "workshops/4.1_modules.html#modularization",
    "href": "workshops/4.1_modules.html#modularization",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5. Modularization",
    "text": "5. Modularization\nThe definition of module libraries simplifies the writing of complex data analysis workflows and makes re-use of processes much easier.\nUsing the rnaseq.nf example from previous section, you can convert the workflow’s processes into modules, then call them within the workflow scope.\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  quant_ch.view()\n\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}"
  },
  {
    "objectID": "workshops/4.1_modules.html#modules",
    "href": "workshops/4.1_modules.html#modules",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1 Modules",
    "text": "5.1 Modules\nNextflow DSL2 allows for the definition of stand-alone module scripts that can be included and shared across multiple workflows. Each module can contain its own process or workflow definition."
  },
  {
    "objectID": "workshops/4.1_modules.html#importing-modules",
    "href": "workshops/4.1_modules.html#importing-modules",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1.1. Importing modules",
    "text": "5.1.1. Importing modules\nComponents defined in the module script can be imported into other Nextflow scripts using the include statement. This allows you to store these components in one or more file(s) that they can be re-used in multiple workflows.\nUsing the rnaseq.nf example, you can achieve this by:\nCreating a file called modules.nf in the top-level directory. Copying and pasting all process definitions for INDEX, QUANTIFICATION, FASTQC and MULTIQC into modules.nf. Removing the process definitions in the rnaseq.nf script. Importing the processes from modules.nf within the rnaseq.nf script anywhere above the workflow definition:\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION } from './modules.nf'\ninclude { FASTQC } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\n\n\n\n\n\n\nTip\n\n\n\nIn general, you would use relative paths to define the location of the module scripts using the ./prefix.\n\n\nExercise\nCreate a modules.nf file with the INDEX, QUANTIFICATION, FASTQC and MULTIQC from rnaseq.nf. Then remove these processes from rnaseq.nf and include them in the workflow using the include definitions shown above.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe rnaseq.nf script should look similar to this:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION } from './modules.nf'\ninclude { FASTQC } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  quant_ch.view()\n\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\n\n\nRun the pipeline to check if the module import is successful\nnextflow run rnaseq.nf --outdir \"results\" -resume\n\n\n\n\n\n\nChallenge\nTry modularising the modules.nf even further to achieve a setup of one tool per module (can be one or more processes), similar to the setup used by most nf-core pipelines\nnfcore/rna-seq\n  | modules\n    | local\n      | multiqc\n      | deseq2_qc\n    | nf-core\n      | fastqc\n      | salmon\n        | index\n          | main.nf\n        | quant\n          | main.nf"
  },
  {
    "objectID": "workshops/4.1_modules.html#multiple-imports",
    "href": "workshops/4.1_modules.html#multiple-imports",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1.2. Multiple imports",
    "text": "5.1.2. Multiple imports\nIf a Nextflow module script contains multiple process definitions they can also be imported using a single include statement as shown in the example below:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX; QUANTIFICATION; FASTQC; MULTIQC } from './modules.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QUANTIFICATION(index_ch, reads_ch)\n  fastqc_ch = FASTQC(reads_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}"
  },
  {
    "objectID": "workshops/4.1_modules.html#module-aliases",
    "href": "workshops/4.1_modules.html#module-aliases",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1.3 Module aliases",
    "text": "5.1.3 Module aliases\nWhen including a module component it is possible to specify a name alias using the as declaration. This allows the inclusion and the invocation of the same component multiple times using different names:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n\n}\nNote how the QUANTIFICATION process is now being refer to as QT, and FASTQC process is imported twice, each time with a different alias, and how these aliases are used to invoke the processes.\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [sharp_meitner] DSL2 - revision: 6afd5bf37c\nexecutor &gt;  local (16)\n[c7/56160a] process &gt; INDEX          [100%] 1 of 1 ✔\n[75/cb99dd] process &gt; QT (3)         [100%] 3 of 3 ✔\n[d9/e298c6] process &gt; FASTQC_one (3) [100%] 3 of 3 ✔\n[5e/7ccc39] process &gt; TRIMGALORE (3) [100%] 3 of 3 ✔\n[a3/3a1e2e] process &gt; FASTQC_two (3) [100%] 3 of 3 ✔\n[e1/411323] process &gt; MULTIQC (3)    [100%] 3 of 3 ✔\n\n\n\n\n\n\nWarning\n\n\n\nWhat do you think will happen if FASTQC is imported only once without alias, but used twice within the workflow?\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nProcess 'FASTQC' has been already used -- If you need to reuse the same component, include it with a different name or include it in a different workflow context"
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-definition",
    "href": "workshops/4.1_modules.html#workflow-definition",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2 Workflow definition",
    "text": "5.2 Workflow definition\nThe workflow scope allows the definition of components that define the invocation of one or more processes or operators:\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow my_workflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nworkflow {\n  my_workflow()\n}\nFor example, the snippet above defines a workflow named my_workflow, that is invoked via another workflow definition."
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-inputs",
    "href": "workshops/4.1_modules.html#workflow-inputs",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.1 Workflow inputs",
    "text": "5.2.1 Workflow inputs\nA workflow component can declare one or more input channels using the take statement. When the take statement is used, the workflow definition needs to be declared within the main block.\nFor example:\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow my_workflow {\n  take:\n  transcriptome_file\n  reads_ch\n\n  main:\n  index_ch = INDEX(transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\nThe input for the workflowcan then be specified as an argument:\nworkflow {\n  my_workflow(Channel.of(params.transcriptome_file), reads_ch)\n}"
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-outputs",
    "href": "workshops/4.1_modules.html#workflow-outputs",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.2 Workflow outputs",
    "text": "5.2.2 Workflow outputs\nA workflow can declare one or more output channels using the emit statement. For example:\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow my_workflow {\n  take:\n  transcriptome_file\n  reads_ch\n\n  main:\n  index_ch = INDEX(transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n\n  emit:\n  quant_ch\n\n}\n\nworkflow {\n  my_workflow(Channel.of(params.transcriptome_file), reads_ch)\n  my_workflow.out.view()\n}\nAs a result, you can use the my_workflow.out notation to access the outputs of my_workflow in the invoking workflow.\nYou can also declare named outputs within the emit block.\n  emit:\n  my_wf_output = quant_ch\nworkflow {\n  my_workflow(Channel.of(params.transcriptome_file), reads_ch)\n  my_workflow.out.my_wf_output.view()\n}\nThe result of the above snippet can then be accessed using my_workflow.out.my_wf_output."
  },
  {
    "objectID": "workshops/4.1_modules.html#calling-named-workflows",
    "href": "workshops/4.1_modules.html#calling-named-workflows",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.3 Calling named workflows",
    "text": "5.2.3 Calling named workflows\nWithin a main.nf script (called rnaseq.nf in our example) you can also have multiple workflows. In which case you may want to call a specific workflow when running the code. For this you could use the entrypoint call -entry &lt;workflow_name&gt;.\nThe following snippet has two named workflows (quant_wf and qc_wf):\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow quant_wf {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n}\n\nworkflow qc_wf {\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nworkflow {\n  quant_wf(Channel.of(params.transcriptome_file), reads_ch)\n  qc_wf(reads_ch, quant_wf.out)\n}\nBy default, running the main.nf (called rnaseq.nf in our example) will execute the main workflow block.\nnextflow run runseq.nf --outdir \"results\"\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq4.nf` [goofy_mahavira] DSL2 - revision: 2125d44217\nexecutor &gt;  local (12)\n[38/e34e41] process &gt; quant_wf:INDEX (1)   [100%] 1 of 1 ✔\n[9e/afc9e0] process &gt; quant_wf:QT (1)      [100%] 1 of 1 ✔\n[c1/dc84fe] process &gt; qc_wf:FASTQC_one (3) [100%] 3 of 3 ✔\n[2b/48680f] process &gt; qc_wf:TRIMGALORE (3) [100%] 3 of 3 ✔\n[13/71e240] process &gt; qc_wf:FASTQC_two (3) [100%] 3 of 3 ✔\n[07/cf203f] process &gt; qc_wf:MULTIQC (1)    [100%] 1 of 1 ✔\nNote that the process is now annotated with &lt;workflow-name&gt;:&lt;process-name&gt;\nBut you can choose which workflow to run by using the entry flag:\nnextflow run runseq.nf --outdir \"results\" -entry quant_wf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq5.nf` [magical_picasso] DSL2 - revision: 4ddb8eaa12\nexecutor &gt;  local (4)\n[a7/152090] process &gt; quant_wf:INDEX  [100%] 1 of 1 ✔\n[cd/612b4a] process &gt; quant_wf:QT (1) [100%] 3 of 3 ✔"
  },
  {
    "objectID": "workshops/4.1_modules.html#importing-subworkflows",
    "href": "workshops/4.1_modules.html#importing-subworkflows",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2.4 Importing Subworkflows",
    "text": "5.2.4 Importing Subworkflows\nSimilar to module script, workflow or sub-workflow can also be imported into other Nextflow scripts using the include statement. This allows you to store these components in one or more file(s) that they can be re-used in multiple workflows.\nAgain using the rnaseq.nf example, you can achieve this by:\nCreating a file called subworkflows.nf in the top-level directory. Copying and pasting all workflow definitions for quant_wf and qc_wf into subworkflows.nf. Removing the workflow definitions in the rnaseq.nf script. Importing the sub-workflows from subworkflows.nf within the rnaseq.nf script anywhere above the workflow definition:\ninclude { QUANT_WF } from './subworkflows.nf'\ninclude { QC_WF } from './subworkflows.nf'\nExercise\nCreate a subworkflows.nf file with the QUANT_WF, and QC_WF from the previous sections. Then remove these processes from rnaseq.nf and include them in the workflow using the include definitions shown above.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe rnaseq.nf script should look similar to this:\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../nf-training/ggal/transcriptome.fa\"\nparams.multiqc = \"/scratch/users/.../nf-training/multiqc\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { QUANT_WF; QC_WF } from './subworkflows.nf'\n\nworkflow {\n  QUANT_WF(Channel.of(params.transcriptome_file), reads_ch)\n  QC_WF(reads_ch, QUANT_WF.out)\n}\nand the subworkflows.nf script should look similar to this:\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow QUANT_WF{\n  take:\n  transcriptome_file\n  reads_ch\n\n  main:\n  index_ch = INDEX(transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n\n  emit:\n  quant_ch\n}\n\nworkflow QC_WF{\n  take:\n  reads_ch\n  quant_ch\n\n  main:\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n\n  emit:\n  multiqc_ch\n}\n\n\n\nRun the pipeline to check if the workflow import is successful\nnextflow run rnaseq.nf --outdir \"results\" -resume\n\n\n\n\n\n\nChallenge\nStructure modules and subworkflows similar to the setup used by most nf-core pipelines (e.g. nf-core/rnaseq)"
  },
  {
    "objectID": "workshops/4.1_modules.html#workflow-structure",
    "href": "workshops/4.1_modules.html#workflow-structure",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3 Workflow Structure",
    "text": "5.3 Workflow Structure\nThere are three directories in a Nextflow workflow repository that have a special purpose:"
  },
  {
    "objectID": "workshops/4.1_modules.html#bin",
    "href": "workshops/4.1_modules.html#bin",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3.1 ./bin",
    "text": "5.3.1 ./bin\nThe bin directory (if it exists) is always added to the $PATH for all tasks. If the tasks are performed on a remote machine, the directory is copied across to the new machine before the task begins. This Nextflow feature is designed to make it easy to include accessory scripts directly in the workflow without having to commit those scripts into the container. This feature also ensures that the scripts used inside of the workflow move on the same revision schedule as the workflow itself.\nIt is important to know that Nextflow will take care of updating $PATH and ensuring the files are available wherever the task is running, but will not change the permissions of any files in that directory. If a file is called by a task as an executable, the workflow developer must ensure that the file has the correct permissions to be executed.\nFor example, let’s say we have a small R script that produces a csv and a tsv:\n\n#!/usr/bin/env Rscript\nlibrary(tidyverse)\n\nplot &lt;- ggplot(mpg, aes(displ, hwy, colour = class)) + geom_point()\nmtcars |&gt; write_tsv(\"cars.tsv\")\nggsave(\"cars.png\", plot = plot)\nWe’d like to use this script in a simple workflow car.nf:\nprocess PlotCars {\n    // container 'rocker/tidyverse:latest'\n    container '/config/binaries/singularity/containers_devel/nextflow/r-dinoflow_0.1.1.sif'\n\n    output:\n    path(\"*.png\"), emit: \"plot\"\n    path(\"*.tsv\"), emit: \"table\"\n\n    script:\n    \"\"\"\n    cars.R\n    \"\"\"\n}\n\nworkflow {\n    PlotCars()\n\n    PlotCars.out.table | view { \"Found a tsv: $it\" }\n    PlotCars.out.plot | view { \"Found a png: $it\" }\n}\nTo do this, we can create the bin directory, write our R script into the directory. Finally, and crucially, we make the script executable:\nchmod +x bin/cars.R\n\n\n\n\n\n\nWarning\n\n\n\nAlways ensure that your scripts are executable. The scripts will not be available to your Nextflow processes without this step.\nYou will get the following error if permission is not set correctly.\nERROR ~ Error executing process &gt; 'PlotCars'\n\nCaused by:\n  Process `PlotCars` terminated with an error exit status (126)\n\nCommand executed:\n\n  cars.R\n\nCommand exit status:\n  126\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: /scratch/users/.../bin/cars.R: Permission denied\n\nWork dir:\n  /scratch/users/.../work/6b/86d3d0060266b1ca515cc851d23890\n\nTip: you can replicate the issue by changing to the process work dir and entering the command `bash .command.run`\n\n -- Check '.nextflow.log' file for details\n\n\nLet’s run the script and see what Nextflow is doing for us behind the scenes:\nnextflow run car.nf\nand then inspect the .command.run file that Nextflow has generated\nYou’ll notice a nxf_container_env bash function that appends our bin directory to $PATH:\nnxf_container_env() {\ncat &lt;&lt; EOF\nexport PATH=\"\\$PATH:/scratch/users/&lt;your-user-name&gt;/.../bin\"\nEOF\n}\nWhen working on the cloud, Nextflow will also ensure that the bin directory is copied onto the virtual machine running your task in addition to the modification of $PATH."
  },
  {
    "objectID": "workshops/4.1_modules.html#templates",
    "href": "workshops/4.1_modules.html#templates",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3.2 ./templates",
    "text": "5.3.2 ./templates\nIf a process script block is becoming too long, it can be moved to a template file. The template file can then be imported into the process script block using the template method. This is useful for keeping the process block tidy and readable. Nextflow’s use of $ to indicate variables also allows for directly testing the template file by running it as a script.\nFor example:\n# cat templates/my_script.sh\n\n#!/bin/bash\necho \"process started at `date`\"\necho $name\necho \"process completed\"\nprocess SayHiTemplate {\n    debug true\n    input: \n      val(name)\n\n    script: \n      template 'my_script.sh'\n}\n\nworkflow {\n    SayHiTemplate(\"Hello World\")\n}\nBy default, Nextflow looks for the my_script.sh template file in the templates directory located alongside the Nextflow script and/or the module script in which the process is defined. Any other location can be specified by using an absolute template path."
  },
  {
    "objectID": "workshops/4.1_modules.html#lib",
    "href": "workshops/4.1_modules.html#lib",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3.3 ./lib",
    "text": "5.3.3 ./lib\nIn the next chapter, we will start looking into adding small helper Groovy functions to the main.nf file. It may at times be helpful to bundle functionality into a new Groovy class. Any classes defined in the lib directory are available for use in the workflow - both main.nf and any imported modules.\nClasses defined in lib directory can be used for a variety of purposes. For example, the nf-core/rnaseq workflow uses five custom classes:\n\nNfcoreSchema.groovy for parsing the schema.json file and validating the workflow parameters.\nNfcoreTemplate.groovy for email templating and nf-core utility functions.\nUtils.groovy for provision of a single checkCondaChannels method.\nWorkflowMain.groovy for workflow setup and to call the NfcoreTemplate class.\nWorkflowRnaseq.groovy for the workflow-specific functions.\n\nThe classes listed above all provide utility executed at the beginning of a workflow, and are generally used to “set up” the workflow. However, classes defined in lib can also be used to provide functionality to the workflow itself."
  },
  {
    "objectID": "workshops/4.1_modules.html#groovy-functions-and-libraries",
    "href": "workshops/4.1_modules.html#groovy-functions-and-libraries",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6. Groovy Functions and Libraries",
    "text": "6. Groovy Functions and Libraries\nNextflow is a domain specific language (DSL) implemented on top of the Groovy programming language, which in turn is a super-set of the Java programming language. This means that Nextflow can run any Groovy or Java code.\nYou have already been using some Groovy code in the previous sections, but now it’s time to learn more about it."
  },
  {
    "objectID": "workshops/4.1_modules.html#some-useful-groovy-introduction",
    "href": "workshops/4.1_modules.html#some-useful-groovy-introduction",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1 Some useful groovy introduction",
    "text": "6.1 Some useful groovy introduction"
  },
  {
    "objectID": "workshops/4.1_modules.html#variables",
    "href": "workshops/4.1_modules.html#variables",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.1 Variables",
    "text": "6.1.1 Variables\nTo define a variable, simply assign a value to it:\nx = 1\nprintln x\n\nx = new java.util.Date()\nprintln x\n\nx = -3.1499392\nprintln x\n\nx = false\nprintln x\n\nx = \"Hi\"\nprintln x\n&gt;&gt; nextflow run variable.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `variable.nf` [trusting_moriondo] DSL2 - revision: ee74c86d04\n1\nWed Jun 05 03:45:19 AEST 2024\n-3.1499392\nfalse\nHi\nLocal variables are defined using the def keyword:\ndef x = 'foo'\nThe def should be always used when defining variables local to a function or a closure."
  },
  {
    "objectID": "workshops/4.1_modules.html#maps",
    "href": "workshops/4.1_modules.html#maps",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.2 Maps",
    "text": "6.1.2 Maps\nMaps are like lists that have an arbitrary key instead of an integer (allow key-value pair).\nmap = [a: 0, b: 1, c: 2]\nMaps can be accessed in a conventional square-bracket syntax or as if the key was a property of the map.\nmap = [a: 0, b: 1, c: 2]\n\nassert map['a'] == 0 \nassert map.b == 1 \nassert map.get('c') == 2 \nTo add data or to modify a map, the syntax is similar to adding values to a list:\nmap = [a: 0, b: 1, c: 2]\n\nmap['a'] = 'x' \nmap.b = 'y' \nmap.put('c', 'z') \nassert map == [a: 'x', b: 'y', c: 'z']\nMap objects implement all methods provided by the java.util.Map interface, plus the extension methods provided by Groovy."
  },
  {
    "objectID": "workshops/4.1_modules.html#if-statement",
    "href": "workshops/4.1_modules.html#if-statement",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.3 If statement",
    "text": "6.1.3 If statement\nThe if statement uses the same syntax common in other programming languages, such as Java, C, and JavaScript.\nif (&lt; boolean expression &gt;) {\n    // true branch\n}\nelse {\n    // false branch\n}\nThe else branch is optional. Also, the curly brackets are optional when the branch defines just a single statement.\nx = 1\nif (x &gt; 10)\n    println 'Hello'\nIn some cases it can be useful to replace the if statement with a ternary expression (aka a conditional expression):\nprintln list ? list : 'The list is empty'\nThe previous statement can be further simplified using the Elvis operator:\nprintln list ?: 'The list is empty'\nExercise\nWe are going to turn the rnaseq.nf into a conditional workflow with an additional params.qc_enabled to set an on/off trigger for the QC parts of the workflow.\nparams.qc_enabled = false\n\nworkflow {\n  QUANT_WF(Channel.of(params.transcriptome_file), reads_ch)\n\n  if (params.qc_enabled) {\n    QC_WF(reads_ch, QUANT_WF.out)\n  }\n}\nRun the workflow again:\nnextflow run rnaseq.nf --outdir \"results\"\nWe should only see the following two stages being executed.\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [hopeful_gautier] DSL2 - revision: 7c50056656\nexecutor &gt;  local (2)\n[c3/91f695] process &gt; QUANT_WF:INDEX (1) [100%] 1 of 1 ✔\n[1d/fac0d9] process &gt; QUANT_WF:QT (1)    [100%] 1 of 1 ✔\nThe params.qc_enabled can be turn on during execution.\nnextflow run rnaseq.nf --outdir \"results\" --qc_enabled true\n\n\n\n\n\n\nChallenge\nThe trimgalore currently only supports paired-end read. How do we update this so the same process can be used for both single-end and paired-end?\nFor reference, the (simplified) command that we can use for single-end can be as follow:\n  trim_galore \\\\\n    --gzip \\\\\n    $reads"
  },
  {
    "objectID": "workshops/4.1_modules.html#functions",
    "href": "workshops/4.1_modules.html#functions",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "6.1.4 Functions",
    "text": "6.1.4 Functions\nIt is possible to define a custom function into a script:\ndef fib(int n) {\n    return n &lt; 2 ? 1 : fib(n - 1) + fib(n - 2)\n}\n\nassert fib(10)==89\nA function can take multiple arguments separating them with a comma.\nThe return keyword can be omitted and the function implicitly returns the value of the last evaluated expression. Also, explicit types can be omitted, though not recommended:\ndef fact(n) {\n    n &gt; 1 ? n * fact(n - 1) : 1\n}\n\nassert fact(5) == 120"
  },
  {
    "objectID": "workshops/4.1_modules.html#testing",
    "href": "workshops/4.1_modules.html#testing",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "7. Testing",
    "text": "7. Testing"
  },
  {
    "objectID": "workshops/4.1_modules.html#stub",
    "href": "workshops/4.1_modules.html#stub",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "7.1 Stub",
    "text": "7.1 Stub\nYou can define a command stub, which replaces the actual process command when the -stub-run or -stub command-line option is enabled:\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n\n    stub:\n    \"\"\"\n    mkdir salmon_idx\n    touch salmon_idx/seq.bin\n    touch salmon_idx/info.json\n    touch salmon_idx/refseq.bin\n    \"\"\"\n}\nThe stub block can be defined before or after the script block. When the pipeline is executed with the -stub-run option and a process’s stub is not defined, the script block is executed.\nThis feature makes it easier to quickly prototype the workflow logic without using the real commands. The developer can use it to provide a dummy script that mimics the execution of the real one in a quicker manner. In other words, it is a way to perform a dry-run.\nExercise\nTry modifying modules.nf to add stub for the INDEX process.\n    \"\"\"\n    mkdir salmon_idx\n    touch salmon_idx/seq.bin\n    touch salmon_idx/info.json\n    touch salmon_idx/refseq.bin\n    \"\"\"\nLet’s keep the workflow to only run the INDEX process, as a new rnaseq_stub.nf\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n}\nAnd run the rnaseq_stub.nf with -stub-run\nnextflow run rnaseq_stub.nf -stub-run\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [lonely_albattani] DSL2 - revision: 11fb1399f0\nexecutor &gt;  local (1)\n[a9/7d3084] process &gt; INDEX [100%] 1 of 1 ✔\nThe process should look like it is running as normal. But if we inspect the work folder a9/7d3084, you will notice that the salmon_idx folder actually consists of three empty files that we touch as part of stub.\nls -la work/a9/7d3084636d95cba6b81a9ce8125289/salmon_idx/\ntotal 1\ndrwxrwxr-x 2 rlupat rlupat 4096 Jun  5 11:05 .\ndrwxrwxr-x 3 rlupat rlupat 4096 Jun  5 11:05 ..\n-rw-rw-r-- 1 rlupat rlupat    0 Jun  5 11:05 info.json\n-rw-rw-r-- 1 rlupat rlupat    0 Jun  5 11:05 refseq.bin\n-rw-rw-r-- 1 rlupat rlupat    0 Jun  5 11:05 seq.bin\n\n\n\n\n\n\nChallenge\nAdd stubs to all modules in modules.nf and try running the full workflow in a stub."
  },
  {
    "objectID": "workshops/4.1_modules.html#nf-test",
    "href": "workshops/4.1_modules.html#nf-test",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "7.2. nf-test",
    "text": "7.2. nf-test\nIt is critical for reproducibility and long-term maintenance to have a way to systematically test that every part of your workflow is doing what it’s supposed to do. To that end, people often focus on top-level tests, in which the workflow is un on some test data from start to finish. This is useful but unfortunately incomplete. You should also implement module-level tests (equivalent to what is called ‘unit tests’ in general software engineering) to verify the functionality of individual components of your workflow, ensuring that each module performs as expected under different conditions and inputs.\nThe nf-test package provides a testing framework that integrates well with Nextflow and makes it straightforward to add both module-level and workflow-level tests to your pipeline. For more background information, read the blog post about nf-test on the nf-core blog.\nSee this tutorial for some examples.\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core"
  },
  {
    "objectID": "workshops/4.1_draft_future_sess.html",
    "href": "workshops/4.1_draft_future_sess.html",
    "title": "Nextflow Development - Metadata Parsing",
    "section": "",
    "text": "Currently, we have defined the reads parameter as a string:\nparams.reads = \"/.../training/nf-training/data/ggal/gut_{1,2}.fq\"\nTo group the reads parameter, the fromFilePairs channel factory can be used. Add the following to the workflow block and run the workflow:\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nreads_ch.view()\nThe reads parameter is being converted into a file pair group using fromFilePairs, and is assigned to reads_ch. The reads_ch consists of a tuple of two items – the first is the grouping key of the matching pair (gut), and the second is a list of paths to each file:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\nGlob patterns can also be used to create channels of file pair groups. Inside the data directory, we have pairs of gut, liver, and lung files that can all be read into reads_ch.\n&gt;&gt;&gt; ls \"/.../training/nf-training/data/ggal/\"\n\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nRun the rnaseq.nf workflow specifying all .fq files inside /.../training/nf-training/data/ggal/ as the reads parameter via the command line:\nnextflow run rnaseq.nf --reads '/.../training/nf-training/data/ggal/*_{1,2}.fq'\nFile paths that include one or more wildcards (ie. *, ?, etc.) MUST be wrapped in single-quoted characters to avoid Bash expanding the glob on the command line.\nThe reads_ch now contains three tuple elements with unique grouping keys:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe grouping key metadata can also be explicitly created without having to rely on file names, using the map channel operator. Let’s start by creating a samplesheet rnaseq_samplesheet.csv with column headings sample_name, fastq1, and fastq2, and fill in a custom sample_name, along with the paths to the .fq files.\nsample_name,fastq1,fastq2\ngut_sample,/.../training/nf-training/data/ggal/gut_1.fq,/.../training/nf-training/data/ggal/gut_2.fq\nliver_sample,/.../training/nf-training/data/ggal/liver_1.fq,/.../training/nf-training/data/ggal/liver_2.fq\nlung_sample,/.../training/nf-training/data/ggal/lung_1.fq,/.../training/nf-training/data/ggal/lung_2.fq\nLet’s now supply the path to rnaseq_samplesheet.csv to the reads parameter in rnaseq.nf.\nparams.reads = \"/.../rnaseq_samplesheet.csv\"\nPreviously, the reads parameter consisted of a string of the .fq files directly. Now, it is a string to a .csv file containing the .fq files. Therefore, the channel factory method that reads the input file also needs to be changed. Since the parameter is now a single file path, the fromPath method can first be used, which creates a channel of Path type object. The splitCsv channel operator can then be used to parse the contents of the channel.\nreads_ch = Channel.fromPath(params.reads)\nreads_ch.view()\n\nreads_ch = reads_ch.splitCsv(header:true)\nreads_ch.view()\nWhen using splitCsv in the above example, header is set to true. This will use the first line of the .csv file as the column names. Let’s run the pipeline containing the new input parameter.\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [distraught_avogadro] DSL2 - revision: 525e081ba2\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\nexecutor &gt;  local (1)\n[4e/eeae2a] process &gt; INDEX [100%] 1 of 1 ✔\n/.../rnaseq_samplesheet.csv\n[sample_name:gut_sample, fastq1:/.../training/nf-training/data/ggal/gut_1.fq, fastq2:/.../training/nf-training/data/ggal/gut_2.fq]\n[sample_name:liver_sample, fastq1:/.../training/nf-training/data/ggal/liver_1.fq, fastq2:/.../training/nf-training/data/ggal/liver_2.f]\n[sample_name:lung_sample, fastq1:/.../training/nf-training/data/ggal/lung_1.fq, fastq2:/.../training/nf-training/data/ggal/lung_2.fq]\nThe /.../rnaseq_samplesheet.csv is the output of reads_ch directly after the fromPath channel factory method was used. Here, the channel is a Path type object. After invoking the splitCsv channel operator, the reads_ch is now replaced with a channel consisting of three elements, where each element is a row in the .csv file, returned as a list. Since header was set to true, each element in the list is also mapped to the column names. This can be used when creating the custom grouping key.\nTo create grouping key metadata from the list output by splitCsv, the map channel operator can be used.\n  reads_ch = reads_ch.map { row -&gt; \n      grp_meta = \"$row.sample_name\"\n      [grp_meta, [row.fastq1, row.fastq2]]\n      }\n  reads_ch.view()\nHere, for each list in reads_ch, we assign it to a variable row. We then create custom grouping key metadata grp_meta based on the sample_name column from the .csv, which can be accessed via the row variable by . separation. After the custom metadata key is assigned, a tuple is created by assigning grp_meta as the first element, and the two .fq files as the second element, accessed via the row variable by . separation.\nLet’s run the pipeline containing the custom grouping key:\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [happy_torricelli] DSL2 - revision: e9e1499a97\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\n[-        ] process &gt; INDEX -\n[gut_sample, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver_sample, [/home/sli/test/training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung_sample, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe custom grouping key can be created from multiple values in the samplesheet. For example, grp_meta = [sample : row.sample_name , file : row.fastq1] will create the metadata key using both the sample_name and fastq1 file names. The samplesheet can also be created to include multiple sample characteristics, such as lane, data_type, etc. Each of these characteristics can be used to ensure an adequte grouping key is creaed for that sample."
  },
  {
    "objectID": "workshops/4.1_draft_future_sess.html#metadata-parsing",
    "href": "workshops/4.1_draft_future_sess.html#metadata-parsing",
    "title": "Nextflow Development - Metadata Parsing",
    "section": "",
    "text": "Currently, we have defined the reads parameter as a string:\nparams.reads = \"/.../training/nf-training/data/ggal/gut_{1,2}.fq\"\nTo group the reads parameter, the fromFilePairs channel factory can be used. Add the following to the workflow block and run the workflow:\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nreads_ch.view()\nThe reads parameter is being converted into a file pair group using fromFilePairs, and is assigned to reads_ch. The reads_ch consists of a tuple of two items – the first is the grouping key of the matching pair (gut), and the second is a list of paths to each file:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\nGlob patterns can also be used to create channels of file pair groups. Inside the data directory, we have pairs of gut, liver, and lung files that can all be read into reads_ch.\n&gt;&gt;&gt; ls \"/.../training/nf-training/data/ggal/\"\n\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nRun the rnaseq.nf workflow specifying all .fq files inside /.../training/nf-training/data/ggal/ as the reads parameter via the command line:\nnextflow run rnaseq.nf --reads '/.../training/nf-training/data/ggal/*_{1,2}.fq'\nFile paths that include one or more wildcards (ie. *, ?, etc.) MUST be wrapped in single-quoted characters to avoid Bash expanding the glob on the command line.\nThe reads_ch now contains three tuple elements with unique grouping keys:\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe grouping key metadata can also be explicitly created without having to rely on file names, using the map channel operator. Let’s start by creating a samplesheet rnaseq_samplesheet.csv with column headings sample_name, fastq1, and fastq2, and fill in a custom sample_name, along with the paths to the .fq files.\nsample_name,fastq1,fastq2\ngut_sample,/.../training/nf-training/data/ggal/gut_1.fq,/.../training/nf-training/data/ggal/gut_2.fq\nliver_sample,/.../training/nf-training/data/ggal/liver_1.fq,/.../training/nf-training/data/ggal/liver_2.fq\nlung_sample,/.../training/nf-training/data/ggal/lung_1.fq,/.../training/nf-training/data/ggal/lung_2.fq\nLet’s now supply the path to rnaseq_samplesheet.csv to the reads parameter in rnaseq.nf.\nparams.reads = \"/.../rnaseq_samplesheet.csv\"\nPreviously, the reads parameter consisted of a string of the .fq files directly. Now, it is a string to a .csv file containing the .fq files. Therefore, the channel factory method that reads the input file also needs to be changed. Since the parameter is now a single file path, the fromPath method can first be used, which creates a channel of Path type object. The splitCsv channel operator can then be used to parse the contents of the channel.\nreads_ch = Channel.fromPath(params.reads)\nreads_ch.view()\n\nreads_ch = reads_ch.splitCsv(header:true)\nreads_ch.view()\nWhen using splitCsv in the above example, header is set to true. This will use the first line of the .csv file as the column names. Let’s run the pipeline containing the new input parameter.\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [distraught_avogadro] DSL2 - revision: 525e081ba2\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\nexecutor &gt;  local (1)\n[4e/eeae2a] process &gt; INDEX [100%] 1 of 1 ✔\n/.../rnaseq_samplesheet.csv\n[sample_name:gut_sample, fastq1:/.../training/nf-training/data/ggal/gut_1.fq, fastq2:/.../training/nf-training/data/ggal/gut_2.fq]\n[sample_name:liver_sample, fastq1:/.../training/nf-training/data/ggal/liver_1.fq, fastq2:/.../training/nf-training/data/ggal/liver_2.f]\n[sample_name:lung_sample, fastq1:/.../training/nf-training/data/ggal/lung_1.fq, fastq2:/.../training/nf-training/data/ggal/lung_2.fq]\nThe /.../rnaseq_samplesheet.csv is the output of reads_ch directly after the fromPath channel factory method was used. Here, the channel is a Path type object. After invoking the splitCsv channel operator, the reads_ch is now replaced with a channel consisting of three elements, where each element is a row in the .csv file, returned as a list. Since header was set to true, each element in the list is also mapped to the column names. This can be used when creating the custom grouping key.\nTo create grouping key metadata from the list output by splitCsv, the map channel operator can be used.\n  reads_ch = reads_ch.map { row -&gt; \n      grp_meta = \"$row.sample_name\"\n      [grp_meta, [row.fastq1, row.fastq2]]\n      }\n  reads_ch.view()\nHere, for each list in reads_ch, we assign it to a variable row. We then create custom grouping key metadata grp_meta based on the sample_name column from the .csv, which can be accessed via the row variable by . separation. After the custom metadata key is assigned, a tuple is created by assigning grp_meta as the first element, and the two .fq files as the second element, accessed via the row variable by . separation.\nLet’s run the pipeline containing the custom grouping key:\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [happy_torricelli] DSL2 - revision: e9e1499a97\nreads: rnaseq_samplesheet.csv\nreads: $params.reads\n[-        ] process &gt; INDEX -\n[gut_sample, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver_sample, [/home/sli/test/training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung_sample, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nThe custom grouping key can be created from multiple values in the samplesheet. For example, grp_meta = [sample : row.sample_name , file : row.fastq1] will create the metadata key using both the sample_name and fastq1 file names. The samplesheet can also be created to include multiple sample characteristics, such as lane, data_type, etc. Each of these characteristics can be used to ensure an adequte grouping key is creaed for that sample."
  },
  {
    "objectID": "workshops/1.1_intro_nextflow.html",
    "href": "workshops/1.1_intro_nextflow.html",
    "title": "Introduction to Nextflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nLearn about the benefits of a workflow manager.\nLearn Nextflow terminology.\nLearn basic commands and options to run a Nextflow workflow"
  },
  {
    "objectID": "workshops/1.1_intro_nextflow.html#footnotes",
    "href": "workshops/1.1_intro_nextflow.html#footnotes",
    "title": "Introduction to Nextflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.lexico.com/definition/workflow↩︎"
  },
  {
    "objectID": "workshops/3.1_creating_a_workflow.html",
    "href": "workshops/3.1_creating_a_workflow.html",
    "title": "Nextflow Development - Creating a Nextflow Workflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of Nextflow channels and processes\nGain an understanding of Nextflow syntax\nRead data of different types into a Nextflow workflow\nCreate Nextflow processes consisting of multiple scripting languages\n\n\n\n\n\nClone the training materials repository on GitHub:\ngit clone https://github.com/nextflow-io/training.git\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nMake sure to always use version 23 and above, as we have encountered problems running nf-core workflows with older versions.\nSince we are using a shared storage, we should consider including common shared paths to where software is stored. These variables can be accessed using the NXF_SINGULARITY_CACHEDIR or the NXF_CONDA_CACHEDIR environment variables.\nCurrently we set the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\n\n\n\n\nA workflow can be defined as sequence of steps through which computational tasks are chained together. Steps may be dependent on other tasks to complete, or they can be run in parallel.\n\nIn Nextflow, each step that will execute a single computational task is known as a process. Channels are used to join processes, and pass the outputs from one task into another task.\n\n\n\nChannels are a key data structure of Nextflow, used to pass data between processes.\n\n\nA queue channel connects two processes or operators, and is implicitly created by process outputs, or using channel factories such as Channel.of or Channel.fromPath.\nThe training/nf-training/snippet.nf script creates a channel where each element in the channel is an arguments provided to it. This script uses the Channel.of channel factory, which creates a channel from parameters such as strings or integers.\nch = Channel.of(1, 2, 3)\nch.view()\nThe following will be returned:\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [shrivelled_brattain] DSL2 - revision: 7e2661e10b\n1\n2\n3\n\n\n\nA value channel differs from a queue channel in that it is bound to a single value, and it can be read unlimited times without consuming its contents. To see the difference between value and queue channels, you can modify training/nf-training/snippet.nf to the following:\nch1 = Channel.of(1, 2, 3)\nch2 = Channel.of(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(ch1, ch2).view()\n}\nThis workflow creates two queue channels, ch1 and ch2, that are input into the SUM process. The SUM process sums the two inputs and prints the result to the standard output using the view() channel operator.\nAfter running the script, the only output is 2, as below:\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [modest_pike] DSL2 - revision: 7e2661e10b\n2\nSince ch1 and ch2 are queue channels, the single element of ch2 has been consumed when it was initially passed to the SUM process with the first element of ch1. Even though there are other elements to be consumed in ch1, no new process instances will be launched. This is because a process waits until it receives an input value from all the channels declared as an input. The channel values are consumed serially one after another and the first empty channel causes the process execution to stop, even though there are values in other channels.\nTo use the single element in ch2 multiple times, you can use the Channel.value channel factory. Modify the second line of training/nf-training/snippet.nf to the following: ch2 = Channel.value(1) and run the script.\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [jolly_archimedes] DSL2 - revision: 7e2661e10b\n2\n3\n4\nNow that ch2 has been read in as a value channel, its value can be read unlimited times without consuming its contents.\nIn many situations, Nextflow will implicitly convert variables to value channels when they are used in a process invocation. When a process is invoked using a workflow parameter, it is automatically cast into a value channel. Modify the invocation of the SUM process to the following: SUM(ch1, 1).view() and run the script”\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [jolly_archimedes] DSL2 - revision: 7e2661e10b\n2\n3\n4\n\n\n\n\n\nIn Nextflow, a process is the basic computing task to execute functions (i.e., custom scripts or tools).\nThe process definition starts with the keyword process, followed by the process name, commly written in upper case by convention, and finally the process body delimited by curly brackets.\nThe process body can contain many definition blocks:\nprocess &lt; name &gt; {\n    [ directives ] \n\n    input: \n    &lt; process inputs &gt;\n\n    output: \n    &lt; process outputs &gt;\n\n    [script|shell|exec]: \n    \"\"\"\n    &lt; user script to be executed &gt;\n    \"\"\"\n}\n\nDirectives are optional declarations of settings such as cpus, time, executor, container.\nInput defines the expected names and qualifiers of variables into the process\nOutput defines the expected names and qualifiers of variables output from the process\nScript is a string statement that defines the command to be executed by the process\n\nInside the script block, all $ characters need to be escaped with a \\. This is true for both referencing Bash variables created inside the script block (ie. echo \\$z) as well as performing commands (ie. echo \\$(($x+$y))), but not when referencing Nextflow variables (ie. $x+$y).\nprocess SUM {\n    debug true \n\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    z='SUM'\n    echo \\$z\n    echo \\$(($x+$y))\n    \"\"\"\n}\nBy default, the process command is interpreted as a Bash script. However, any other scripting language can be used by simply starting the script with the corresponding Shebang declaration. To reference Python variables created inside the Python script, no $ is required. For example:\nprocess PYSTUFF {\n    debug true \n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    x = 'Hello'\n    y = 'world!'\n    print (\"%s - %s\" % (x, y))\n    \"\"\"\n}\n\nworkflow {\n    PYSTUFF()\n}\n\n\nThe val qualifier allows any data type to be received as input. In the example below, num queue channel is created from integers 1, 2 and 3, and input into the BASICEXAMPLE process, where it is declared with the qualifier val and assigned to the variable x. Within this process, the channel input is referred to and accessed locally by the specified variable name x, prepended with $.\nnum = Channel.of(1, 2, 3)\n\nprocess BASICEXAMPLE {\n    debug true\n\n    input:\n    val x\n\n    script:\n    \"\"\"\n    echo process job $x\n    \"\"\"\n}\n\nworkflow {\n    BASICEXAMPLE(num)\n}\nIn the above example the process is executed three times, for each element in the channel num. Thus, it results in an output similar to the one shown below:\nprocess job 1\nprocess job 2\nprocess job 3\nThe val qualifier can also be used to specify the process output. In this example, the Hello World! string is implicitly converted into a channel that is input to the FOO process. This process prints the input to a file named file.txt, and returns the same input value as the output.\nprocess FOO {\n    input:\n    val x\n\n    output:\n    val x\n\n    script:\n    \"\"\"\n    echo $x &gt; file.txt\n    \"\"\"\n}\n\nworkflow {\n    out_ch = FOO(\"Hello world!\")\n    out_ch.view()\n}\nThe output from FOO is assigned to out_ch, and its contents printed using the view() channel operator.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [dreamy_turing] DSL2 - revision: 0d1a07970e\nexecutor &gt;  local (1)\n[a4/f710b3] process &gt; FOO [100%] 1 of 1 ✔\nHello world!\n\n\n\n\nThe path qualifier allows the handling of files inside a process. When a new instance of a process is executed, a new process execution director will be created just for that process. When the path qualifier is specified as the input, Nextflow will stage the file inside the process execution directory, allowing it to be accessed by the script using the specified name in the input declaration.\nIn this example, the reads channel is created from multiple .fq files inside training/nf-training/data/ggal, and input into process FOO. In the input declaration of the process, the file is referred to as sample.fastq.\nThe training/nf-training/data/ggal folder contains multiple .fq files, along with a .fa file. The wildcard *is used to match only .fq to be used as input.\n&gt;&gt;&gt; ls training/nf-training/data/ggal\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nSave the following code block as foo.nf.\nreads = Channel.fromPath('training/nf-training/data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path 'sample.fastq'\n\n    script:\n    \"\"\"\n    ls sample.fastq\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads)\n}\nWhen the script is ran, the FOO process is executed six times and will print the name of the file sample.fastq six times, since this is the name assigned in the input declaration.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [nasty_lamport] DSL2 - revision: b214838b82\n[78/a8a52d] process &gt; FOO [100%] 6 of 6 ✔\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nInside the process execution directory (ie. work/78/a8a52d...), the input file has been staged (symbolically linked) under the input declaration name. This allows the script to access the file within the execution directory via the declaration name.\n&gt;&gt;&gt; ll work/78/a8a52d...\nsample.fastq -&gt; /.../training/nf-training/data/ggal/liver_1.fq\nSimilarly, the path qualifier can also be used to specify one or more files that will be output by the process. In this example, the RANDOMNUM process creates a file results.txt containing a random number. Note that the Bash function is escaped with a back-slash character (ie. \\$RANDOM).\nprocess RANDOMNUM {\n    output:\n    path \"*.txt\"\n\n    script:\n    \"\"\"\n    echo \\$RANDOM &gt; result.txt\n    \"\"\"\n}\n\nworkflow {\n    receiver_ch = RANDOMNUM()\n    receiver_ch.view()\n}\nThe output file is declared with the path qualifier, and specified using the wildcard * that will output all files with .txt extension. The output of the RANDOMNUM process is assigned to receiver_ch, which can be used for downstream processes.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [nostalgic_cajal] DSL2 - revision: 9e260eead5\nexecutor &gt;  local (1)\n[76/7e8e36] process &gt; RANDOMNUM [100%] 1 of 1 ✔\n/...work/8c/792157d409524d06b89faf2c1e6d75/result.txt\n\n\n\n\nTo define paired/grouped input and output information, the tuple qualifier can be used. The input and output declarations for tuples must be declared with a tuple qualifier followed by the definition of each element in the tuple.\nIn the example below, reads_ch is a channel created using the fromFilePairs channel factory, which automatically creates a tuple from file pairs.\nreads_ch = Channel.fromFilePairs(\"training/nf-training/data/ggal/*_{1,2}.fq\")\nreads_ch.view()\nThe created tuple consists of two elements – the first element is always the grouping key of the matching pair (based on similarities in the file name), and the second is a list of paths to each file.\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nTo input a tuple into a process, the tuple qualifier must be used in the input block. Below, the first element of the tuple (ie. the grouping key) is declared with the val qualifier, and the second element of the tuple is declared with the path qualifier. The FOO process then prints the .fq file paths to a file called sample.txt, and returns it as a tuple containing the same grouping key, declared with val, and the output file created inside the process, declared with path.\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.txt')\n\n    script:\n    \"\"\"\n    echo $sample_id_paths &gt; sample.txt\n    \"\"\"\n}\n\nworkflow {\n    sample_ch = FOO(reads_ch)\n    sample_ch.view()\n}\nUpdate foo.nf to the above, and run the script.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `test.nf` [sharp_becquerel] DSL2 - revision: cd652fc08b\nexecutor &gt;  local (3)\n[65/54124a] process &gt; FOO (3) [100%] 3 of 3 ✔\n[lung, /.../work/23/fe268295bab990a40b95b7091530b6/sample.txt]\n[liver, /.../work/32/656b96a01a460f27fa207e85995ead/sample.txt]\n[gut, /.../work/ae/3cfc7cf0748a598c5e2da750b6bac6/sample.txt]\nIt’s worth noting that the FOO process is executed three times in parallel, so there’s no guarantee of a particular execution order. Therefore, if the script was ran again, the final result may be printed out in a different order:\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [high_mendel] DSL2 - revision: cd652fc08b\nexecutor &gt;  local (3)\n[82/71a961] process &gt; FOO (1) [100%] 3 of 3 ✔\n[gut, /.../work/ae/3cfc7cf0748a598c5e2da750b6bac6/sample.txt]\n[lung, /.../work/23/fe268295bab990a40b95b7091530b6/sample.txt]\n[liver, /.../work/32/656b96a01a460f27fa207e85995ead/sample.txt]\nThus, if the output of a process is being used as an input into another process, the use of the tuple qualifier that contains metadata information is especially important to ensure the correct inputs are being used for downstream processes.\n\n\n\n\n\n\nKey points\n\n\n\n\nThe contents of value channels can be consumed an unlimited amount of times, wheres queue channels cannot\nDifferent channel factories can be used to read different input types\n$ characters need to be escaped with \\ when referencing Bash variables and functions, while Nextflow variables do not\nThe scripting language within a process can be altered by starting the script with the desired Shebang declaration"
  },
  {
    "objectID": "workshops/3.1_creating_a_workflow.html#nextflow-channels-and-processes",
    "href": "workshops/3.1_creating_a_workflow.html#nextflow-channels-and-processes",
    "title": "Nextflow Development - Creating a Nextflow Workflow",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of Nextflow channels and processes\nGain an understanding of Nextflow syntax\nRead data of different types into a Nextflow workflow\nCreate Nextflow processes consisting of multiple scripting languages\n\n\n\n\n\nClone the training materials repository on GitHub:\ngit clone https://github.com/nextflow-io/training.git\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nMake sure to always use version 23 and above, as we have encountered problems running nf-core workflows with older versions.\nSince we are using a shared storage, we should consider including common shared paths to where software is stored. These variables can be accessed using the NXF_SINGULARITY_CACHEDIR or the NXF_CONDA_CACHEDIR environment variables.\nCurrently we set the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\n\n\n\n\nA workflow can be defined as sequence of steps through which computational tasks are chained together. Steps may be dependent on other tasks to complete, or they can be run in parallel.\n\nIn Nextflow, each step that will execute a single computational task is known as a process. Channels are used to join processes, and pass the outputs from one task into another task.\n\n\n\nChannels are a key data structure of Nextflow, used to pass data between processes.\n\n\nA queue channel connects two processes or operators, and is implicitly created by process outputs, or using channel factories such as Channel.of or Channel.fromPath.\nThe training/nf-training/snippet.nf script creates a channel where each element in the channel is an arguments provided to it. This script uses the Channel.of channel factory, which creates a channel from parameters such as strings or integers.\nch = Channel.of(1, 2, 3)\nch.view()\nThe following will be returned:\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [shrivelled_brattain] DSL2 - revision: 7e2661e10b\n1\n2\n3\n\n\n\nA value channel differs from a queue channel in that it is bound to a single value, and it can be read unlimited times without consuming its contents. To see the difference between value and queue channels, you can modify training/nf-training/snippet.nf to the following:\nch1 = Channel.of(1, 2, 3)\nch2 = Channel.of(1)\n\nprocess SUM {\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    echo \\$(($x+$y))\n    \"\"\"\n}\n\nworkflow {\n    SUM(ch1, ch2).view()\n}\nThis workflow creates two queue channels, ch1 and ch2, that are input into the SUM process. The SUM process sums the two inputs and prints the result to the standard output using the view() channel operator.\nAfter running the script, the only output is 2, as below:\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [modest_pike] DSL2 - revision: 7e2661e10b\n2\nSince ch1 and ch2 are queue channels, the single element of ch2 has been consumed when it was initially passed to the SUM process with the first element of ch1. Even though there are other elements to be consumed in ch1, no new process instances will be launched. This is because a process waits until it receives an input value from all the channels declared as an input. The channel values are consumed serially one after another and the first empty channel causes the process execution to stop, even though there are values in other channels.\nTo use the single element in ch2 multiple times, you can use the Channel.value channel factory. Modify the second line of training/nf-training/snippet.nf to the following: ch2 = Channel.value(1) and run the script.\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [jolly_archimedes] DSL2 - revision: 7e2661e10b\n2\n3\n4\nNow that ch2 has been read in as a value channel, its value can be read unlimited times without consuming its contents.\nIn many situations, Nextflow will implicitly convert variables to value channels when they are used in a process invocation. When a process is invoked using a workflow parameter, it is automatically cast into a value channel. Modify the invocation of the SUM process to the following: SUM(ch1, 1).view() and run the script”\n&gt;&gt;&gt; nextflow run training/nf-training/snippet.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `training/nf-training/snippet.nf` [jolly_archimedes] DSL2 - revision: 7e2661e10b\n2\n3\n4\n\n\n\n\n\nIn Nextflow, a process is the basic computing task to execute functions (i.e., custom scripts or tools).\nThe process definition starts with the keyword process, followed by the process name, commly written in upper case by convention, and finally the process body delimited by curly brackets.\nThe process body can contain many definition blocks:\nprocess &lt; name &gt; {\n    [ directives ] \n\n    input: \n    &lt; process inputs &gt;\n\n    output: \n    &lt; process outputs &gt;\n\n    [script|shell|exec]: \n    \"\"\"\n    &lt; user script to be executed &gt;\n    \"\"\"\n}\n\nDirectives are optional declarations of settings such as cpus, time, executor, container.\nInput defines the expected names and qualifiers of variables into the process\nOutput defines the expected names and qualifiers of variables output from the process\nScript is a string statement that defines the command to be executed by the process\n\nInside the script block, all $ characters need to be escaped with a \\. This is true for both referencing Bash variables created inside the script block (ie. echo \\$z) as well as performing commands (ie. echo \\$(($x+$y))), but not when referencing Nextflow variables (ie. $x+$y).\nprocess SUM {\n    debug true \n\n    input:\n    val x\n    val y\n\n    output:\n    stdout\n\n    script:\n    \"\"\"\n    z='SUM'\n    echo \\$z\n    echo \\$(($x+$y))\n    \"\"\"\n}\nBy default, the process command is interpreted as a Bash script. However, any other scripting language can be used by simply starting the script with the corresponding Shebang declaration. To reference Python variables created inside the Python script, no $ is required. For example:\nprocess PYSTUFF {\n    debug true \n\n    script:\n    \"\"\"\n    #!/usr/bin/env python\n\n    x = 'Hello'\n    y = 'world!'\n    print (\"%s - %s\" % (x, y))\n    \"\"\"\n}\n\nworkflow {\n    PYSTUFF()\n}\n\n\nThe val qualifier allows any data type to be received as input. In the example below, num queue channel is created from integers 1, 2 and 3, and input into the BASICEXAMPLE process, where it is declared with the qualifier val and assigned to the variable x. Within this process, the channel input is referred to and accessed locally by the specified variable name x, prepended with $.\nnum = Channel.of(1, 2, 3)\n\nprocess BASICEXAMPLE {\n    debug true\n\n    input:\n    val x\n\n    script:\n    \"\"\"\n    echo process job $x\n    \"\"\"\n}\n\nworkflow {\n    BASICEXAMPLE(num)\n}\nIn the above example the process is executed three times, for each element in the channel num. Thus, it results in an output similar to the one shown below:\nprocess job 1\nprocess job 2\nprocess job 3\nThe val qualifier can also be used to specify the process output. In this example, the Hello World! string is implicitly converted into a channel that is input to the FOO process. This process prints the input to a file named file.txt, and returns the same input value as the output.\nprocess FOO {\n    input:\n    val x\n\n    output:\n    val x\n\n    script:\n    \"\"\"\n    echo $x &gt; file.txt\n    \"\"\"\n}\n\nworkflow {\n    out_ch = FOO(\"Hello world!\")\n    out_ch.view()\n}\nThe output from FOO is assigned to out_ch, and its contents printed using the view() channel operator.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [dreamy_turing] DSL2 - revision: 0d1a07970e\nexecutor &gt;  local (1)\n[a4/f710b3] process &gt; FOO [100%] 1 of 1 ✔\nHello world!\n\n\n\n\nThe path qualifier allows the handling of files inside a process. When a new instance of a process is executed, a new process execution director will be created just for that process. When the path qualifier is specified as the input, Nextflow will stage the file inside the process execution directory, allowing it to be accessed by the script using the specified name in the input declaration.\nIn this example, the reads channel is created from multiple .fq files inside training/nf-training/data/ggal, and input into process FOO. In the input declaration of the process, the file is referred to as sample.fastq.\nThe training/nf-training/data/ggal folder contains multiple .fq files, along with a .fa file. The wildcard *is used to match only .fq to be used as input.\n&gt;&gt;&gt; ls training/nf-training/data/ggal\ngut_1.fq  gut_2.fq  liver_1.fq  liver_2.fq  lung_1.fq  lung_2.fq  transcriptome.fa\nSave the following code block as foo.nf.\nreads = Channel.fromPath('training/nf-training/data/ggal/*.fq')\n\nprocess FOO {\n    debug true\n\n    input:\n    path 'sample.fastq'\n\n    script:\n    \"\"\"\n    ls sample.fastq\n    \"\"\"\n}\n\nworkflow {\n    FOO(reads)\n}\nWhen the script is ran, the FOO process is executed six times and will print the name of the file sample.fastq six times, since this is the name assigned in the input declaration.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [nasty_lamport] DSL2 - revision: b214838b82\n[78/a8a52d] process &gt; FOO [100%] 6 of 6 ✔\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nsample.fastq\nInside the process execution directory (ie. work/78/a8a52d...), the input file has been staged (symbolically linked) under the input declaration name. This allows the script to access the file within the execution directory via the declaration name.\n&gt;&gt;&gt; ll work/78/a8a52d...\nsample.fastq -&gt; /.../training/nf-training/data/ggal/liver_1.fq\nSimilarly, the path qualifier can also be used to specify one or more files that will be output by the process. In this example, the RANDOMNUM process creates a file results.txt containing a random number. Note that the Bash function is escaped with a back-slash character (ie. \\$RANDOM).\nprocess RANDOMNUM {\n    output:\n    path \"*.txt\"\n\n    script:\n    \"\"\"\n    echo \\$RANDOM &gt; result.txt\n    \"\"\"\n}\n\nworkflow {\n    receiver_ch = RANDOMNUM()\n    receiver_ch.view()\n}\nThe output file is declared with the path qualifier, and specified using the wildcard * that will output all files with .txt extension. The output of the RANDOMNUM process is assigned to receiver_ch, which can be used for downstream processes.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [nostalgic_cajal] DSL2 - revision: 9e260eead5\nexecutor &gt;  local (1)\n[76/7e8e36] process &gt; RANDOMNUM [100%] 1 of 1 ✔\n/...work/8c/792157d409524d06b89faf2c1e6d75/result.txt\n\n\n\n\nTo define paired/grouped input and output information, the tuple qualifier can be used. The input and output declarations for tuples must be declared with a tuple qualifier followed by the definition of each element in the tuple.\nIn the example below, reads_ch is a channel created using the fromFilePairs channel factory, which automatically creates a tuple from file pairs.\nreads_ch = Channel.fromFilePairs(\"training/nf-training/data/ggal/*_{1,2}.fq\")\nreads_ch.view()\nThe created tuple consists of two elements – the first element is always the grouping key of the matching pair (based on similarities in the file name), and the second is a list of paths to each file.\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\nTo input a tuple into a process, the tuple qualifier must be used in the input block. Below, the first element of the tuple (ie. the grouping key) is declared with the val qualifier, and the second element of the tuple is declared with the path qualifier. The FOO process then prints the .fq file paths to a file called sample.txt, and returns it as a tuple containing the same grouping key, declared with val, and the output file created inside the process, declared with path.\nprocess FOO {\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path('sample.txt')\n\n    script:\n    \"\"\"\n    echo $sample_id_paths &gt; sample.txt\n    \"\"\"\n}\n\nworkflow {\n    sample_ch = FOO(reads_ch)\n    sample_ch.view()\n}\nUpdate foo.nf to the above, and run the script.\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `test.nf` [sharp_becquerel] DSL2 - revision: cd652fc08b\nexecutor &gt;  local (3)\n[65/54124a] process &gt; FOO (3) [100%] 3 of 3 ✔\n[lung, /.../work/23/fe268295bab990a40b95b7091530b6/sample.txt]\n[liver, /.../work/32/656b96a01a460f27fa207e85995ead/sample.txt]\n[gut, /.../work/ae/3cfc7cf0748a598c5e2da750b6bac6/sample.txt]\nIt’s worth noting that the FOO process is executed three times in parallel, so there’s no guarantee of a particular execution order. Therefore, if the script was ran again, the final result may be printed out in a different order:\n&gt;&gt;&gt; nextflow run foo.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `foo.nf` [high_mendel] DSL2 - revision: cd652fc08b\nexecutor &gt;  local (3)\n[82/71a961] process &gt; FOO (1) [100%] 3 of 3 ✔\n[gut, /.../work/ae/3cfc7cf0748a598c5e2da750b6bac6/sample.txt]\n[lung, /.../work/23/fe268295bab990a40b95b7091530b6/sample.txt]\n[liver, /.../work/32/656b96a01a460f27fa207e85995ead/sample.txt]\nThus, if the output of a process is being used as an input into another process, the use of the tuple qualifier that contains metadata information is especially important to ensure the correct inputs are being used for downstream processes.\n\n\n\n\n\n\nKey points\n\n\n\n\nThe contents of value channels can be consumed an unlimited amount of times, wheres queue channels cannot\nDifferent channel factories can be used to read different input types\n$ characters need to be escaped with \\ when referencing Bash variables and functions, while Nextflow variables do not\nThe scripting language within a process can be altered by starting the script with the desired Shebang declaration"
  },
  {
    "objectID": "workshops/3.1_creating_a_workflow.html#creating-an-rnaseq-workflow",
    "href": "workshops/3.1_creating_a_workflow.html#creating-an-rnaseq-workflow",
    "title": "Nextflow Development - Creating a Nextflow Workflow",
    "section": "Creating an RNAseq Workflow",
    "text": "Creating an RNAseq Workflow\n\n\n\n\n\n\nObjectives\n\n\n\n\nDevelop a Nextflow workflow\nRead data of different types into a Nextflow workflow\nOutput Nextflow process results to a predefined directory\n\n\n\n\n4.1.1. Define Workflow Parameters\nLet’s create a Nextflow script rnaseq.nf for a RNA-seq workflow. The code begins with a shebang, which declares Nextflow as the interpreter.\n#!/usr/bin/env nextflow\nOne way to define the workflow parameters is inside the Nextflow script.\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/.../training/nf-training/data/ggal/transcriptome.fa\"\nparams.multiqc = \"/.../training/nf-training/multiqc\"\n\nprintln \"reads: $params.reads\"\nWorkflow parameters can be defined and accessed inside the Nextflow script by prepending the prefix params to a variable name, separated by a dot character, eg. params.reads.\nDifferent data types can be assigned as a parameter in Nextflow. The reads parameter is defined as multiple .fq files. The transcriptome_file parameter is defined as one file, /.../training/nf-training/data/ggal/transcriptome.fa. The multiqc parameter is defined as a directory, /.../training/nf-training/data/ggal/multiqc.\nThe Groovy println command is then used to print the contents of the reads parameter, which is access with the $ character.\nRun the script:\n&gt;&gt;&gt; nextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [astonishing_raman] DSL2 - revision: 8c9adc1772\nreads: /.../training/nf-training/data/ggal/*_{1,2}.fq\n\n\n\n4.1.2. Create a transcriptome index file\nCommands or scripts can be executed inside a process.\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nThe INDEX process takes an input path, and assigns that input as the variable transcriptome. The path type qualifier will allow Nextflow to stage the files in the process execution directory, where they can be accessed by the script via the defined variable name, ie. transcriptome. The code between the three double-quotes of the script block will be executed, and accesses the input transcriptome variable using $. The output is a path, with a filename salmon_idx. The output path can also be defined using wildcards, eg. path \"*_idx\".\nNote that the name of the input file is not used and is only referenced to by the input variable name. This feature allows pipeline tasks to be self-contained and decoupled from the execution environment. As best practice, avoid referencing files that are not defined in the process script.\nTo execute the INDEX process, a workflow scope will need to be added.\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n}\nHere, the params.transcriptome_file parameter we defined earlier in the Nextflow script is used as an input into the INDEX process. The output of the process is assigned to the index_ch channel.\nRun the Nextflow script:\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\nERROR ~ Error executing process &gt; 'INDEX'\n\nCaused by:\n  Process `INDEX` terminated with an error exit status (127)\n\nCommand executed:\n\n  salmon index --threads 1 -t transcriptome.fa -i salmon_index\n\nCommand exit status:\n  127\n\nCommand output:\n  (empty)\n\nCommand error:\n  .command.sh: line 2: salmon: command not found\n\nWork dir:\n  /.../work/85/495a21afcaaf5f94780aff6b2a964c\n\nTip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`\n\n -- Check '.nextflow.log' file for details\nWhen a process execution exits with a non-zero exit status, the workflow will be stopped. Nextflow will output the cause of the error, the command that caused the error, the exit status, the standard output (if available), the comand standard error, and the work directory where the process was executed.\nLet’s first look inside the process execution directory:\n&gt;&gt;&gt; ls -a /.../work/85/495a21afcaaf5f94780aff6b2a964c \n\n.   .command.begin  .command.log  .command.run  .exitcode\n..  .command.err    .command.out  .command.sh   transcriptome.fa\nWe can see that the input file transcriptome.fa has been staged inside this process execution directory by being symbolically linked. This allows it to be accessed by the script.\nInside the .command.err script, we can see that the salmon command was not found, resulting in the termination of the Nextflow workflow.\nSingularity containers can be used to execute the process within an environment that contains the package of interest. Create a config file nextflow.config containing the following:\nsingularity {\n  enabled = true\n  autoMounts = true\n  cacheDir = \"/config/binaries/singularity/containers_devel/nextflow\"\n}\nThe container process directive can be used to specify the required container:\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nRun the Nextflow script:\n&gt;&gt;&gt; nextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [distraught_goldwasser] DSL2 - revision: bdebf34e16\nexecutor &gt;  local (1)\n[37/7ef8f0] process &gt; INDEX [100%] 1 of 1 ✔\nThe newly created nextflow.config files does not need to be specified in the nextflow run command. This file is automatically searched for and used by Nextflow.\nAn alternative to singularity containers is the use of a module. Since the script block is executed as a Bash script, it can contain any command or script normally executed on the command line. If there is a module present in the host environment, it can be loaded as part of the process script.\nprocess INDEX {\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    module purge\n    module load salmon/1.3.0\n\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nRun the Nextflow script:\n&gt;&gt;&gt; nextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [reverent_liskov] DSL2 - revision: b74c22049d\nexecutor &gt;  local (1)\n[ba/3c12ab] process &gt; INDEX [100%] 1 of 1 ✔\n\n\n\n4.1.3. Collect Read Files By Pairs\nPreviously, we have defined the reads parameter to be the following:\nparams.reads = \"/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nChallenge: Convert the reads parameter into a tuple channel called reads_ch, where the first element is a unique grouping key, and the second element is the paired .fq files. Then, view the contents of reads_ch\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nreads_ch.view()\nThe fromFilePairs channel factory will automatically group input files into a tuple with a unique grouping key. The view() channel operator can be used to view the contents of the channel.\n&gt;&gt;&gt; nextflow run rnaseq.nf\n\n[gut, [/.../training/nf-training/data/ggal/gut_1.fq, /.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/.../training/nf-training/data/ggal/liver_1.fq, /.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, [/.../training/nf-training/data/ggal/lung_1.fq, /.../training/nf-training/data/ggal/lung_2.fq]]\n\n\n\n\n\n4.1.4. Perform Expression Quantification\nLet’s add a new process QUANTIFICATION that uses both the indexed transcriptome file and the .fq file pairs to execute the salmon quant command.\nprocess QUANTIFICATION {\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nThe QUANTIFICATION process takes two inputs, the first is the path to the salmon_index created from the INDEX process. The second input is set to match the output of fromFilePairs – a tuple where the first element is a value (ie. grouping key), and the second element is a list of paths to the .fq reads.\nIn the script block, the salmon quant command saves the output of the tool as $sample_id. This output is emitted by the QUANTIFICATION process, using $ to access the Nextflow variable.\nChallenge:\nSet the following as the execution container for QUANTIFICATION:\n/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\nAssign index_ch and reads_ch as the inputs to this process, and emit the process outputs as quant_ch. View the contents of quant_ch\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nTo assign a container to a process, the container directive can be used.\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nTo run the QUANTIFICATION process and emit the outputs as quant_ch, the following can be added to the end of the workflow block:\nquant_ch = QUANTIFICATION(index_ch, reads_ch)\nquant_ch.view()\nThe script can now be run:\n&gt;&gt;&gt; nextflow run rnaseq.nf \nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [elated_cray] DSL2 - revision: abe41f4f69\nexecutor &gt;  local (4)\n[e5/e75095] process &gt; INDEX              [100%] 1 of 1 ✔\n[4c/68a000] process &gt; QUANTIFICATION (1) [100%] 3 of 3 ✔\n/.../work/b1/d861d26d4d36864a17d2cec8d67c80/liver\n/.../work/b4/a6545471c1f949b2723d43a9cce05f/lung\n/.../work/4c/68a000f7c6503e8ae1fe4d0d3c93d8/gut\nIn the Nextflow output, we can see that the QUANTIFICATION process has been ran three times, since the reads_ch consists of three elements. Nextflow will automatically run the QUANTIFICATION process on each of the elements in the input channel, creating separate process execution work directories for each execution.\n\n\n\n\n\n4.1.5. Quality Control\nNow, let’s implement a FASTQC quality control process for the input fastq reads.\nChallenge:\nCreate a process called FASTQC that takes reads_ch as an input, and declares the process input to be a tuple matching the structure of reads_ch, where the first element is assigned the variable sample_id, and the second variable is assigned the variable reads. This FASTQC process will first create an output directory fastqc_${sample_id}_logs, then perform fastqc on the input reads and save the results in the newly created directory fastqc_${sample_id}_logs:\nmkdir fastqc_${sample_id}_logs\nfastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\nTake fastqc_${sample_id}_logs as the output of the process, and assign it to the channel fastqc_ch. Finally, specify the process container to be the following:\n/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe process FASTQC is created in rnaseq.nf. Since the input channel is a tuple, the process input declaration is a tuple containing elements that match the structure of the incoming channel. The first element of the tuple is assigned the variable sample_id, and the second element of the tuple is assigned the variable reads. The relevant container is specified using the container process directive.\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\nIn the workflow scope, the following can be added:\nfastqc_ch = FASTQC(reads_ch)\nThe FASTQC process is called, taking reads_ch as an input. The output of the process is assigned to be fastqc_ch.\n&gt;&gt;&gt; nextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [sad_jennings] DSL2 - revision: cfae7ccc0e\nexecutor &gt;  local (7)\n[b5/6bece3] process &gt; INDEX              [100%] 1 of 1 ✔\n[32/46f20b] process &gt; QUANTIFICATION (3) [100%] 3 of 3 ✔\n[44/27aa8d] process &gt; FASTQC (2)         [100%] 3 of 3 ✔\nIn the Nextflow output, we can see that the FASTQC has been ran three times as expected, since the reads_ch consists of three elements.\n\n\n\n\n\n4.1.6. MultiQC Report\nSo far, the generated outputs have all been saved inside the Nextflow work directory. For the FASTQC process, the specified output directory is only created inside the process execution directory. To save results to a specified folder, the publishDir process directive can be used.\nLet’s create a new MULTIQC process in our workflow that takes the outputs from the QUANTIFICATION and FASTQC processes to create a final report using the multiqc tool, and publish the process outputs to a directory outside of the process execution directory.\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\nIn the MULTIQC process, the multiqc command is performed on both quantification and fastqc inputs, and publishes the report to a directory defined by the outdir parameter. Only files that match the declaration in the output block are published, not all the outputs of a process. By default, files are published to the target folder creating a symbolic link to the file produced in the process execution directory. This behavior can be modified using the mode option, eg. copy, which copies the file from the process execution directory to the specified output directory.\nAdd the following to the end of workflow scope:\nmultiqc_ch = MULTIQC(quant_ch, fastqc_ch)\nRun the pipeline, specifying an output directory using the outdir parameter:\nnextflow run rnaseq.nf --outdir \"results\"\nA results directory containing the output multiqc reports will be created outside of the process execution directory.\n&gt;&gt;&gt; ls results\ngut.html  liver.html  lung.html\n\n\n\n\n\n\n\nKey points\n\n\n\n\nCommands or scripts can be executed inside a process\nEnvironments can be defined using the container process directive\nThe input declaration for a process must match the structure of the channel that is being passed into that process\n\n\n\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, and Nextflow Patterns materials from Nextflow and nf-core\n^*Draft for Future Sessions"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html",
    "href": "workshops/5.1_nf_core_template.html",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "",
    "text": "Objectives\n\n\n\n\nDevelop a basic Nextflow workflow with nf-core templates\nTest and set up profiles for a Nextflow workflow\nCreate conditional processes, and conditional scripts within a processs\nRead data of different types into a Nextflow workflow"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#environment-setup",
    "href": "workshops/5.1_nf_core_template.html#environment-setup",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\nSet up a python virtual environment with nf-core/tools installed:\nmodule load python/3.11.2\npython -m venv /scratch/users/${USER}/nfcorevenv\n\nsource /scratch/users/${USER}/nfcorevenv/bin/activate\n\npip install nf-core==2.14.1"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core",
    "href": "workshops/5.1_nf_core_template.html#nf-core",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5. Nf-core",
    "text": "5. Nf-core\nnf-core is a community effort to collect a curated set of analysis workflows built using Nextflow.\nnf-core provides a standardized set of best practices, guidelines, and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular, scalable, and portable, allowing researchers to easily adapt and execute them using their own data and compute resources.\nThe community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics.\nOne of the key benefits of nf-core is that it promotes open development, testing, and peer review, ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries.\nnf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276–278 (2020). Nature Biotechnology\nKey Features of nf-core workflows\n\nDocumentation\n\nnf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won’t be left in the dark.\n\nStable Releases\n\nnf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible.\n\nPackaged software\n\nPipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations.\n\nPortable and reproducible\n\nnf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute.\n\nCloud-ready\n\nnf-core workflows are tested on AWS"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core-tools",
    "href": "workshops/5.1_nf_core_template.html#nf-core-tools",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.1 Nf-core tools",
    "text": "5.1 Nf-core tools\nnf-core-tools is a python package with helper tools for the nf-core community.\nThese helper tools can be used for both building and running nf-core workflows.\nToday we will be focusing on the developer commands to build a workflow using nf-core templates and structures.\nTake a look at what is within with nf-core-tools suite\nnf-core -h\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\n                                                                                                    \n Usage: nf-core [OPTIONS] COMMAND [ARGS]...                                                         \n                                                                                                    \n nf-core/tools provides a set of helper tools for use with nf-core Nextflow pipelines.              \n It is designed for both end-users running pipelines and also developers creating new pipelines.    \n                                                                                                    \n╭─ Options ────────────────────────────────────────────────────────────────────────────────────────╮\n│ --version                        Show the version and exit.                                      │\n│ --verbose        -v              Print verbose output to the console.                            │\n│ --hide-progress                  Don't show progress bars.                                       │\n│ --log-file       -l  &lt;filename&gt;  Save a verbose log to a file.                                   │\n│ --help           -h              Show this message and exit.                                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Commands for users ─────────────────────────────────────────────────────────────────────────────╮\n│ list                  List available nf-core pipelines with local info.                          │\n│ launch                Launch a pipeline using a web GUI or command line prompts.                 │\n│ create-params-file    Build a parameter file for a pipeline.                                     │\n│ download              Download a pipeline, nf-core/configs and pipeline singularity images.      │\n│ licences              List software licences for a given workflow (DSL1 only).                   │\n│ tui                   Open Textual TUI.                                                          │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\n╭─ Commands for developers ────────────────────────────────────────────────────────────────────────╮\n│ create            Create a new pipeline using the nf-core template.                              │\n│ lint              Check pipeline code against nf-core guidelines.                                │\n│ modules           Commands to manage Nextflow DSL2 modules (tool wrappers).                      │\n│ subworkflows      Commands to manage Nextflow DSL2 subworkflows (tool wrappers).                 │\n│ schema            Suite of tools for developers to manage pipeline schema.                       │\n│ create-logo       Generate a logo with the nf-core logo template.                                │\n│ bump-version      Update nf-core pipeline version number.                                        │\n│ sync              Sync a pipeline TEMPLATE branch with the nf-core template.                     │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nToday we will be predominately focusing on most of the tools for developers."
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core-pipeline",
    "href": "workshops/5.1_nf_core_template.html#nf-core-pipeline",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.2 Nf-core Pipeline",
    "text": "5.2 Nf-core Pipeline\nLet’s review the structure of the nf-core/rnaseq pipeline.\nAlmost all of the structure provided here is from the nf-core templates. As we briefly covered last week in Developing Modularised Workflows, it is good practice to separate your workflow from subworkflows and modules. As this allows you to modularise your workflows and reuse modules.\nNf-core assists in enforcing this structure with the subfolders:\n\nworkflows - contains the main workflow\nsubworkflows - contains subworkflows either as written by the nf-core community or self-written\nmodules - contains modules either as written by the nf-core community or self-written\n\nIn our Introduction to Nextflow and running nf-core workflows workshop in Customising & running nf-core pipelines, we briefly touched on configuration files in the conf/ folder and nextflow.config.\nToday we will be working on files in these locations and expanding our use of the nf-core template to include:\n\nfiles in the assets folder\nnextflow_schema.json\n\n\n\n5.2.1 nf-core create\nThe create subcommand makes a new pipeline using the nf-core base template. With a given pipeline name, description and author, it makes a starter pipeline which follows nf-core best practices.\nAfter creating the files, the command initialises the folder as a git repository and makes an initial commit. This first “vanilla” commit which is identical to the output from the templating tool is important, as it allows us to keep your pipeline in sync with the base template in the future. See the nf-core syncing docs for more information.\nLet’s set up the nf-core template for today’s workshop:\nnf-core create\nAs we progress through the interactive prompts, we will use the following values below: \nRemember to swap out the Author name with your own!\nThe creates a pipeline called myrnaseq in the directory pmcc-myrnaseq (&lt;prefix&gt;-&lt;name&gt;) with mmyeung as the author. If selected exclude the following:\n\ngithub: removed all files required for GitHub hosting of the pipeline. Specifically, the .github folder and .gitignore file.\nci: removes the GitHub continuous integration tests from the pipeline. Specifically, the .github/workflows/ folder.\ngithub_badges: removes GitHub badges from the README.md file.\nigenomes: removes pipeline options related to iGenomes. Including the conf/igenomes.config file and all references to it.\nnf_core_configs: excludes nf_core/configs repository options, which make multiple config profiles for various institutional clusters available.\n\nTo run the pipeline creation silently (i.e. without any prompts) with the nf-core template, you can use the --plain option.\n\n\n\n\n\n\nAuthor name\n\n\n\nTypically, we would use your github username as the value here, this allows an extra layer of traceability.\n\n\n\n\n\n\n\n\nCustomised pipeline prefix\n\n\n\nRemember we are currently only making the most of the nf-core templates and not contributing back to nf-core. As such, we should not use the nf-core prefix to our pipeline.\n\n\n\n\n\n\n\n\nSkipped templates\n\n\n\nNote that the highlighted values under Skip template areas? are the sections that will be skipped. As this is a test pipeline we are skipping the set up of github CI and badges\n\n\nAs we have requested GitHub hosting, on completion of the command, you will note there are suggested github commands included in the output. Use these commands to push the commits from your computer. You can then continue to edit, commit and push normally as you build your pipeline.\n\n\n\nnf-core template\nLet’s see what has been minimally provided by nf-core create\nll pmcc-myrnaseq/\ntotal 47\ndrwxrwxr-x 2 myeung myeung  4096 Jun 11 15:00 assets\n-rw-rw-r-- 1 myeung myeung   372 Jun 11 15:00 CHANGELOG.md\n-rw-rw-r-- 1 myeung myeung  2729 Jun 11 15:00 CITATIONS.md\ndrwxrwxr-x 2 myeung myeung  4096 Jun 11 15:00 conf\ndrwxrwxr-x 3 myeung myeung  4096 Jun 11 15:00 docs\n-rw-rw-r-- 1 myeung myeung  1060 Jun 11 15:00 LICENSE\n-rw-rw-r-- 1 myeung myeung  3108 Jun 11 15:00 main.nf\ndrwxrwxr-x 3 myeung myeung  4096 Jun 11 15:00 modules\n-rw-rw-r-- 1 myeung myeung  1561 Jun 11 15:00 modules.json\n-rw-rw-r-- 1 myeung myeung  9982 Jun 11 15:00 nextflow.config\n-rw-rw-r-- 1 myeung myeung 16657 Jun 11 15:00 nextflow_schema.json\n-rw-rw-r-- 1 myeung myeung  3843 Jun 11 15:00 README.md\ndrwxrwxr-x 4 myeung myeung  4096 Jun 11 15:00 subworkflows\n-rw-rw-r-- 1 myeung myeung   165 Jun 11 15:00 tower.yml\ndrwxrwxr-x 2 myeung myeung  4096 Jun 11 15:00 workflows\nAs you take look through the files created you will see many comments through the files starting with // TODO nf-core. These are pointers from nf-core towards areas of the pipeline that you may be intersted in changing.\nThey are also the “key word” used by the nf-core lint.\n\nAlternative setups for nf-core create\nAside from the interactive setup we have just completed for nf-core create, there are two alternative methods.\n\nProvide the option using the optional flags from nf-core create\nProvide a template.yaml via the --template-yaml option\n\n\n\n\n\n\n\nChallenge\n\n\n\nCreate a second pipeline template using the optional flags with the name “myworkflow”, provide a description, author name and set the version to “0.0.1”\nWhat options are still you still prompted for?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the following:\nnf-core create --name myworkflow --description \"my workflow test\" --author \"@mmyeung\" --version \"0.0.1\"\nNote that you are still prompted for any additional customisations such as the pipeline prefix and steps to skip\n\n\n\n\n\n\n\n\n\nAdvanced Challange\n\n\n\nCreate another pipeline template using a yaml file called mytemplate.yaml\nHint: the key values in the yaml should be name, description, author, prefix and skip\nSet the pipeline to skip ci, igenomes and nf_core_configs\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the following:\nvim mytemplate.yaml\nValues in mytemplate.yaml\nname: coolpipe\ndescription: A cool pipeline\nauthor: me\nprefix: myorg\nskip:\n  - ci\n  - igenomes\n  - nf_core_configs\nnf-core create --template-yaml mytemplate.yaml"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#test-profile",
    "href": "workshops/5.1_nf_core_template.html#test-profile",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.3 Test Profile",
    "text": "5.3 Test Profile\nnf-core tries to encourage software engineering concepts such as minimal test sets, this can be set up using the conf/test.config and conf/test_full.config\nFor the duration of this workshop we will be making use of the conf/test.config, to test our pipeline.\nLet’s take a look at what is currently in the conf/test.config.\ncat pmcc-myrnaseq/conf/test.config\n/*\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Nextflow config file for running minimal tests\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    Defines input files and everything required to run a fast and simple pipeline test.\n\n    Use as follows:\n        nextflow run pmcc/myrnaseq -profile test,&lt;docker/singularity&gt; --outdir &lt;OUTDIR&gt;\n\n----------------------------------------------------------------------------------------\n*/\n\nparams {\n    config_profile_name        = 'Test profile'\n    config_profile_description = 'Minimal test dataset to check pipeline function'\n\n    // Limit resources so that this can run on GitHub Actions\n    max_cpus   = 2\n    max_memory = '6.GB'\n    max_time   = '6.h'\n\n    // Input data\n    // TODO nf-core: Specify the paths to your test data on nf-core/test-datasets\n    // TODO nf-core: Give any required params for the test so that command line flags are not needed\n    input  = params.pipelines_testdata_base_path + 'viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv'\n\n    // Genome references\n    genome = 'R64-1-1'\n}\nFrom this, we can see that this config uses the params scope to define:\n\nMaximal values for resources\nDirects the input parameter to a sample sheet hosted in the nf-core/testdata github\nSets the genome to “R64-1-1”\n\n\n\n\n\n\n\nHow does setting the parameter genome set all the genome references?\n\n\n\nThis is possible due to us using the igenomes configs from nf-core.\nYou can see in the conf/igenomes.config how nested within each genome definition are paths to various reference files.\nTo find out more about the igenomes project here\n\n\nFor the duration of this workshop we are going to use the data from nf-training that was cloned in the first workshop. We are also going to update our test.config to contain the igenomes_base parameter, as we have a local cache on the cluster.\ninput = \"/home/Shared/For_NF_Workshop/training/nf-training/data/ggal/samplesheet.csv\"\noutdir = \"/scratch/users/${USER}/myrnaseqtest\"\n\n// genome references\ngenome = \"GRCh38\"\nigenomes_base = \"/data/janis/nextflow/references/genomes/ngi-igenomes\"\nAlso, we will need to change the value, custom_config_base to null, in nextflow.config\ncustom_config_base         = null\nLet’s quickly check that our pipeline runs with the test profile.\ncd ..\nnextflow run ./pmcc-myrnaseq -profile test,singularity\n\n\n\n\n\n\nWhat’s the difference between the test.config and the test_full.config\n\n\n\nTypically the test.config contains the minimal test example, while the test_full.config contains at least one full sized example data."
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core-modules",
    "href": "workshops/5.1_nf_core_template.html#nf-core-modules",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.4 Nf-core modules",
    "text": "5.4 Nf-core modules\nYou can find all the nf-core modules that have been accepted and peer-tested by the community in nf-core modules.\nor with\nnf-core modules list remote\nyou can check which modules are installed localling in your pipeline by running nf-core modules list local, within the pipeline folder.\ncd pmcc-myrnaseq\n\nnf-core modules list local\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nINFO     Repository type: pipeline\nINFO     Modules installed in '.':\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name ┃ Repository      ┃ Version SHA ┃ Message                              ┃ Date       ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc      │ nf-core/modules │ 285a505     │ Fix FastQC memory allocation (#5432) │ 2024-04-05 │\n│ multiqc     │ nf-core/modules │ b7ebe95     │ Update MQC container (#5006)         │ 2024-02-29 │\n└─────────────┴─────────────────┴─────────────┴──────────────────────────────────────┴────────────┘\n\n\n\n\n\n\nOverall Challenge\n\n\n\nWe are going to replicate sections of the workflow from last week.\nFASTQC -&gt; Trimgalore -&gt; FASTQC -&gt; MULTIQC\n\n\n\n5.3.1 Installing nf-core modules\nThe general format for installing modules is as below.\nnf-core modules install &lt;tool&gt;/&lt;subcommand&gt;\n\n\n\n\n\n\nTip\n\n\n\nNote that if you search for the modules on the nf-core modules website, you can find the install command at the top of the tool\n\n\n\n\n\n\n\n\nTip\n\n\n\nRemember to run the install commands from within the nf-core pipeline folder (in this case the pmcc-myrnaseq folder)\nIf you are not in an nf-core folder you will see the following error\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nWARNING  'repository_type' not defined in .nf-core.yml\n? Is this repository an nf-core pipeline or a fork of nf-core/modules? (Use arrow keys)\n » Pipeline\n   nf-core/modules\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nInstall the following nf-core modules\n\ntrimgalore\nsalmon quant\nfastqc\n\nWhat happens when we try to install the fastqc module?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUnfortunately, nf-core does not allow the installation of multiple modules in one line therefore we mush provide the commands separately for each module.\nnf-core modules install trimgalore\nnf-core modules install salmon/quant\nnf-core modules install fastqc\nNote that from above, when we checked which modules have been installed locally in our pipeline, fastqc was already installed. As such, we see the following output warning us that fastqc is installed and we can either force the reinstallation or we can update the module\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\nINFO     Module 'fastqc' is already installed.\nINFO     To update 'fastqc' run 'nf-core modules update fastqc'. To force reinstallation use '--force'. \n\n\n\n\n\n\n\n\n\nAdvanced Challenge\n\n\n\nCan you think of a way to streamline the installation of modules?\n\n\nfollowing the installation what files changed, check with\ngit status\nOn branch master\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n        modified:   modules.json\n\nUntracked files:\n  (use \"git add &lt;file&gt;...\" to include in what will be committed)\n        modules/nf-core/salmon/\n        modules/nf-core/trimgalore/\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nmodules.json is a running record of the modules installed and should be included in your pipeline. Note: you can find the github SHA for the exact “version” of the module installed.\nThis insulates your pipeline from when a module is deleted.\nrm -r modules/nf-core/salmon/quant\n\nnf-core modules list local\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nINFO     Repository type: pipeline\nINFO     Reinstalling modules found in 'modules.json' but missing from directory: 'modules/nf-core/salmon/quant'\nINFO     Modules installed in '.':\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ Module Name  ┃ Repository      ┃ Version SHA ┃ Message                                ┃ Date       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ fastqc       │ nf-core/modules │ 285a505     │ Fix FastQC memory allocation (#5432)   │ 2024-04-05 │\n│ multiqc      │ nf-core/modules │ b7ebe95     │ Update MQC container (#5006)           │ 2024-02-29 │\n│ salmon/quant │ nf-core/modules │ cb6b2b9     │ fix stubs salmon (#5517)               │ 2024-04-24 │\n│ trimgalore   │ nf-core/modules │ a984184     │ run nf-core lint on trimgalore (#5129) │ 2024-03-15 │\n└──────────────┴─────────────────┴─────────────┴────────────────────────────────────────┴────────────┘\n\n\n\n\n\n\nAdvanced Challenge\n\n\n\nHow would you look up previous versions of the module?\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nThere are a few ways to approach this.\n\nYou could hop onto github and search throught the git history for the main.nf of the particular module, to identify the git SHA and provide it to the --sha flag.\nYou could run the install command with the --prompt flag, as seen below\n\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nINFO     Module 'fastqc' is already installed.\n? Module fastqc is already installed.\nDo you want to force the reinstallation? Yes\n? Select 'fastqc' commit: (Use arrow keys)\n   Fix FastQC memory allocation (#5432) 285a50500f9e02578d90b3ce6382ea3c30216acd (installed version)\n   Update FASTQC to use unique names for snapshots (#4825) f4ae1d942bd50c5c0b9bd2de1393ce38315ba57c\n   CHORES: update fasqc tests with new data organisation (#4760) c9488585ce7bd35ccd2a30faa2371454c8112fb9\n   fix fastqc tests n snap (#4669) 617777a807a1770f73deb38c80004bac06807eef\n   Update version strings (#4556) 65ad3e0b9a4099592e1102e92e10455dc661cf53\n   Remove pytest-workflow tests for modules covered by nf-test (#4521) 3e8b0c1144ccf60b7848efbdc2be285ff20b49ee\n   Add conda environment names (#4327) 3f5420aa22e00bd030a2556dfdffc9e164ec0ec5\n   Fix conda declaration (#4252) 8fc1d24c710ebe1d5de0f2447ec9439fd3d9d66a\n   Move conda environment to yml (#4079) 516189e968feb4ebdd9921806988b4c12b4ac2dc\n   authors =&gt; maintainers (#4173) cfd937a668919d948f6fcbf4218e79de50c2f36f\n » older commits\n\n\n\n\n\n5.3.2 Updating nf-core modules\nAbove we got and error message for fastq because the module was already installed. As listed in the output, one of the suggested solutions is that we might be looking to update the module\nnf-core modules update fastqc\nAfter running the command you will find that you are prompted for whether you wish to view the differences between the current installation and the update.\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\n? Do you want to view diffs of the proposed changes? (Use arrow keys)\n » No previews, just update everything\n   Preview diff in terminal, choose whether to update files\n   Just write diffs to a patch file\nFor the sake of this exercise, we are going to roll fastqc back by one commit.\nIf you select the 2nd option Preview diff in terminal, choose whether to update files\nnf-core modules update fastqc -p\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\n? Do you want to view diffs of the proposed changes? Preview diff in terminal, choose whether to update files\n? Select 'fastqc' commit: (Use arrow keys)\n   Fix FastQC memory allocation (#5432) 285a50500f9e02578d90b3ce6382ea3c30216acd (installed version)\n » Update FASTQC to use unique names for snapshots (#4825) f4ae1d942bd50c5c0b9bd2de1393ce38315ba57c\n   CHORES: update fasqc tests with new data organisation (#4760) c9488585ce7bd35ccd2a30faa2371454c8112fb9\n   fix fastqc tests n snap (#4669) 617777a807a1770f73deb38c80004bac06807eef\n   Update version strings (#4556) 65ad3e0b9a4099592e1102e92e10455dc661cf53\n   Remove pytest-workflow tests for modules covered by nf-test (#4521) 3e8b0c1144ccf60b7848efbdc2be285ff20b49ee\n   Add conda environment names (#4327) 3f5420aa22e00bd030a2556dfdffc9e164ec0ec5\n   Fix conda declaration (#4252) 8fc1d24c710ebe1d5de0f2447ec9439fd3d9d66a\n   Move conda environment to yml (#4079) 516189e968feb4ebdd9921806988b4c12b4ac2dc\n   authors =&gt; maintainers (#4173) cfd937a668919d948f6fcbf4218e79de50c2f36f\n   older commits\n? Select 'fastqc' commit: Update FASTQC to use unique names for snapshots (#4825) f4ae1d942bd50c5c0b9bd2de1393ce38315ba57c\nINFO     Changes in module 'nf-core/fastqc' between (285a50500f9e02578d90b3ce6382ea3c30216acd) and (f4ae1d942bd50c5c0b9bd2de1393ce38315ba57c)\nINFO     Changes in 'fastqc/main.nf':\n --- modules/nf-core/fastqc/main.nf\n +++ modules/nf-core/fastqc/main.nf\n @@ -25,11 +25,6 @@\n      def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \"${prefix}.${reads.extension}\" ]] : reads.withIndex().collect { entry, index -&gt; [ entry, \"${prefix}_${index + 1}.${entry.extension}\" ] }\n      def rename_to = old_new_pairs*.join(' ').join(' ')\n      def renamed_files = old_new_pairs.collect{ old_name, new_name -&gt; new_name }.join(' ')\n -\n -    def memory_in_mb = MemoryUnit.of(\"${task.memory}\").toUnit('MB')\n -    // FastQC memory value allowed range (100 - 10000)\n -    def fastqc_memory = memory_in_mb &gt; 10000 ? 10000 : (memory_in_mb &lt; 100 ? 100 : memory_in_mb)\n -\n      \"\"\"\n      printf \"%s %s\\\\n\" $rename_to | while read old_name new_name; do\n          [ -f \"\\${new_name}\" ] || ln -s \\$old_name \\$new_name\n @@ -38,7 +33,6 @@\n      fastqc \\\\\n          $args \\\\\n          --threads $task.cpus \\\\\n -        --memory $fastqc_memory \\\\\n          $renamed_files\n\n      cat &lt;&lt;-END_VERSIONS &gt; versions.yml\nINFO     'modules/nf-core/fastqc/meta.yml' is unchanged\nINFO     'modules/nf-core/fastqc/environment.yml' is unchanged\nINFO     'modules/nf-core/fastqc/tests/main.nf.test.snap' is unchanged\nINFO     'modules/nf-core/fastqc/tests/tags.yml' is unchanged\nINFO     'modules/nf-core/fastqc/tests/main.nf.test' is unchanged\n? Update module 'fastqc'? No\nINFO     Updates complete ✨   \n\n\n5.3.3 Removing nf-core modules\nAs mentioned above, if you decide that you don’t need a module anymore, you can’t just remove the folder with rm -r.\nFor nf-core to no longer register the module is to be distributed with your pipeline you need to use:\nnf-core modules remove\nAs an exercise, we are going to install the samtools/sort module\nnf-core modules install samtools/sort\nQuickly view the modules.json or use nf-core modules list local to view the changes from installing the module.\nNow remove the samtools/sort module\nnf-core modules remove samtools/sort\n\n\n\n\n\n\nOverall Challenge\n\n\n\nNow add the include module statements to the our workflows/myrnaseq.nf\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\ninclude { FASTQC as FASTQC_one } from '../modules/nf-core/fastq/main' \ninclude { FASTQC as FASTQC_two } from '../modules/nf-core/fastq/main' \n\ninclude { TRIMGALORE } from '../modules/nf-core/trimgalore/main'\n\n\n\n\n\n5.3.4 Writing modules with nf-core template\nFor this section we are going to refer to the nf-core guidelines for modules.\nWhile these are the full guidelines for contributing back to nf-core, there are still some general components that are good practice even if you are NOT planning to contribute.\n\n\n\n\n\n\nSummary of guidelines\n\n\n\n\nAll required and optional input files must be included in the input as a path variable\nThe command should run without any additional argument, any required flag values should be included as an input val variable\ntask.ext.args must be provided as a variable\nWhere possible all input and output files should be compressed (i.e. fastq.gz and .bam)\nA versions.yml file is output\nNaming conventions include using all lowercase without puntuation and follows the convention of software/tool (i.e. bwa/mem)\nAll outputs must include an emit definition\n\n\n\nWe are going to write up our own samtools/view module.\nnf-core modules create \n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nINFO     Repository type: pipeline\nINFO     Press enter to use default values (shown in brackets) or type your own responses. ctrl+click underlined text to open links.\nName of tool/subtool: samtools/view\nINFO     Using Bioconda package: 'bioconda::samtools=1.20'\nINFO     Could not find a Docker/Singularity container (Unexpected response code `500` for https://api.biocontainers.pro/ga4gh/trs/v2/tools/samtools/versions/samtools-1.20) ## Cluster\nGitHub Username: (@author): @mmyeung\nINFO     Provide an appropriate resource label for the process, taken from the nf-core pipeline template.\n         For example: process_single, process_low, process_medium, process_high, process_long\n? Process resource label: process_low\nINFO     Where applicable all sample-specific information e.g. 'id', 'single_end', 'read_group' MUST be provided as an input via a Groovy Map called\n         'meta'. This information may not be required in some instances, for example indexing reference genome files.\nWill the module require a meta map of sample information? [y/n] (y): y\nINFO     Created component template: 'samtools/view'\nINFO     Created following files:\n           modules/local/samtools/view.nf\nAs we progressed through the interactive prompt, you will have noticed that nf-core always attempts to locate the corresponding bioconda package and singularity/Docker container.\n\n\n\n\n\n\nWhat happens when there is no bioconda package or container?\n\n\n\n\n\nnf-core modules create --author @mmyeung --label process_single --meta testscript\nThe command will indicate that the there is no bioconda package with the software name, and prompt you for a package name you might wish to use.\nINFO     Repository type: pipeline\nINFO     Press enter to use default values (shown in brackets) or type your own responses. ctrl+click underlined text to open links.\nWARNING  Could not find Conda dependency using the Anaconda API: 'testscript'\nDo you want to enter a different Bioconda package name? [y/n]: n\nWARNING  Could not find Conda dependency using the Anaconda API: 'testscript'\n         Building module without tool software and meta, you will need to enter this information manually.\nINFO     Created component template: 'testscript'\nINFO     Created following files:\n           modules/local/testscript.nf      \nwithin the module .nf script you will note that the definitions for the conda and container are incomplete for the tool.\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/YOUR-TOOL-HERE':\n        'biocontainers/YOUR-TOOL-HERE' }\"\nnf-core has a large cache of containers here. Though you can also provide a simple path to docker hub.\n    container \"mmyeung/trccustomunix:0.0.1\"\n\n\n\nThe resource labels, are those as defined in conf/base.config\n\n\n\n\n\n\nChallenge\n\n\n\nWrite up the inputs, outputs and script for samtools/view.\nAssume that all the inputs will be .bam and the outputs will also be .bam.\nFor reference look at the documentation for samtools/view\nAre there optional flags that take file inputs? What options need to set to ensure that the command runs without error?\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nprocess SAMTOOLS_VIEW {\n    tag \"$meta.id\"\n    label 'process_low'\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/samtools:1.20--h50ea8bc_0' :\n        'biocontainers/samtools:1.20--h50ea8bc_0' }\"\n\n    input:\n    tuple val(meta), path(input), path(index)\n    tuple val(meta2), path(fasta)\n    path bed\n    path qname\n\n    output:\n    tuple val(meta), path(\"*.bam\"),  emit: bam\n    path  \"versions.yml\",            emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def args2 = task.ext.args2 ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def reference = fasta ? \"--reference ${fasta}\" : \"\"\n    def readnames = qname ? \"--qname-file ${qname}\": \"\"\n    def regions = bed ? \"-L ${bed}\": \"\"\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n    \"\"\"\n    samtools \\\\\n        view \\\\\n        -hb \\\\\n        --threads ${task.cpus-1} \\\\\n        ${reference} \\\\\n        ${readnames} \\\\\n        ${regions} \\\\\n        $args \\\\\n        -o ${prefix}.bam \\\\\n        $input \\\\\n        $args2\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2&gt;&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    def file_type = args.contains(\"--output-fmt sam\") ? \"sam\" :\n                    args.contains(\"--output-fmt bam\") ? \"bam\" :\n                    args.contains(\"--output-fmt cram\") ? \"cram\" :\n                    input.getExtension()\n    if (\"$input\" == \"${prefix}.${file_type}\") error \"Input and output names are the same, use \\\"task.ext.prefix\\\" to disambiguate!\"\n\n    def index = args.contains(\"--write-index\") ? \"touch ${prefix}.csi\" : \"\"\n\n    \"\"\"\n    touch ${prefix}.${file_type}\n    ${index}\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        samtools: \\$(echo \\$(samtools --version 2&gt;&1) | sed 's/^.*samtools //; s/Using.*\\$//')\n    END_VERSIONS\n    \"\"\"\n\n\n\nSimilar to nf-core create you can minimise a the number of prompts by using optional flags.\n\n\n\n\n\n\nOverall Challenge\n\n\n\nWrite up the short workflow as discussed above\nFASTQC -&gt; trimgalore -&gt; FASTQC -&gt; MULTIQC"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core-subworkflow",
    "href": "workshops/5.1_nf_core_template.html#nf-core-subworkflow",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.4 Nf-core subworkflow",
    "text": "5.4 Nf-core subworkflow\nnf-core subworkflows\nor with\nnf-core subworkflows list remote\n\n5.4.1 Installing nf-core subworkflows\nSubworkflows can be updated/removed like modules\n\n\n\n\n\n\nChallenge\n\n\n\nInstall the subworkflow fastq_subsample_fq_salmon into the workflow\n\n\n\n\n\n\n\n\nCaution\n\n\n\n\n\nnf-core subworkflows install fastq_subsample_fq_salmon\n\n\n\n\n\n5.4.2 Writing subworkflows with nf-core template\n\n\n\n\n\n\nChallenge\n\n\n\nWrite up the QC_WF subworkflow from last week."
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#nf-core-schema-and-input-validation",
    "href": "workshops/5.1_nf_core_template.html#nf-core-schema-and-input-validation",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "5.5 Nf-core schema and input validation",
    "text": "5.5 Nf-core schema and input validation\nRelies on plugins written by nf-core community\nIn particular nf-validation\nnextflow_schmea.json is for pipeline parameters\nnf-core schema build\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.14.1 - https://nf-co.re\n\n\nINFO     [✓] Default parameters match schema validation\nINFO     [✓] Pipeline schema looks valid (found 32 params)\nINFO     Writing schema with 32 params: 'nextflow_schema.json'\n🚀  Launch web builder for customisation and editing? [y/n]: y\nINFO     Opening URL: https://nf-co.re/pipeline_schema_builder?id=1718112529_0841fa08f86f\nINFO     Waiting for form to be completed in the browser. Remember to click Finished when you're done.\n⢿ Use ctrl+c to stop waiting and force exit.\nRecommend writing in web browser\njson format details additional reading\n\n\n\n\n\n\nChallenge\n\n\n\nWe are going add the input parameter for the transcript.fa\nThen install salmon/index and write up quant_wf subworkflow from last week.git\n\n\n\n5.5.2 Nf-core inputs\nnested in this schema is the input or samplesheet schema. unfortunately there isn’t a nice interface to help you write this schema yet.\n\nmeta: Allows you to predesignate the “key” with in the “meta”\nrequired: value must be included\ndependency: value is dependant on other value existing in samplesheet (i.e. fastq_2 must imply there is a fastq_1)\n\n\n\n5.6 Nf-core tools for launching\ncreate-params-file\n\n\n5.7 Nf-core for pipeline management\nbump-version ==&gt; good software management to note down versions"
  },
  {
    "objectID": "workshops/5.1_nf_core_template.html#contributing-to-nf-core",
    "href": "workshops/5.1_nf_core_template.html#contributing-to-nf-core",
    "title": "Nextflow Development - Developing Modularised Workflows",
    "section": "Contributing to nf-core",
    "text": "Contributing to nf-core\nFull pipelines Please see the nf-core documentation for a full walkthrough of how to create a new nf-core workflow. — This workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, Nextflow Patterns materials from Nextflow, nf-core nf-core tools documentation and nf-validation"
  },
  {
    "objectID": "workshops/2.2_troubleshooting.html",
    "href": "workshops/2.2_troubleshooting.html",
    "title": "Troubleshooting Nextflow run",
    "section": "",
    "text": "2.2.1. Nextflow log\nIt is important to keep a record of the commands you have run to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow log command:\nnextflow log\nThe log command has multiple options to facilitate the queries and is especially useful while debugging a workflow and inspecting execution metadata. You can view all of the possible log options with -h flag:\nnextflow log -h\nTo query a specific execution you can use the RUN NAME or a SESSION ID:\nnextflow log &lt;run name&gt;\nTo get more information, you can use the -f option with named fields. For example:\nnextflow log &lt;run name&gt; -f process,hash,duration\nThere are many other fields you can query. You can view a full list of fields with the -l option:\nnextflow log -l\n\n\n\n\n\n\nChallenge\n\n\n\nUse the log command to view with process, hash, and script fields for your tasks from your most recent Nextflow execution.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the log command to get a list of you recent executions:\nnextflow log\nTIMESTAMP           DURATION    RUN NAME            STATUS  REVISION ID SESSION ID                              COMMAND \n2023-11-21 22:43:14 14m 17s     jovial_angela       OK      3bec2331ca  319751c3-25a6-4085-845c-6da28cd771df    nextflow run nf-core/rnaseq\n2023-11-21 23:05:49 1m 36s      marvelous_shannon   OK      3bec2331ca  319751c3-25a6-4085-845c-6da28cd771df    nextflow run nf-core/rnaseq\n2023-11-21 23:10:00 1m 35s      deadly_babbage      OK      3bec2331ca  319751c3-25a6-4085-845c-6da28cd771df    nextflow run nf-core/rnaseq\nQuery the process, hash, and script using the -f option for the most recent run:\nnextflow log marvelous_shannon -f process,hash,script\n\n[... truncated ...]\n\nNFCORE_RNASEQ:RNASEQ:SUBREAD_FEATURECOUNTS  7c/f936d4   \n    featureCounts \\\n        -B -C -g gene_biotype -t exon \\\n        -p \\\n        -T 2 \\\n        -a chr22_with_ERCC92.gtf \\\n        -s 2 \\\n        -o HBR_Rep1_ERCC.featureCounts.txt \\\n        HBR_Rep1_ERCC.markdup.sorted.bam\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"NFCORE_RNASEQ:RNASEQ:SUBREAD_FEATURECOUNTS\":\n        subread: $( echo $(featureCounts -v 2&gt;&1) | sed -e \"s/featureCounts v//g\")\n    END_VERSIONS\n\n[... truncated ... ]\n\nNFCORE_RNASEQ:RNASEQ:MULTIQC    7a/8449d7   \n    multiqc \\\n        -f \\\n         \\\n         \\\n        .\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"NFCORE_RNASEQ:RNASEQ:MULTIQC\":\n        multiqc: $( multiqc --version | sed -e \"s/multiqc, version //g\" )\n    END_VERSIONS\n    \n\n\n\n\n\n2.2.2. Execution cache and resume\nTask execution caching is an essential feature of modern workflow managers. As such, Nextflow provides an automated caching mechanism for every execution. When using the Nextflow -resume option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks.\nNextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID’s are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you.\nYou can re-launch the previously executed nf-core/rnaseq workflow again, but with a -resume flag, and observe the progress. Notice the time it takes to complete the workflow.\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    --input samplesheet.csv \\\n    --outdir ./my_results \\\n    --fasta $materials/ref/chr22_with_ERCC92.fa \\\n    --gtf $materials/ref/chr22_with_ERCC92.gtf \\\n    -profile singularity \\\n    --skip_markduplicates true \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \n\n[80/ec6ff8] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF2BED (chr22_with_ERCC92.gtf)                  [100%] 1 of 1, cached: 1 ✔\n[1a/7bec9c] process &gt; NFCORE_RNASEQ:RNASEQ:PREPARE_GENOME:GTF_GENE_FILTER (chr22_with_ERCC92.fa)           [100%] 1 of 1, cached: 1 ✔\nExecuting this workflow will create a my_results directory with selected results files and add some further sub-directories into the work directory\nIn the schematic above, the hexadecimal numbers, such as 80/ec6ff8, identify the unique task execution. These numbers are also the prefix of the work directories where each task is executed.\nYou can inspect the files produced by a task by looking inside the work directory and using these numbers to find the task-specific execution path:\nls work/80/ec6ff8ba69a8b5b8eede3679e9f978/\nIf you look inside the work directory of a FASTQC task, you will find the files that were staged and created when this task was executed:\n&gt;&gt;&gt; ls -la  work/e9/60b2e80b2835a3e1ad595d55ac5bf5/ \n\ntotal 15895\ndrwxrwxr-x 2 rlupat rlupat    4096 Nov 22 03:39 .\ndrwxrwxr-x 4 rlupat rlupat    4096 Nov 22 03:38 ..\n-rw-rw-r-- 1 rlupat rlupat       0 Nov 22 03:39 .command.begin\n-rw-rw-r-- 1 rlupat rlupat    9509 Nov 22 03:39 .command.err\n-rw-rw-r-- 1 rlupat rlupat    9609 Nov 22 03:39 .command.log\n-rw-rw-r-- 1 rlupat rlupat     100 Nov 22 03:39 .command.out\n-rw-rw-r-- 1 rlupat rlupat   10914 Nov 22 03:39 .command.run\n-rw-rw-r-- 1 rlupat rlupat     671 Nov 22 03:39 .command.sh\n-rw-rw-r-- 1 rlupat rlupat     231 Nov 22 03:39 .command.trace\n-rw-rw-r-- 1 rlupat rlupat       1 Nov 22 03:39 .exitcode\nlrwxrwxrwx 1 rlupat rlupat      63 Nov 22 03:39 HBR_Rep1_ERCC_1.fastq.gz -&gt; HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz\n-rw-rw-r-- 1 rlupat rlupat    2368 Nov 22 03:39 HBR_Rep1_ERCC_1.fastq.gz_trimming_report.txt\n-rw-rw-r-- 1 rlupat rlupat  697080 Nov 22 03:39 HBR_Rep1_ERCC_1_val_1_fastqc.html\n-rw-rw-r-- 1 rlupat rlupat  490526 Nov 22 03:39 HBR_Rep1_ERCC_1_val_1_fastqc.zip\n-rw-rw-r-- 1 rlupat rlupat 6735205 Nov 22 03:39 HBR_Rep1_ERCC_1_val_1.fq.gz\nlrwxrwxrwx 1 rlupat rlupat      63 Nov 22 03:39 HBR_Rep1_ERCC_2.fastq.gz -&gt; HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz\n-rw-rw-r-- 1 rlupat rlupat    2688 Nov 22 03:39 HBR_Rep1_ERCC_2.fastq.gz_trimming_report.txt\n-rw-rw-r-- 1 rlupat rlupat  695591 Nov 22 03:39 HBR_Rep1_ERCC_2_val_2_fastqc.html\n-rw-rw-r-- 1 rlupat rlupat  485732 Nov 22 03:39 HBR_Rep1_ERCC_2_val_2_fastqc.zip\n-rw-rw-r-- 1 rlupat rlupat 7088948 Nov 22 03:39 HBR_Rep1_ERCC_2_val_2.fq.gz\nlrwxrwxrwx 1 rlupat rlupat     102 Nov 22 03:39 HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz -&gt; /data/seqliner/test-data/rna-seq/fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz\nlrwxrwxrwx 1 rlupat rlupat     102 Nov 22 03:39 HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz -&gt; /data/seqliner/test-data/rna-seq/fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz\n-rw-rw-r-- 1 rlupat rlupat     109 Nov 22 03:39 versions.yml\nThe FASTQC process runs twice, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [e9/60b2e8] represents just one of the four sets of input data that was processed.\nIt’s very likely you will execute a workflow multiple times as you find the parameters that best suit your data. You can save a lot of spaces (and time) by resuming a workflow from the last step that was completed successfully and/or unmodified.\nIn practical terms, the workflow is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are used as the process results.\nNotably, the -resume functionality is very sensitive. Even touching a file in the work directory can invalidate the cache.\n\n\n\n\n\n\nChallenge\n\n\n\nInvalidate the cache by touching a .fastq.gz file in a FASTQC task work directory (you can use the touch command). Execute the workflow again with the -resume option to show that the cache has been invalidated.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nExecute the workflow for the first time (if you have not already).\nUse the task ID shown for the FASTQC process and use it to find and touch a the sample1_R1.fastq.gz file:\ntouch work/ff/21abfa87cc7cdec037ce4f36807d32/HBR_Rep1_ERCC_1.fastq.gz\nExecute the workflow again with the -resume command option:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    --input samplesheet.csv \\\n    --outdir ./my_results \\\n    --fasta $materials/ref/chr22_with_ERCC92.fa \\\n    --gtf $materials/ref/chr22_with_ERCC92.gtf \\\n    -profile singularity \\\n    --skip_markduplicates true \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \nYou should see that some task were invalid and were executed again.\nWhy did this happen?\nIn this example, the cache of two FASTQC tasks were invalid. The fastq file we touch is used by in the pipeline in multiple places. Thus, touching the symlink for this file and changing the date of last modification disrupted two task executions.\n\n\n\n\n\n2.2.3. Troubleshoot warning and error messages\nWhile our previous workflow execution completed successfully, there were a couple of warning messages that may be cause for concern:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 20-Nov-2023 00:29:04\nDuration    : 10m 15s\nCPU hours   : 0.3 \nSucceeded   : 72\n\n\n\n\n\n\nHandling dodgy error messages 🤬\n\n\n\nThe first warning message isn’t very descriptive (see this pull request). You might come across issues like this when running nf-core pipelines, too. Bug reports and user feedback is very important to open source software communities like nf-core. If you come across any issues, submit a GitHub issue or start a discussion in the relevant nf-core Slack channel so others are aware and it can be addressed by the pipeline’s developers.\n\n\n➤ Take a look at the MultiQC report, as directed by the second message. You can find the MultiQC report in the lesson2.1/ directory:\nls -la lesson2.1/multiqc/star_salmon/\ntotal 1402\ndrwxrwxr-x 4 rlupat rlupat    4096 Nov 22 00:29 .\ndrwxrwxr-x 3 rlupat rlupat    4096 Nov 22 00:29 ..\ndrwxrwxr-x 2 rlupat rlupat    8192 Nov 22 00:29 multiqc_data\ndrwxrwxr-x 5 rlupat rlupat    4096 Nov 22 00:29 multiqc_plots\n-rw-rw-r-- 1 rlupat rlupat 1419998 Nov 22 00:29 multiqc_report.html\n➤ Download the multiqc_report.html the file navigator panel on the left side of your VS Code window by right-clicking on it and then selecting Download. Open the file on your computer.\nTake a look a the section labelled WARNING: Fail Strand Check\nThe warning we have received is indicating that the read strandedness we specified in our samplesheet.csv and inferred strandedness identified by the RSeqQC process in the pipeline do not match. It looks like the test samplesheet have incorrectly specified strandedness as forward in the samplesheet.csv when our raw reads actually show an equal distribution of sense and antisense reads.\nFor those who are not familiar with RNAseq data, incorrectly specified strandedness may negatively impact the read quantification step (process: Salmon quant) and give us inaccurate results. So, let’s clarify how the Salmon quant process is gathering strandedness information for our input files by default and find a way to address this with the parameters provided by the nf-core/rnaseq pipeline.\n\n\n\n2.2.4. Identify the run command for a process\nTo observe exactly what command is being run for a process, we can attempt to infer this information from the module’s main.nf script in the modules/ directory. However, given all the different parameters that may be applied at the process level, this may not be very clear.\n➤ Take a look at the Salmon quant main.nf file:\nnf-core-rnaseq-3.11.1/workflow/modules/nf-core/salmon/quant/main.nf\nUnless you are familiar with developing nf-core pipelines, it can be very hard to see what is actually happening in the code, given all the different variables and conditional arguments inside this script. Above the script block we can see strandedness is being applied using a few different conditional arguments. Instead of trying to infer how the $strandedness variable is being defined and applied to the process, let’s use the hidden command files saved for this task in the work/ directory.\n\n\n\n\n\n\nHidden files in the work directory!\n\n\n\nRemember that the pipeline’s results are cached in the work directory. In addition to the cached files, each task execution directories inside the work directory contains a number of hidden files:\n\n.command.sh: The command script run for the task.\n.command.run: The command wrapped used to run the task.\n.command.out: The task’s standard output log.\n.command.err: The task’s standard error log.\n.command.log: The wrapper execution output.\n.command.begin: A file created as soon as the job is launched.\n.exitcode: A file containing the task exit code (0 if successful)\n\n\n\nWith nextflow log command that we discussed previously, there are multiple options to facilitate the queries and is especially useful while debugging a pipeline and while inspecting pipeline execution metadata.\nTo understand how Salmon quant is interpreting strandedness, we’re going to use this command to track down the hidden .command.sh scripts for each Salmon quant task that was run. This will allow us to find out how Salmon quant handles strandedness and if there is a way for us to override this.\n➤ Use the Nextflow log command to get the unique run name information of the previously executed pipelines:\nnextflow log &lt;run-name&gt;\nThat command will list out all the work subdirectories for all processes run.\nAnd we now need to find the specific hidden.command.sh for Salmon tasks. But how to find them? 🤔\n➤ Let’s add some custom bash code to query a Nextflow run with the run name from the previous lesson. First, save your run name in a bash variable. For example:\nrun_name=marvelous_shannon\n➤ And let’s save the tool of interest (salmon) in another bash variable to pull it from a run command:\ntool=salmon\n➤ Next, run the following bash command:\nnextflow log ${run_name} | while read line;\n    do\n      cmd=$(ls ${line}/.command.sh 2&gt;/dev/null);\n      if grep -q $tool $cmd;\n      then  \n        echo $cmd;     \n      fi; \n    done \nThat will list all process .command.sh scripts containing ‘salmon’. There are a few different processes that run Salmon to perform other steps in the workflow. We are looking for Salmon quant which performs the read quantification:\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/57/fba8f9a2385dac5fa31688ba1afa9b/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/30/0113a58c14ca8d3099df04ebf388f3/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/ec/95d6bd12d578c3bce22b5de4ed43fe/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/49/6fedcb09e666432ae6ddf8b1e8f488/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/b4/2ca8d05b049438262745cde92955e9/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/38/875d68dae270504138bb3d72d511a7/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/72/776810a99695b1c114cbb103f4a0e6/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/1c/dc3f54cc7952bf55e6742dd4783392/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/f3/5116a5b412bde7106645671e4c6ffb/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/17/fb0c791810f42a438e812d5c894ebf/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/4c/931a9b60b2f3cf770028854b1c673b/.command.sh\n/scratch/users/rlupat/nfWorkshop/lesson2.1/work/91/e1c99d8acb5adf295b37fd3bbc86a5/.command.sh\nCompared with the salmon quant main.nf file, we get a lot more fine scale details from the .command.sh process scripts:\n&gt;&gt;&gt; cat main.nf\nsalmon quant \\\\\n        --geneMap $gtf \\\\\n        --threads $task.cpus \\\\\n        --libType=$strandedness \\\\\n        $reference \\\\\n        $input_reads \\\\\n        $args \\\\\n        -o $prefix\n&gt;&gt;&gt; cat .command.sh\nsalmon quant \\\n    --geneMap chr22_with_ERCC92.gtf \\\n    --threads 2 \\\n    --libType=ISF \\\n    -t genome.transcripts.fa \\\n    -a HBR_Rep1_ERCC.Aligned.toTranscriptome.out.bam \\\n     \\\n    -o HBR_Rep1_ERCC\nLooking at the nf-core/rnaseq Parameter documentation and Salmon documentation, we found that we can override this default using the --salmon_quant_libtype A parameter to indicate our data is unstranded and override samplesheet.csv input.\n\n\n\n\n\n\nHow do I get rid of the strandedness check warning message?\n\n\n\nIf we want to get rid of the warning message Please check MultiQC report: 2/2 samples failed strandedness check, we’ll have to change the strandedness fields in our samplesheet.csv. Keep in mind, doing this will invalidate the pipeline’s cache and cause the pipeline to run from the beginning.\n\n\n\n\n\n2.2.5. Write a parameter file\nFrom the previous section we learn that Nextflow accepts either yaml or json formats for parameter files. Any of the pipeline-specific parameters can be supplied to a Nextflow pipeline in this way.\n\n\n\n\n\n\nChallenge\n\n\n\nFill in the parameters file below and save as workshop-params.yaml. This time, include the --salmon_quant_libtype A parameter.\n💡 YAML formatting tips!\n\nStrings need to be inside double quotes\nBooleans (true/false) and numbers do not require quotes\n\ninput: \"\"\noutdir: \"lesson2.2\"\nfasta: \"\"\ngtf: \"\"\nstar_index: \"\"\nsalmon_index: \"\"\nskip_markduplicates: \nsave_trimmed: \nsave_unaligned: \nsalmon_quant_libtype: \"A\" \n\n\n\n\n2.2.6. Apply the parameter file\n➤ Once your params file has been saved, run:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    -params-file workshop-params.yaml\n    -profile singularity \\\n    --max_memory '6.GB' \\\n    --max_cpus 2 \\\n    -resume \nThe number of pipeline-specific parameters we’ve added to our run command has been significantly reduced. The only -- parameters we’ve provided to the run command relate to how the pipeline is executed on our interative job. These resource limits won’t be applicable to others who will run the pipeline on a different infrastructure.\nAs the workflow runs a second time, you will notice 4 things:\n\nThe command is much tidier thanks to offloading some parameters to the params file\nThe -resume flag. Nextflow has lots of run options including the ability to use cached output!\nSome processes will be pulled from the cache. These processes remain unaffected by our addition of a new parameter.\n\nThis run of the pipeline will complete in a much shorter time.\n\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 21-Apr-2023 05:58:06\nDuration    : 1m 51s\nCPU hours   : 0.3 (82.2% cached)\nSucceeded   : 11\nCached      : 55\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html",
    "href": "workshops/8.1_scatter_gather_output.html",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of how to structure nextflow published outputs\nGain an understanding of how to do scatter & gather processes"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#environment-setup",
    "href": "workshops/8.1_scatter_gather_output.html#environment-setup",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\nThe training data can be cloned from:\ngit clone https://github.com/nextflow-io/training.git"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#rna-seq-workflow-and-module-files",
    "href": "workshops/8.1_scatter_gather_output.html#rna-seq-workflow-and-module-files",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "RNA-seq Workflow and Module Files ",
    "text": "RNA-seq Workflow and Module Files \nPreviously, we created three Nextflow files and one config file:\n├── nextflow.config\n├── rnaseq.nf\n├── modules.nf\n└── modules\n    └── trimgalore.nf\n\nrnaseq.nf: main workflow script where parameters are defined and processes were called.\n\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nmodules.nf: script containing the majority of modules, including INDEX, QUANTIFICATION, FASTQC, and MULTIQC\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nmodules/trimgalore.nf: script inside a modules folder, containing only the TRIMGALORE process\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}\n\nnextflow.config: config file that enables singularity\n\nsingularity {\n  enabled = true\n  autoMounts = true\n  cacheDir = \"/config/binaries/singularity/containers_devel/nextflow\"\n}\nRun the pipeline, specifying --outdir:\n&gt;&gt;&gt; nextflow run rnaseq.nf --outdir output\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [soggy_jennings] DSL2 - revision: 87afc1d98d\nexecutor &gt;  local (16)\n[93/d37ef0] process &gt; INDEX          [100%] 1 of 1 ✔\n[b3/4c4d9c] process &gt; QT (1)         [100%] 3 of 3 ✔\n[d0/173a6e] process &gt; FASTQC_one (3) [100%] 3 of 3 ✔\n[58/0b8af2] process &gt; TRIMGALORE (3) [100%] 3 of 3 ✔\n[c6/def175] process &gt; FASTQC_two (3) [100%] 3 of 3 ✔\n[e0/bcf904] process &gt; MULTIQC (3)    [100%] 3 of 3 ✔"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#organise-outputs",
    "href": "workshops/8.1_scatter_gather_output.html#organise-outputs",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.1. Organise outputs",
    "text": "8.1. Organise outputs\nThe output declaration block defines the channels used by the process to send out the results produced. However, this output only stays in the work/ directory if there is no publishDir directive specified.\nGiven each task is being executed in separate temporary work/ folder (e.g., work/f1/850698…), you may want to save important, non-intermediary, and/or final files in a results folder.\nTo store our workflow result files, you need to explicitly mark them using the directive publishDir in the process that’s creating the files. For example:\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\nThe above example will copy all html files created by the MULTIQC process into the directory path specified in the params.outdir"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#store-outputs-matching-a-glob-pattern",
    "href": "workshops/8.1_scatter_gather_output.html#store-outputs-matching-a-glob-pattern",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.1.1. Store outputs matching a glob pattern",
    "text": "8.1.1. Store outputs matching a glob pattern\nYou can use more than one publishDir to keep different outputs in separate directories. For each directive specify a different glob pattern using the pattern option to store into each directory only the files that match the provided pattern.\nFor example:\nreads_ch = Channel.fromFilePairs('data/ggal/*_{1,2}.fq')\n\nprocess FOO {\n    publishDir \"results/bam\", pattern: \"*.bam\"\n    publishDir \"results/bai\", pattern: \"*.bai\"\n\n    input:\n    tuple val(sample_id), path(sample_id_paths)\n\n    output:\n    tuple val(sample_id), path(\"*.bam\")\n    tuple val(sample_id), path(\"*.bai\")\n\n    script:\n    \"\"\"\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bam\n    echo your_command_here --sample $sample_id_paths &gt; ${sample_id}.bai\n    \"\"\"\n}\nExercise\nUse publishDir and pattern to keep the outputs from the trimgalore.nf into separate directories.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n  publishDir \"$params.outdir/report\", mode: \"copy\", pattern:\"*report.txt\"\n  publishDir \"$params.outdir/trimmed_fastq\", mode: \"copy\", pattern:\"*fq.gz\"\n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}\nOutput should now look like\n&gt;&gt;&gt; tree ./output\n./output\n├── gut.html\n├── liver.html\n├── lung.html\n├── report\n│   ├── gut_1.fq_trimming_report.txt\n│   ├── gut_2.fq_trimming_report.txt\n│   ├── liver_1.fq_trimming_report.txt\n│   ├── liver_2.fq_trimming_report.txt\n│   ├── lung_1.fq_trimming_report.txt\n│   └── lung_2.fq_trimming_report.txt\n└── trimmed_fastq\n    ├── gut_1_val_1.fq.gz\n    ├── gut_2_val_2.fq.gz\n    ├── liver_1_val_1.fq.gz\n    ├── liver_2_val_2.fq.gz\n    ├── lung_1_val_1.fq.gz\n    └── lung_2_val_2.fq.gz\n\n2 directories, 15 files"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#store-outputs-renaming-files-or-in-a-sub-directory",
    "href": "workshops/8.1_scatter_gather_output.html#store-outputs-renaming-files-or-in-a-sub-directory",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.1.2. Store outputs renaming files or in a sub-directory",
    "text": "8.1.2. Store outputs renaming files or in a sub-directory\nThe publishDir directive also allow the use of saveAs option to give each file a name of your choice, providing a custom rule as a closure.\nprocess foo {\n  publishDir 'results', saveAs: { filename -&gt; \"foo_$filename\" }\n\n  output: \n  path '*.txt'\n\n  '''\n  touch this.txt\n  touch that.txt\n  '''\n}\nThe same pattern can be used to store specific files in separate directories depending on the actual name.\nprocess foo {\n  publishDir 'results', saveAs: { filename -&gt; filename.endsWith(\".zip\") ? \"zips/$filename\" : filename }\n\n  output: \n  path '*'\n\n  '''\n  touch this.txt\n  touch that.zip\n  '''\n}\nExercise\nModify the MULTIQC output with saveAs such that resulting folder is as follow:\n./output\n├── MultiQC\n│   ├── multiqc_gut.html\n│   ├── multiqc_liver.html\n│   └── multiqc_lung.html\n├── report\n│   ├── gut_1.fq_trimming_report.txt\n│   ├── gut_2.fq_trimming_report.txt\n│   ├── liver_1.fq_trimming_report.txt\n│   ├── liver_2.fq_trimming_report.txt\n│   ├── lung_1.fq_trimming_report.txt\n│   └── lung_2.fq_trimming_report.txt\n└── trimmed_fastq\n    ├── gut_1_val_1.fq.gz\n    ├── gut_2_val_2.fq.gz\n    ├── liver_1_val_1.fq.gz\n    ├── liver_2_val_2.fq.gz\n    ├── lung_1_val_1.fq.gz\n    └── lung_2_val_2.fq.gz\n\n3 directories, 15 files\n\n\n\n\n\n\nWarning\n\n\n\nYou need to remove existing output folder/files if you want to have a clean output. By default, nextflow will overwrite existing files, and keep all the remaining files in the same specified output directory.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy', saveAs: { filename -&gt; filename.endsWith(\".html\") ? \"MultiQC/multiqc_$filename\" : filename }\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\n\n\nChallenge\nModify all the processes in rnaseq.nf such that we will have the following output structure\n./output\n├── gut\n│   ├── QC\n│   │   ├── fastqc_gut_logs\n│   │   │   ├── gut_1_fastqc.html\n│   │   │   ├── gut_1_fastqc.zip\n│   │   │   ├── gut_2_fastqc.html\n│   │   │   └── gut_2_fastqc.zip\n│   │   └── gut.html\n│   ├── report\n│   │   ├── gut_1.fq_trimming_report.txt\n│   │   └── gut_2.fq_trimming_report.txt\n│   └── trimmed_fastq\n│       ├── gut_1_val_1.fq.gz\n│       └── gut_2_val_2.fq.gz\n├── liver\n│   ├── QC\n│   │   ├── fastqc_liver_logs\n│   │   │   ├── liver_1_fastqc.html\n│   │   │   ├── liver_1_fastqc.zip\n│   │   │   ├── liver_2_fastqc.html\n│   │   │   └── liver_2_fastqc.zip\n│   │   └── liver.html\n│   ├── report\n│   │   ├── liver_1.fq_trimming_report.txt\n│   │   └── liver_2.fq_trimming_report.txt\n│   └── trimmed_fastq\n│       ├── liver_1_val_1.fq.gz\n│       └── liver_2_val_2.fq.gz\n└── lung\n    ├── QC\n    │   ├── fastqc_lung_logs\n    │   │   ├── lung_1_fastqc.html\n    │   │   ├── lung_1_fastqc.zip\n    │   │   ├── lung_2_fastqc.html\n    │   │   └── lung_2_fastqc.zip\n    │   └── lung.html\n    ├── report\n    │   ├── lung_1.fq_trimming_report.txt\n    │   └── lung_2.fq_trimming_report.txt\n    └── trimmed_fastq\n        ├── lung_1_val_1.fq.gz\n        └── lung_2_val_2.fq.gz\n\n15 directories, 27 files\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess FASTQC {\n    publishDir \"$params.outdir/$sample_id/QC\", mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    //publishDir params.outdir, mode:'copy', saveAs: { filename -&gt; filename.endsWith(\".html\") ? \"MultiQC/multiqc_$filename\" : filename }\n    publishDir \"$params.outdir/$quantification/QC\", mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img'\n  publishDir \"${params.outdir}/${sample_id}/report\", mode: \"copy\", pattern:\"*report.txt\"\n  publishDir \"${params.outdir}/${sample_id}/trimmed_fastq\", mode: \"copy\", pattern:\"*fq.gz\"\n\n  input:\n    tuple val(sample_id), path(reads)\n\n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#scatter",
    "href": "workshops/8.1_scatter_gather_output.html#scatter",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.2 Scatter",
    "text": "8.2 Scatter\nThe scatter operation involves distributing large input data into smaller chunks that can be analysed across multiple processes in parallel.\nOne very simple example of native scatter is how nextflow handles Channel factories with the Channel.fromPath or Channel.fromFilePairs method, where multiple input data is processed in parallel.\nparams.reads = \"/scratch/users/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { FASTQC as FASTQC_one } from './modules.nf'\n\nworkflow {\n  fastqc_ch = FASTQC_one(reads_ch)\n}\nFrom the above snippet from our rnaseq.nf, we will get three execution of FASTQC_one for each pairs of our input data.\nOther than natively splitting execution by input data, Nextflow also provides operators to scatter existing input data for various benefits, such as faster processing. For example:\n\nsplitText\nsplitFasta\nsplitFastq\nmap with from or fromList\nflatten"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#process-per-file-chunk",
    "href": "workshops/8.1_scatter_gather_output.html#process-per-file-chunk",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.2.1 Process per file chunk",
    "text": "8.2.1 Process per file chunk\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess count_line {\n  debug true\n  input: \n  file x\n\n  script:\n  \"\"\"\n  wc -l $x \n  \"\"\"\n}\n\nworkflow {\n  Channel.fromPath(params.infile) \\\n    | splitText(by: params.size, file: true) \\\n    | count_line\n}\nExercise\nparams.infile = \"/scratch/users/rlupat/nfWorkshop/dev1/training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.size = 1000\n\nworkflow {\n  Channel.fromFilePairs(params.infile, flat: true) \\\n    | splitFastq(by: params.size, pe: true, file: true) \\\n    | view()\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#process-per-file-range",
    "href": "workshops/8.1_scatter_gather_output.html#process-per-file-range",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.2.1 Process per file range",
    "text": "8.2.1 Process per file range\nExercise\nChannel.from(1..22) \\\n   | map { chr -&gt; [\"sample${chr}\", file(\"${chr}.indels.vcf\"), file(\"${chr}.vcf\")] } \\\n   | view()\n&gt;&gt; nextflow run test_scatter.nf\n\n[sample1, /scratch/users/${users}/1.indels.vcf, /scratch/users/${users}/1.vcf]\n[sample2, /scratch/users/${users}/2.indels.vcf, /scratch/users/${users}/2.vcf]\n[sample3, /scratch/users/${users}/3.indels.vcf, /scratch/users/${users}/3.vcf]\n[sample4, /scratch/users/${users}/4.indels.vcf, /scratch/users/${users}/4.vcf]\n[sample5, /scratch/users/${users}/5.indels.vcf, /scratch/users/${users}/5.vcf]\n[sample6, /scratch/users/${users}/6.indels.vcf, /scratch/users/${users}/6.vcf]\n[sample7, /scratch/users/${users}/7.indels.vcf, /scratch/users/${users}/7.vcf]\n[sample8, /scratch/users/${users}/8.indels.vcf, /scratch/users/${users}/8.vcf]\n[sample9, /scratch/users/${users}/9.indels.vcf, /scratch/users/${users}/9.vcf]\n[sample10, /scratch/users${users}/10.indels.vcf, /scratch/users${users}/10.vcf]\n[sample11, /scratch/users${users}/11.indels.vcf, /scratch/users${users}/11.vcf]\n[sample12, /scratch/users${users}/12.indels.vcf, /scratch/users${users}/12.vcf]\n[sample13, /scratch/users${users}/13.indels.vcf, /scratch/users${users}/13.vcf]\n[sample14, /scratch/users${users}/14.indels.vcf, /scratch/users${users}/14.vcf]\n[sample15, /scratch/users${users}/15.indels.vcf, /scratch/users${users}/15.vcf]\n[sample16, /scratch/users${users}/16.indels.vcf, /scratch/users${users}/16.vcf]\n[sample17, /scratch/users${users}/17.indels.vcf, /scratch/users${users}/17.vcf]\n[sample18, /scratch/users${users}/18.indels.vcf, /scratch/users${users}/18.vcf]\n[sample19, /scratch/users${users}/19.indels.vcf, /scratch/users${users}/19.vcf]\n[sample20, /scratch/users${users}/20.indels.vcf, /scratch/users${users}/20.vcf]\n[sample21, /scratch/users${users}/21.indels.vcf, /scratch/users${users}/21.vcf]\n[sample22, /scratch/users${users}/22.indels.vcf, /scratch/users${users}/22.vcf]\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess split_bed_by_chr {\n  debug true\n\n  input:\n  path bed\n  val chr\n\n  output:\n  path \"*.bed\"\n\n  script:\n  \"\"\"\n  grep ^${chr}\\t ${bed} &gt; ${chr}.bed\n  \"\"\"\n}\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22)) | view()\n}\nChallenge\nHow do we include chr X and Y into the above split by chromosome?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22,'X','Y').flatten()) | view()\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#gather",
    "href": "workshops/8.1_scatter_gather_output.html#gather",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.3 Gather",
    "text": "8.3 Gather\nThe gather operation consolidates results from parallel computations (can be from scatter) into a centralized process for aggregation and further processing.\nSome of the Nextflow provided operators that facilitate this gather operation, include:\n\ncollect\ncollectFile\nmap + groupTuple"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#process-all-outputs-altogether",
    "href": "workshops/8.1_scatter_gather_output.html#process-all-outputs-altogether",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.3.1. Process all outputs altogether",
    "text": "8.3.1. Process all outputs altogether\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess split_bed_by_chr {\n  debug true\n\n  input:\n  path bed\n  val chr\n\n  output:\n  path \"*.bed\"\n\n  script:\n  \"\"\"\n  grep ^${chr}\\t ${bed} &gt; ${chr}.bed\n  \"\"\"\n}\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22,'X','Y').flatten()) | collect | view()\n}"
  },
  {
    "objectID": "workshops/8.1_scatter_gather_output.html#collect-outputs-into-a-file",
    "href": "workshops/8.1_scatter_gather_output.html#collect-outputs-into-a-file",
    "title": "Nextflow Development - Outputs, Scatter, and Gather",
    "section": "8.3.2. Collect outputs into a file",
    "text": "8.3.2. Collect outputs into a file\nExercise\nparams.infile = \"/data/reference/bed_files/Agilent_CRE_v2/S30409818_Covered_MERGED.bed\"\nparams.size = 100000\n\nprocess split_bed_by_chr {\n  debug true\n\n  input:\n  path bed\n  val chr\n\n  output:\n  path \"*.bed\"\n\n  script:\n  \"\"\"\n  grep ^${chr}\\t ${bed} &gt; ${chr}.bed\n  \"\"\"\n}\n\nworkflow {\n    split_bed_by_chr(params.infile, Channel.from(1..22,'X','Y').flatten()) | collectFile(name: 'merged.bed', newLine:true) | view()\n}\nExercise\nworkflow {\n  Channel.fromPath(\"/scratch/users/rlupat/nfWorkshop/dev1/training/nf-training/data/ggal/*_1.fq\", checkIfExists: true) \\\n    | collectFile(name: 'combined_1.fq', newLine:true) \\\n    | view\n}"
  },
  {
    "objectID": "workshops/6.1_operators.html",
    "href": "workshops/6.1_operators.html",
    "title": "Nextflow Development - Channel Operators",
    "section": "",
    "text": "Objectives\n\n\n\n\nGain an understanding of Nextflow channel operators"
  },
  {
    "objectID": "workshops/6.1_operators.html#environment-setup",
    "href": "workshops/6.1_operators.html#environment-setup",
    "title": "Nextflow Development - Channel Operators",
    "section": "Environment Setup",
    "text": "Environment Setup\nSet up an interactive shell to run our Nextflow workflow:\nsrun --pty -p prod_short --mem 8GB --mincpus 2 -t 0-2:00 bash\nLoad the required modules to run Nextflow:\nmodule load nextflow/23.04.1\nmodule load singularity/3.7.3\nSet the singularity cache environment variable:\nexport NXF_SINGULARITY_CACHEDIR=/config/binaries/singularity/containers_devel/nextflow\nSingularity images downloaded by workflow executions will now be stored in this directory.\nYou may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don’t need to export variables every session. A complete list of environment variables can be found here.\nThe training data can be cloned from:\ngit clone https://github.com/nextflow-io/training.git"
  },
  {
    "objectID": "workshops/6.1_operators.html#rna-seq-workflow-and-module-files",
    "href": "workshops/6.1_operators.html#rna-seq-workflow-and-module-files",
    "title": "Nextflow Development - Channel Operators",
    "section": "RNA-seq Workflow and Module Files ",
    "text": "RNA-seq Workflow and Module Files \nPreviously, we created three Nextflow files and one config file:\n├── nextflow.config\n├── rnaseq.nf\n├── modules.nf\n└── modules\n    └── trimgalore.nf\n\nrnaseq.nf: main workflow script where parameters are defined and processes were called.\n\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../training/nf-training/data/ggal/*_{1,2}.fq\"\nparams.transcriptome_file = \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\n\ninclude { INDEX } from './modules.nf'\ninclude { QUANTIFICATION as QT } from './modules.nf'\ninclude { FASTQC as FASTQC_one } from './modules.nf'\ninclude { FASTQC as FASTQC_two } from './modules.nf'\ninclude { MULTIQC } from './modules.nf'\ninclude { TRIMGALORE } from './modules/trimgalore.nf'\n\nworkflow {\n  index_ch = INDEX(params.transcriptome_file)\n  quant_ch = QT(index_ch, reads_ch)\n  fastqc_ch = FASTQC_one(reads_ch)\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\nmodules.nf: script containing the majority of modules, including INDEX, QUANTIFICATION, FASTQC, and MULTIQC\n\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path transcriptome\n\n    output:\n    path \"salmon_idx\"\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\n\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    path salmon_index\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"$sample_id\"\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\n\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    path \"fastqc_${sample_id}_logs\"\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    path quantification\n    path fastqc\n\n    output:\n    path \"*.html\"\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\nmodules/trimgalore.nf: script inside a modules folder, containing only the TRIMGALORE process\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n}\n\nnextflow.config: config file that enables singularity\n\nsingularity {\n  enabled = true\n  autoMounts = true\n  cacheDir = \"/config/binaries/singularity/containers_devel/nextflow\"\n}\nRun the pipeline, specifying --outdir:\n&gt;&gt;&gt; nextflow run rnaseq.nf --outdir output\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [soggy_jennings] DSL2 - revision: 87afc1d98d\nexecutor &gt;  local (16)\n[93/d37ef0] process &gt; INDEX          [100%] 1 of 1 ✔\n[b3/4c4d9c] process &gt; QT (1)         [100%] 3 of 3 ✔\n[d0/173a6e] process &gt; FASTQC_one (3) [100%] 3 of 3 ✔\n[58/0b8af2] process &gt; TRIMGALORE (3) [100%] 3 of 3 ✔\n[c6/def175] process &gt; FASTQC_two (3) [100%] 3 of 3 ✔\n[e0/bcf904] process &gt; MULTIQC (3)    [100%] 3 of 3 ✔"
  },
  {
    "objectID": "workshops/6.1_operators.html#map",
    "href": "workshops/6.1_operators.html#map",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.1 map ",
    "text": "6.1.1 map \nThe map operator applies a mapping function to each item in a channel. This function is expressed using the Groovy closure { }.\nChannel\n    .of('hello', 'world')\n    .map { word -&gt; \n        def word_size = word.size()\n        [word, word_size] \n    }\n    .view()\nIn this example, a channel containing the strings hello and world is created.\nInside the map operator, the local variable word is declared, and used to represent each input value that is passed to the function, ie. each element in the channel, hello and world.\nThe map operator ‘loops’ through each element in the channel and assigns that element to the local varialbe word. A new local variable word_size is defined inside the map function, and calculates the length of the string using size(). Finally, a tuple is returned, where the first element is the string represented by the local word variable, and the second element is the length of the string, represented by the local word_size variable.\nOutput:\n[hello, 5]\n[world, 5]\nFor our RNA-seq pipeline, let’s first create separate transcriptome files for each organ: lung.transcriptome.fa, liver.transcriptome.fa, gut.transcriptome.fa\ncp \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\" \"/scratch/users/.../training/nf-training/data/ggal/lung.transcriptome.fa\"\ncp \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\" \"/scratch/users/.../training/nf-training/data/ggal/liver.transcriptome.fa\"\nmv \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\" \"/scratch/users/.../training/nf-training/data/ggal/gut.transcriptome.fa\"\nEnsure transcriptome.fa no longer exists:\n&gt;&gt;&gt; ls /scratch/users/.../training/nf-training/data/ggal/\ngut_1.fq\ngut_2.fq\ngut.transcriptome.fa\nliver_1.fq\nliver_2.fq\nliver.transcriptome.fa\nlung_1.fq\nlung_2.fq\nlung.transcriptome.fa\nExercise\nCurrently in the rnaseq.nf script, we define the transcriptome_file parameter to be a single file.\nparams.transcriptome_file = \"/scratch/users/.../training/nf-training/data/ggal/transcriptome.fa\"\nSet the transcriptome_file parameter to match for all three .fa files using a glob path matcher.\nUse the fromPath channel factory to read in the transcriptome files, and the map operator to create a tuple where the first element is the sample (organ type) of the .fa, and the second element is the path of the .fa file. Assign the final output to be a channel called transcriptome_ch.\nThe getSimpleName() Groovy method can be used extract the sample name from our .fa file, for example:\ndef sample = fasta.getSimpleName()\nUse the view() channel operator to view the transcriptome_ch channel. The expected output:\n[lung, /scratch/users/.../training/nf-training/data/ggal/lung.transcriptome.fa]\n[liver, /scratch/users/.../training/nf-training/data/ggal/liver.transcriptome.fa]\n[gut, /scratch/users/.../training/nf-training/data/ggal/gut.transcriptome.fa]\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe transcriptome_file parameter is defined using *, using glob to match for all three .fa files. The fromPath channel factory is used to read the .fa files, and the map operator is used to create the tuple.\nIn the map function, the variable file was chosen to represent each element that is passed to the function. The function emits a tuple where the first element is the sample name, returned by the getSimpleName() method, and the second element is the .fa file path.\nparams.transcriptome_file = \"/scratch/users/.../nf-training/data/ggal/*.fa\"\n\ntranscriptome_ch = Channel.fromPath(\"$params.transcriptome_file\")\n    .map { fasta -&gt; \n    def sample = fasta.getSimpleName()\n    [sample, fasta]\n    }\n    .view()\n\n\n\n\nChallenge\nModify the INDEX process to match the input structure of transcriptome_ch. Modify the output of INDEX so that a tuple is emitted, where the first elememt is the value of the grouping key, and the second element is the path of the salmon_idx folder.\nIndex the transcriptome_ch using the INDEX process. Emit the output as index_ch.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe input is now defined to be a tuple of two elements, where the first element is the grouping key and the second element is the path of the transcriptome file.\nprocess INDEX {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    tuple val(sample_id), path(transcriptome)\n\n    output:\n    tuple val(sample_id), path(\"salmon_idx\")\n\n    script:\n    \"\"\"\n    salmon index --threads $task.cpus -t $transcriptome -i salmon_idx\n    \"\"\"\n}\nInside the workflow block, transcriptome_ch is used as input into the INDEX process. The process outputs are emitted as index_ch\nworkflow {\n  index_ch = INDEX(transcriptome_ch)\n  index_ch.view()\n}\nThe index_ch channel is now a tuple where the first element is the grouping key, and the second element is the path to the salmon index folder.\n&gt;&gt;&gt; nextflow run rnaseq.nf\nN E X T F L O W  ~  version 23.04.1\nLaunching `rnaseq.nf` [dreamy_linnaeus] DSL2 - revision: b4ec1d02bd\n[21/91088a] process &gt; INDEX (3) [100%] 3 of 3\n[liver, /scratch/users/.../work/06/f0a54ba9191cce9f73f5a97bfb7bea/salmon_idx]\n[lung, /scratch/users/.../work/60/e84b1b1f06c43c8cf69a5c621d5a41/salmon_idx]\n[gut, /scratch/users/.../work/21/91088aafb553cb4b933bc2b3493f33/salmon_idx]\n\n\n\nCopy the new INDEX process into modules.nf. In the workflow block of rnaseq.nf, use transcriptome_ch as the input to the process INDEX."
  },
  {
    "objectID": "workshops/6.1_operators.html#combine",
    "href": "workshops/6.1_operators.html#combine",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.2 combine ",
    "text": "6.1.2 combine \nThe combine operator produces the cross product (ie. outer product) combinations of two source channels.\nFor example: The words channel is combined with the numbers channel, emitting a channel where each element of numbers is paired with each element of words.\nnumbers = Channel.of(1, 2, 3)\nwords = Channel.of('hello', 'ciao')\n\nnumbers.combine(words).view()\nOutput:\n[1, hello]\n[2, hello]\n[3, hello]\n[1, ciao]\n[2, ciao]\n[3, ciao]\nThe by option can be used to combine items that share a matching key. This value is zero-based, and represents the index or list of indices for the grouping key. The emitted tuple will consist of multiple elements.\nFor example: source and target are channels consisting of multiple tuples, where the first element of each tuple represents the grouping key. Since indexing is zero-based, by is set to 0 to represent the first element of the tuple.\nsource = Channel.of( [1, 'alpha'], [2, 'beta'] )\ntarget = Channel.of( [1, 'x'], [1, 'y'], [1, 'z'], [2, 'p'], [2, 'q'], [2, 't'] )\n\nsource.combine(target, by: 0).view()\nEach value within the source and target channels are separate elements, resulting in the emitted tuple each containing 3 elements:\n[1, alpha, x]\n[1, alpha, y]\n[1, alpha, z]\n[2, beta, p]\n[2, beta, q]\n[2, beta, t]\nExercise\nIn our RNA-seq pipeline, create a channel quant_inputs_ch that contains the reads_ch combined with the index_ch via a matching key. The emitted channel should contain three elements, where the first element is the grouping key, the second element is the path to the salmon index folder, and the third element is a list of the .fq pairs.\nThe expected output:\n[liver, /scratch/users/.../work/cf/42458b80e050a466d62baf99d0c1cf/salmon_idx, [/scratch/users/.../training/nf-training/data/ggal/liver_1.fq, /scratch/users/.../training/nf-training/data/ggal/liver_2.fq]]\n[lung, /scratch/users/.../work/64/90a77a5f1ed5a0000f6620fd1fab9a/salmon_idx, [/scratch/users/.../training/nf-training/data/ggal/lung_1.fq, /scratch/users/.../training/nf-training/data/ggal/lung_2.fq]]\n[gut, /scratch/users/.../work/37/352b00bfb71156a9250150428ddf1d/salmon_idx, [/scratch/users/.../training/nf-training/data/ggal/gut_1.fq, /scratch/users/.../training/nf-training/data/ggal/gut_2.fq]]\nUse quant_inputs_ch as the input for the QT process within the workflow block.\nModify the process such that the input will be a tuple consisting of three elements, where the first element is the grouping key, the second element is the salmon index and the third element is the list of .fq reads. Also modify the output of the QT process to emit a tuple of two elements, where the first element is the grouping key and the second element is the $sample_id folder. Emit the process output as quant_ch in the workflow block.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe reads_ch is combined with the index_ch using the combine channel operator with by: 0, and is assigned to the channel quant_inputs_ch. The new quant_inputs_ch channel is input into the QT process.\nworkflow {\n  index_ch = INDEX(transcriptome_ch)\n\n  quant_inputs_ch = index_ch.combine(reads_ch, by: 0)\n  quant_ch = QT(quant_inputs_ch)\n}\nIn te QT process, the input has been modified to be a tuple of three elements - the first element is the grouping key, the second element is the path to the salmon index, and the third element is the list of .fq reads.\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    tuple val(sample_id), path(salmon_index), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"$sample_id\")\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}"
  },
  {
    "objectID": "workshops/6.1_operators.html#grouptuple",
    "href": "workshops/6.1_operators.html#grouptuple",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.3 groupTuple ",
    "text": "6.1.3 groupTuple \nThe groupTuple operator collects tuples into groups based on a similar grouping key, emitting a new tuple for each distinct key. The groupTuple differs from the combine operator in that it is performed on one input channel, and the matching values are emitted as a list.\nChannel.of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )\n    .groupTuple()\n    .view()\nOutput:\n[1, [A, B, C]]\n[2, [C, A]]\n[3, [B, D]]\nBy default, the first element of each tuple is used as the grouping key. The by option can be used to specify a different index. For example, to group by the second element of each tuple:\nChannel.of( [1, 'A'], [1, 'B'], [2, 'C'], [3, 'B'], [1, 'C'], [2, 'A'], [3, 'D'] )\n    .groupTuple(by: 1)\n    .view()\n[[1, 2], A]\n[[1, 3], B]\n[[2, 1], C]\n[[3], D]\n\nIn the workflow script rnaseq.nf we defined the reads parameter to be multiple paired .fq files that are created into a channel using the fromFilePairs channel factory. This created a tuple where the first element is a unique grouping key, created automatically based on similarities in file name, and the second element contains the list of paired files.\n#!/usr/bin/env nextflow\n\nparams.reads = \"/scratch/users/.../nf-training/data/ggal/*_{1,2}.fq\"\n\nreads_ch = Channel.fromFilePairs(\"$params.reads\")\nNow, move the /scratch/users/.../nf-training/data/ggal/lung_2.fq file into another directory so the folder contains one lung .fq file:\n&gt;&gt;&gt; mv /scratch/users/.../training/nf-training/data/ggal/lung_2.fq .\n&gt;&gt;&gt; ls /scratch/users/.../training/nf-training/data/ggal\ngut_1.fq\ngut_2.fq\ngut.transcriptome.fa\nliver_1.fq\nliver_2.fq\nliver.transcriptome.fa\nlung_1.fq\nlung.transcriptome.fa\nExercise\nUse the fromPath channel factory to read all .fq files as separate elements.\nThen, use map to create a mapping function that returns a tuple, where the first element is the grouping key, and the second element is the .fq file(s).\nThen, use groupTuple() to create channels containing both single and paired .fq files. Within the groupTuple() operator, set sort: true, which orders the groups numerically, ensuring the first .fq is first.\nExpected output:\n[lung, [/scratch/users/.../training/nf-training/data/ggal/lung_1.fq]]\n[gut, [/scratch/users/.../training/nf-training/data/ggal/gut_1.fq, /scratch/users/.../training/nf-training/data/ggal/gut_2.fq]]\n[liver, [/scratch/users/.../training/nf-training/data/ggal/liver_1.fq, /scratch/users/.../training/nf-training/data/ggal/liver_2.fq]]\nInside the map function, the following can be used to extract the sample name from the .fq files. file is the local variable defined inside the function that represents each .fq file. The getName() method will return the file name without the full path, and replaceAll is used to remove the _2.fq and _1.fq file suffixes.\ndef group_key = file.getName().replaceAll(/_2.fq/,'').replaceAll(/_1.fq/,'')\nFor a full list of Nextflow file attributes, see here.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe fromPath channel is used to read all .fq files separately. The map function is then used to create a two-element tuple where the first element is a grouping key and the second element is the list of .fq file(s).\nreads_ch = Channel.fromPath(\"/home/sli/nextflow_training/training/nf-training/data/ggal/*.fq\")\n  .map { file -&gt;\n    def group_key = file.getName().replaceAll(/_2.fq/,'').replaceAll(/_1.fq/,'')\n    [group_key, file]\n  }\n  .groupTuple(sort: true)\n  .view()\n\n\n\nNow, run the workflow up to the combine step. The quant_inputs_ch should now consist of:\n[liver, /scratch/users/.../work/cf/42458b80e050a466d62baf99d0c1cf/salmon_idx, [/scratch/users/.../nf-training/data/ggal/liver_1.fq, /scratch/users/.../nf-training/data/ggal/liver_2.fq]]\n[lung, /scratch/users/.../work/64/90a77a5f1ed5a0000f6620fd1fab9a/salmon_idx, [/scratch/users/.../nf-training/data/ggal/lung_1.fq]]\n[gut, /scratch/users/.../work/37/352b00bfb71156a9250150428ddf1d/salmon_idx, [/scratch/users/.../nf-training/data/ggal/gut_1.fq, /scratch/users/.../nf-training/data/ggal/gut_2.fq]]"
  },
  {
    "objectID": "workshops/6.1_operators.html#flatten",
    "href": "workshops/6.1_operators.html#flatten",
    "title": "Nextflow Development - Channel Operators",
    "section": "6.1.4 flatten ",
    "text": "6.1.4 flatten \nThe flatten operator flattens each item from a source channel and emits the elements separately. Deeply nested inputs are also flattened.\nChannel.of( [1, [2, 3]], 4, [5, [6]] )\n    .flatten()\n    .view()\nOutput:\n1\n2\n3\n4\n5\n6\n\nWithin the script block of the QUANTIFICATION process in the RNA-seq pipeline, we are assuming the reads are paired, and specify -1 ${reads[0]} -2 ${reads[1]} as inputs to salmon quant.\nprocess QUANTIFICATION {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-salmon-1.10.1--h7e5ed60_0.img\"\n\n    input:\n    tuple val(sample_id), path(salmon_index), path(reads)\n\n    output:\n    tuple val(sample_id) path(\"$sample_id\")\n\n    script:\n    \"\"\"\n    salmon quant --threads $task.cpus --libType=U \\\n    -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n    \"\"\"\n}\nNow that the input reads can be either single or paired, the QUANTIFICATION process needs to be modified to allow for either input type. This can be done using the flatten() operator, and conditional script statements. Additionally, the size() method can be used to calculate the size of a list.\nThe script block can be changed to the following:\n    script:\n    def input_reads = [reads]\n    if( input_reads.flatten().size() == 1 )\n        \"\"\"\n        salmon quant --threads $task.cpus --libType=U \\\n        -i $salmon_index -r $reads -o $sample_id\n        \"\"\"\n    else \n        \"\"\"\n        salmon quant --threads $task.cpus --libType=U \\\\\n        -i $salmon_index -1 ${reads[0]} -2 ${reads[1]} -o $sample_id\n        \"\"\"\nFirst, a new variable input_reads is defined, which consists of the reads input being converted into a list. This has to be done since Nextflow will automatically convert a list of length 1 into a path within process. If the size() method was used on a path type input, it will return the size of the file in bytes, and not the list size. Therefore, all inputs must first be converted into a list in order to correctly caculate the number of files.\ndef input_reads = [reads]\nFor reads that are already in a list (ie. paired reads), this will nest the input into another list, for example:\n[ [ file1, file2 ] ]\nIf the size() operator is used on this input, it will always return 1 since the encompassing list only contains one element. Therefore, the flatten() operator has to be used to emit the files as separate elements.\nThe final definition to obtain the number of files in reads becomes:\ninput_reads.flatten().size()\nFor single reads, the input to salmon quant becomes -r $reads\n\nExercise\nCurrently the TRIMGALORE process only accounts for paired reads.\nprocess TRIMGALORE {\n    container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n    input:\n        tuple val(sample_id), path(reads)\n    \n    output:\n        tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n        tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n        tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n        tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n        tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n    script:\n        \"\"\"\n        trim_galore \\\\\n        --paired \\\\\n        --gzip \\\\\n        ${reads[0]} \\\\\n        ${reads[1]}\n        \"\"\"\n}\nModify the process such that both single and paired reads can be used. For single reads, the following script block can be used:\n\"\"\"\ntrim_galore \\\\\n  --gzip \\\\\n  $reads\n\"\"\"\n\n\n\n\n\n\nSolution\n\n\n\n\n\nprocess TRIMGALORE {\n  container '/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-trim-galore-0.6.6--0.img' \n\n  input:\n    tuple val(sample_id), path(reads)\n  \n  output:\n    tuple val(sample_id), path(\"*{3prime,5prime,trimmed,val}*.fq.gz\"), emit: reads\n    tuple val(sample_id), path(\"*report.txt\")                        , emit: log     , optional: true\n    tuple val(sample_id), path(\"*unpaired*.fq.gz\")                   , emit: unpaired, optional: true\n    tuple val(sample_id), path(\"*.html\")                             , emit: html    , optional: true\n    tuple val(sample_id), path(\"*.zip\")                              , emit: zip     , optional: true\n\n  script:\n  def input_reads = [reads]\n\n  if( input_reads.flatten().size() == 1 )\n    \"\"\"\n    trim_galore \\\\\n      --gzip \\\\\n      $reads\n    \"\"\"\n  else\n    \"\"\"\n    trim_galore \\\\\n      --paired \\\\\n      --gzip \\\\\n      ${reads[0]} \\\\\n      ${reads[1]}\n    \"\"\"\n\n}\n\n\n\nExtension\nModify the FASTQC process such that the output is a tuple where the first element is the grouping key, and the second element is the path to the fastqc logs.\nModify the MULTIQC process such that the output is a tuple where the first element is the grouping key, and the second element is the path to the generated html file.\nFinally, run the entire workflow, specifying an --outdir. The workflow block should look like this:\nworkflow {\n  index_ch = INDEX(transcriptome_ch)\n\n  quant_inputs_ch = index_ch.combine(reads_ch, by: 0)\n  quant_ch = QT(quant_inputs_ch)\n\n  trimgalore_out_ch = TRIMGALORE(reads_ch).reads\n\n  fastqc_ch = FASTQC_one(reads_ch)\n  fastqc_cleaned_ch = FASTQC_two(trimgalore_out_ch)\n  multiqc_ch = MULTIQC(quant_ch, fastqc_ch)\n}\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe output block of both processes have been modified to be tuples containing a grouping key.\nprocess FASTQC {\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-fastqc-0.12.1--hdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(reads)\n\n    output:\n    tuple val(sample_id), path(\"fastqc_${sample_id}_logs\")\n\n    script:\n    \"\"\"\n    mkdir fastqc_${sample_id}_logs\n    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n    \"\"\"\n}\n\nprocess MULTIQC {\n    publishDir params.outdir, mode:'copy'\n    container \"/config/binaries/singularity/containers_devel/nextflow/depot.galaxyproject.org-singularity-multiqc-1.21--pyhdfd78af_0.img\"\n\n    input:\n    tuple val(sample_id), path(quantification)\n    tuple val(sample_id), path(fastqc)\n\n    output:\n    tuple val(sample_id), path(\"*.html\")\n\n    script:\n    \"\"\"\n    multiqc . --filename $quantification\n    \"\"\"\n}\n\n\n\n\nThis workshop is adapted from Fundamentals Training, Advanced Training, Developer Tutorials, Nextflow Patterns materials from Nextflow, nf-core nf-core tools documentation and nf-validation"
  },
  {
    "objectID": "workshops/00_setup.html",
    "href": "workshops/00_setup.html",
    "title": "Essential Workshop Preparation",
    "section": "",
    "text": "In this workshop, we will be using Peter Mac’s HPC to run nextflow and nf-core workflows.\nBefore joining the workshop, please complete the following checklist:"
  },
  {
    "objectID": "workshops/00_setup.html#option-1-install-and-set-up-visual-studio-code",
    "href": "workshops/00_setup.html#option-1-install-and-set-up-visual-studio-code",
    "title": "Essential Workshop Preparation",
    "section": "Option 1: Install and set up Visual Studio Code",
    "text": "Option 1: Install and set up Visual Studio Code\nWe recommend Visual Studio Code as a source code editor because it is lightweight and has rich support for extensions and syntax highlighting available across various popular operating system.\nDownload Visual Studio Code on your computer and follow the instructions for your specific Operating System as required:\n\nmacOS\nWindows\nLinux\n\nOnce installed, open VS Code on your computer.\n\n\nInstall the Nextflow Extension\nThe Nextflow extension provides syntax highlighting and quick shortcuts for common code snippets.\nClick on the extensions button (four blocks) on the left side bar. Search for “Nextflow” in the extensions search bar, then click on the blue “Install” button.\n\n\n\nInstall the Remote-SSH Extension\nRemote-SSH allows us to use any remote machine with a SSH server as your development environment. This lets us work directly on the our cluster’s storage.\nClick on the extensions button (four blocks) on the left side bar. Search for “Remote - SSH” in the extensions search bar, then click on the blue “Install” button.\n\n\n\nLogin via Visual Studio Code\nConnect to your instance with VS code by adding the host details to your .ssh config file (if you have not done this previously)\n\nIn a new VS code window, type Ctrl+Shift+P if you’re on a Windows machine or Cmd+Shift+P for MacOS to open the command palette\nSelect Remote-SSH: Open SSH configuration file and select your .ssh config file\nAdd a new entry with your details to login to cluster, and save your .ssh config file:\n\nHost pmac-cluster\n    HostName kvmpr-res-lnode1.unix.petermac.org.au\n    User &lt;your-cluster-user-name&gt;\n\nType Ctrl+Shift+P and select Remote-SSH: Connect to Host; and pmac-cluster (or whatever you name your host above)\nWhen prompted, select Linux as the platform of the remote host from the dropdown menu\nType in your password and hit enter\n\nHaving successfully logged in, you should see a small blue or green box in the bottom left corner of your screen:\n\nTo set up your VS Code window for the workshop:\n\nOpen a new folder in the file explorer panel on the left side of the screen by typing Ctrl + K, Ctrl + O if you’re running Windows or Cmd+K+ Cmd + O for MacOS\nSelect /scratch/users/&lt;your-username&gt;/nfWorkshop to open our working directory. If you encountered an error that the directory does not exist, you would need to ssh in to the cluster and create that directory first before attempting this step.\nWhen prompted, select the box for Trust the authors of all files in the parent folder ‘home’ then click Yes, I trust the authors\nYou can dismiss the warning message saying our cluster’s git version is outdated\nTo open a terminal, type Ctrl+J if you’re on a Windows machine or Cmd+J on MacOS"
  },
  {
    "objectID": "workshops/00_setup.html#option-2-terminal",
    "href": "workshops/00_setup.html#option-2-terminal",
    "title": "Essential Workshop Preparation",
    "section": "Option 2: Terminal",
    "text": "Option 2: Terminal\nNo additional setup required. SSH to cluster as usual, and we assume you are already familiar with command line if you decided to go with this option. 😄\n\nThis setup instruction is adapted from Customising Nf-Core Workshop materials from Sydney Informatics Hub"
  },
  {
    "objectID": "workshops/2.1_customise_and_run.html",
    "href": "workshops/2.1_customise_and_run.html",
    "title": "Customising & running nf-core pipelines",
    "section": "",
    "text": "2.1.1. Pipeline setup\nIn this session we are using Singularity containers to manage software installation for all nf-core/rnaseq tools. Confirm the Singularity cache directory we set in the previous session using the $NXF_SINGULARITY_CACHEDIR Nextflow environmental variable:\necho $NXF_SINGULARITY_CACHEDIR\n👀 This should match the directory you set in the previous session:\n/config/binaries/singularity/containers_devel/nextflow\n\n\n2.1.2. Design your run command\nAs we learnt in lesson 1.2.4, all nf-core pipelines have a unique set of pipeline-specific parameters that can be used in conjunction with Nextflow parameters to configure the workflow. Generally, nf-core pipelines can be customised at a few different levels:\n\n\n\n\n\n\n\nLevel of effect\nCustomisation feature\n\n\n\n\nThe workflow\nWhere diverging methods are available for a pipeline, you may choose a path to follow\n\n\nA process\nWhere more than one tool is available for a single step, you may choose which to use\n\n\nA tool\nApply specific thresholds or optional flags for a tool on top of the default run command\n\n\nCompute resources\nSpecify resource thresholds or software execution methods for the workflow or a process\n\n\n\nAll nf-core pipelines are provided with comprehensive documentation that explain what the default workflow structure entails and options for customising this based on your needs. It is important to remember that nf-core pipelines typically do not include all possible tool parameters. This makes it challenging to piece these different sources of information together to determine which parameters you should be using.\nThe following sections of the documentation can be used to understand what the pipeline is doing and inform your choices about aspects of pipeline-specific customisations:\n\n\n\nDocs\nDescription\nCustomisation level\n\n\n\n\nIntroduction\nWorkflow summary\n\nworkflow\nprocess\n\n\n\nUsage\nInputs and options\n\nworkflow\nprocess\n\n\n\nParameters\nAvailable flags\n\nworkflow\nprocess\ncompute resources\n\n\n\nOutput\nFiles from all processes processes\n\nworkflow\nprocess\ntool\n\n\n\n\n\n\nChallenge\nView the parameters for the nf-core/rnaseq workflow using the command line for the specific version 3.11.1\n:::\n\nPipeline structure\n👀 Looking at the nf-core/rnaseq pipeline structure provided in the introduction, we can see that the developers have:\n\nOrganised the workflow into 5 stages based on the type of work that is being done\nProvided a choice of multiple methods and specified defaults\nProvided a choice of tool for some steps\n\n\n\n\n\n\n\n\nThoughts? 💭\n\n\n\n❓ Observing the diagram above, which statement is true regarding the choice of alignment and quantification methods provided by the nf-core/rnaseq pipeline?\na. The pipeline uses a fixed method for read alignment and quantification.\nb. Users can choose between several different methods for read alignment and quantification.\nc. The pipeline always performs read alignment and quantification using STAR or HISAT2.\nd. The choice of alignment and quantification method is determined automatically based on the input data.\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe correct answer is b. The nf-core/rnaseq pipeline allows users to choose between pseudo-alignment and quantification or several different methods for genome-based read alignment and quantification.\n\na is incorrect because the pipeline is not limited to a single method.\n\nc is incorrect because while read alignment and quantification using STAR and Salmon are the default method, users can choose pseudo-alignment method.\nd is also incorrect, as the pipeline only accepts fastq files as input and the choice of alignment and quantification method must be specified by the user.\n\n\n\n\n\n\n\nDefault pipeline usage\nThe number and type of default and optional parameters an nf-core pipeline accepts is at the discretion of it’s developers. However, at a minimum, nf-core pipelines typically:\n\nRequire users to specify a sample sheet (--input) detailing sample data and relevant metadata\nAutogenerate or acquire missing reference files from iGenomes ( using the --genome) if not provided by the user.\n\nYou can see the recommended (typical) run command and all the parameters available for the nf-core/rnaseq pipeline by running:\nnextflow run nf-core/rnaseq -r 3.11.1 --help \nThe typical or recommended run command for this pipeline is provided at the top of the screen:\n\nIt outlines a requirement for a few basic things:\n\nAn input samplesheet\nA location to store outputs\nRelevant reference data\nA software management method\n\n\n\n\n\n\n\nReminder: hyphens matter in Nextflow!\n\n\n\nNextflow-specific parameters use one (-) hyphen, whereas pipeline-specific parameters use two (--). In the typical run command above -profile is a Nextflow parameter, while --input is an nf-core parameter.\n\n\n\nRequired input: --input\nMost of us will need to adjust the default run command for our experiments. Today we’ll be adjusting the typical nf-core/rnaseq run command by:\n\nProviding our own reference files\nUsing the Singularity software management profile, instead of Docker\nCustomising the execution of some processes\nSpecifying the computing resource limitations of our instances (2 CPUs, 8 Gb RAM)\n\nOur input fastq files (fastq/), reference data (ref/), and full sample sheet (samplesheet.csv) are already available on the cluster. Take a look at the files:\nls -l /data/seqliner/test-data/rna-seq\ntotal 24\ndrwxrwsr-x 2 jyu bioinf-core 8192 Sep 14 10:13 ERCC_index\ndrwxrwsr-x 2 jyu bioinf-core 8192 Sep 14 10:12 fastq\ndrwxrwsr-x 2 jyu bioinf-core 8192 Sep 14 10:12 ref\nTo make life easier store the path to our test data in a variable.\nmaterials=/data/seqliner/test-data/rna-seq\nGiven we are only testing the pipeline in this session, we only need to work with a couple of samples. Copy the first two samples from the full prepared sample sheet to a local version of the file:\nhead -n 3 $materials/samplesheet.csv &gt; samplesheet.csv\ncat samplesheet.csv\nsample,fastq_1,fastq_2,strandedness\nHBR_Rep1_ERCC,fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz,fastq/HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz,forward\nHBR_Rep2_ERCC,fastq/HBR_Rep2_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1.fastq.gz,fastq/HBR_Rep2_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2.fastq.gz,forward\n\n\nRequired input: reference data\nMany nf-core pipelines have a minimum requirement for reference data inputs. The input reference data requirements for this pipeline are provided in the usage documentation. We can replace the --genome flag in the typical run command with our own files. To see what reference files we can specify using parameters, rerun the pipeline’s help command to view all the available parameters:\nnextflow run nf-core/rnaseq -r 3.11.1 --help\nFrom the Reference genome options parameters, we will provide our own files using:\n\n--fasta $materials/ref/chr22_with_ERCC92.fa\n\n--gtf $materials/ref/chr22_with_ERCC92.gtf\n\n\n\n\n\n\n\nBeware the hidden parameters!\n\n\n\nNotice the message at the bottom of the screen:\n!! Hiding 24 params, use --show_hidden_params to show them !!\nKeep in mind that both this help command and the nf-core parameters documentation hides less common parameters.\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nRe-run the help command to output the parameters for the nf-core/rnaseq pipeline and including all hidden parameters for version 3.11.1\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRun the following:\nnextflow run nf-core/rnaseq -r 3.11.1 --help --show_hidden_params\n\n\n\n\n\n\nOptional parameters\nNow that we have prepared our input and reference data, we will customise the typical run command by:\n\nUsing Nextflow’s -profile parameter to specify that we will be running the Singularity profile instead of the Docker profile\nAdding additional process-specific flags to skip duplicate read marking, save trimmed reads and save unaligned reads\nAdding additional max resource flags to specify the number of CPUs and amount of memory available to the pipeline.\n\nThe parameters we will use are:\n\n-profile singularity\n--skip_markduplicates true\n--save_trimmed true\n--save_unaligned true\n--max_memory '6.GB'\n--max_cpus 2\n\nYou can see how we’ve customised the typical run command in the diagram below:\n\n\n\n2.1.3. Run the pipeline\nWe will also create a separate output directory for this section.\ncd /scratch/users/&lt;your-username&gt;/nfWorkshop; mkdir ./lesson2 && cd $_\nNow that we have prepared our data and chosen which parameters to apply, run the pipeline:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n    --input samplesheet.csv \\\n    --outdir ./lesson2.1 \\\n    --fasta $materials/ref/chr22_with_ERCC92.fa \\\n    --gtf $materials/ref/chr22_with_ERCC92.gtf \\\n    -profile singularity \\\n    --skip_markduplicates true \\\n    --save_trimmed true \\\n    --save_unaligned true \\\n    --max_memory '6.GB' \\\n    --max_cpus 2\n👀 Take a look at the stdout printed to the screen. Your workflow configuration and parameter customisations are all documented here. You can use this to confirm if your parameters have been correctly passed to the run command:\n\nAs the workflow starts, you will also see a number of processes spawn out underneath this. Recall from the earlier session that processes are executed independently and can run in parallel. Nextflow manages the data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied.\nTo understand how this is coordinated, consider the STAR_ALIGN process that is being run.\n\n👀 You’ll notice a few things:\n\nWe can see which inputs are being processed by looking at the end of the process name\nWhen a process starts it progressively spawns tasks for all inputs to be processed\nA single TRIMGALORE process is run across both samples in our samplesheet.csv before STAR_ALIGN begins\nOnce a TRIMGALORE task is completed for a sample, the STAR_ALIGN task for that sample begins\nWhen the STAR_ALIGN process starts, it spawns 2 tasks.\n\n\n\n\n\n\n\nChallenge\n\n\n\nRecall from earlier Convert the parameter definitions into a YAML file, workshop-params.yaml, compatible with -params-file\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\ngtf: \"/data/seqliner/test-data/rna-seq/ref/chr22_with_ERCC92.gtf\"\nfasta: \"/data/seqliner/test-data/rna-seq/ref/chr22_with_ERCC92.fa\"\nskip_markduplicates: true\nsave_trimmed: true\nsave_unaligned: true\nmax_memory: \"6.GB\" \nmax_cpus: 2\ninput: \"samplesheet.csv\"\n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core pipelines are provided with sensible default settings and required inputs.\nAn nf-core pipeline’s Usage, Output, and Parameters documentation can be used to design a suitable run command.\nParameters can be used to customise the workflow, processes, tools, and compute resources.\n\n\n\n\nIn the previous exercises, we have explored how to customise a run with workflow parameters on the command line or within a parameters file. In this lesson we will now look at configuration settings, which manage how the workflow is implemented on your system.\n\n\n\n\n\n\nNote\n\n\n\nNextflow’s portability is achieved by separating workflow implementation (input data, custom parameters, etc.) from the configuration settings (tool access, compute resources, etc.) required to execute it. This portability facilitates reproducibility: by applying the same parameters as a colleague, and adjusting configurations to suit your platform, you can achieve the same results on any machine with no requirement to edit the code.\n\n\n\n\n2.1.4. Default nf-core configuration\nTogether nextflow.config and base.config, define the default execution settings and parameters of an nf-core workflow.\nLet’s take a look at these two configuration files to gain an understanding of how defaults are applied. conf/base.config\nThe generic base.config sets the default compute resource settings to be used by the processes in the nf-core workflow. It uses process labels to enable different sets of resources to be applied to groups of processes that require similar compute. These labels are specified within the main.nf file for a process.\n We can over-ride these default compute resources using a custom configuration file.\nThen take a few moments to look through workflow/nextflow.config\nThe nextflow.config file is more workflow-specific, and sets the defaults for the workflow parameters, as well as defines profiles to change the default software access from $PATH to the specified access method, eg Singularity.\n We can over-ride these parameters on the command line or with a parameters file, and over-ride the default behaviour of searching for tools on $PATH by specifying a -profile.\nDefault settings for --max_cpus, --max_memory and --max_time are applied within the nf-core workflow/nextflow.config. These are generous values expecting to be over-ridden with your custom settings, to ensure that no single process attempts to use more resources than you have available on your platform.\nWithin workflow/conf/base.config, the check_max() function over-rides the process resources if the custom ‘max’ setting is lower than the default setting for that process.\n\n\n\n\n\n\n\nChallenge\n\n\n\n\nWhat are the default settings for CPU, memory and walltime for the STAR_ALIGN module?\nHow have these defaults been changed from our applied customisations in the previous runs?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo uncover these answers, we need to understand what process label has been assigned to the STAR_ALIGN module.\nSTAR_ALIGN has the label process_high which has the settings 12 CPUs, 72 GB mem, 16 hrs walltime applied by the default base.config. We have previosuly applied --max_cpus 2 and --max_memory 6.GB, so the check_max() function would have reduced the resources given to the STAR alignment process to 2 CPUs and 6 GB RAM, while retaining the default max walltime.\n\n\n\n\n\n2.1.5. When to use a custom config file\nIn our runs so far, we have avoided the need for a custom resource configuration file by:\n\nOver-riding the default tool access method of $PATH by specifying the singularity profile defined in workflow/nextflow.config\n\nWithout this, our runs for this workshop would fail because we do not have the workflow tools (eg STAR, salmon) installed locally on our VMs\n\nOver-riding the default values for CPUs and memory set in nextflow.config with --max_cpus 2 and --max_memory 6.GB to fit within our interactive sessions\n\nWithout these parameters, our runs would fail at the first process that requests more than this, because Nextflow workflows check that the requested resources are available before attempting to execute a workflow\n\n\nThese are basic configurations. What if:\n\nWe wanted to increase the resources used above what is requested with process labels to take advantage of high CPU or high memory infrastructures?\nWe wanted to run on a HPC or cloud infrastructure?\nWe wanted to execute specific modules on specific node types on a cluster?\nWe wanted to use a non-default software container?\nWe wanted to customise outputs beyond what was possible with the nf-core workflow parameters?\n\n\n\n2.1.6. Custom resource configuration using process labels\nCapping workflow resources using the max parameters is a bit of a blunt instrument.\nTo achieve optimum computational efficiency on your platform, more granular control may be required.\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf you appled --max_cpus 16 to the nf-core rnaseq workflow, the STAR_ALIGN module would still only utilise 12 CPUs, as this module (as we learnt in 2.1.5) has the label process_high which sets CPUs to 12.\nIf there were no processes with fulfilled input channels that could make use of the 4 remaining CPUs, those resources would sit idle until the STAR_ALIGN process had completed.\nOptimisation for this platform might for example set max_cpus to 8 so two samples could be aligned concurrently, or over-ride the CPU resources assigned to the STAR_ALIGN module to 16.\n\n\n\nThe next two lessons will demonstrate how to achieve this using custom configuration files that fine-tune resources using process labels to assign the same resources to groups of processes sharing the same label, or withName to target specific processes.\nIn order to do this, we need to use the process scope. Nextflow has a number of different scopes that can be included in configuration files, for example the params scope you covered in lesson 1.2.5 and applied to your config in lesson 2.1.8.\nWithin the process scope, we can configure resources and additional arguments for processes.\n\n\n\n\n\n\nWarning\n\n\n\nThe following exercise is trivial given the limitations of our interactive session. Consider how this approach can be really powerful when working on HPC or cloud infrastructures, where the executor and queue directives enable you to take full advantage of the compute resources available on your platform.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n💡 View the file conf/base.config for syntax example\nAdd a process scope inside the my_resources.config\nUse withLabel: &lt;label_name&gt; to set resources for each of the following labels:\n\nprocess_low\nprocess_medium\nprocess_high\n\n\n\n\nprocess {\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 2\n        memory = 6.GB\n    } \n    withLabel: process_high {\n        cpus = 2\n        memory = 6.GB\n    }\n}\nSave the file then re-run the workflow with our custom configuration, setting outdir parameter to lesson2.1.7:\n\n\n2.1.7. Examine the outputs\nOnce your pipeline has completed, you should see this message printed to your terminal:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 21-Nov-2023 03:31:24\nDuration    : 10m 50s\nCPU hours   : 0.3\nSucceeded   : 67\nThe pipeline ran successfully, however, note the warning about all samples having failed the strandedness check. We’ll explore that in the next section.\nIn the meantime, list (ls -la) the contents of your directory, you’ll see a few new directories (and a hidden directory and log file) have been created:\ntotal 356\ndrwxrwxr-x  6 rlupat rlupat   4096 Nov 22 03:33 .\ndrwxrwxr-x  7 rlupat rlupat   4096 Nov 22 03:14 ..\ndrwxrwxr-x  7 rlupat rlupat   4096 Nov 22 03:31 lesson2.1\ndrwxrwxr-x  4 rlupat rlupat   4096 Nov 22 03:31 .nextflow\n-rw-rw-r--  1 rlupat rlupat 283889 Nov 22 03:31 .nextflow.log\n-rw-rw-r--  1 rlupat rlupat  66150 Nov 22 03:20 .nextflow.log.1\n-rw-rw-r--  1 rlupat rlupat    492 Nov 22 03:15 samplesheet.csv\ndrwxrwxr-x 69 rlupat rlupat   4096 Nov 22 03:29 work\n👀 Nextflow has created 2 new output directories, work and lesson2.1 in the current directory.\n\nThe work directory\nAs each job is run, a unique sub-directory is created in the work directory. These directories house temporary files and various command logs created by a process. We can find all information regarding this process that we need to troubleshoot a failed process.\n\n\nThe lesson2.1 directory\nAll final outputs will be presented in a directory specified by the --outdir flag.\n\n\n\n\n\n\nBefore executing this run command\n\n\n\nIf you haven’t done so already, check that the run from lession 2.1.3 has completed successfully.\nYou run should have a summary message similar to below:\n-[nf-core/rnaseq] Pipeline completed successfully with skipped sampl(es)-\n-[nf-core/rnaseq] Please check MultiQC report: 2/2 samples failed strandedness check.-\nCompleted at: 11-May-2023 13:21:50\nDuration    : 7m 8s\nCPU hours   : 0.2\nSucceeded   : 66\nwith the following output directories:\n$ ls lesson2.1\nfastqc  multiqc  pipeline_info  star_salmon  trimgalore\n\n\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n  -profile singularity\n  -c my_resources.config \\\n  -params-file workshop-params.yaml \\\n  --outdir lesson2.1.6 \\\n  -resume\n👀 Notice that the Max job request options are no longer listed on the run log printed to screen, because we are setting them within the process scope rather than params scope.\n\n\n\n\n\n\nConfiguration order of priority\n\n\n\nThe order of priority in which parameters and configurations are applied by Nextflow.\nThe settings we specify with -c my_resources.config will over-ride those that appear in the default nf-core configurations workflow/nextflow.config and workflow/conf/base.config.\nSettings that are not over-ridden by -c &lt;config&gt; or any parameter from params file or provided on the command line will still be set to the nf-core defaults specified in nextflow.config, base.config or main.nf.\nTo avoid confusion, it is best not to name your custom configuration files nextflow.config!\n\n\n\n\n\n2.1.8. Custom resource configuration using process names\nThis exercise will demonstrate how to adjust the resource configurations for a specific process using the withName process selector, using the STAR_ALIGN module as example.\nwithName is a powerful tool:\n\nSpecifically target individual modules\nMultiple module names can be supplied using wildcards or ‘or’ (* or |) notation\nNo need to edit the module main.nf file to add a process label\nHas a higher priority than withLabel\n\nTo utilise withName, we first need to ensure we have the correct and specific executuion path for the module/modules that we wish to target.\nIdentify the execution path for the STAR_ALIGN module:\n\n\n\n\n\n\nFinding the module execution path\n\n\n\n\nThe extended execution path is built from the pipeline, workflow, subworkflow, and module names\nIt can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the view the modules.conf file on Github\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNote that this does not provide the PIPELINE or WORKFLOW name at the front of the path. You can add these manually (eg PIPELINE is NFCORE_RNASEQ and WORKFLOW is RNASEQ) but the path within modules.config is usually all that is required for specificity within a workflow\nIf you have previously run the pipeline, you could also extract the complete module execution path from your run log printed to screen, or the execution trace, timeline or report files within &lt;outdir&gt;/pipeline_info\n\n\n\nFor STAR_ALIGN within the nf-core/rnaseq workflow, any of the following would be correct and specific:\n'NFCORE_RNASEQ:RNASEQ:ALIGN_STAR:STAR_ALIGN'\n'.*:RNASEQ:ALIGN_STAR:STAR_ALIGN'\n'.*:ALIGN_STAR:STAR_ALIGN'\n\nContinue editing my_resources.config. Inside the process scope, provide the execution path for the STAR_ALIGN module to the withName selector:\nprocess {\n  withName: '.*:RNASEQ:ALIGN_STAR:STAR_ALIGN' {\n  }\n}      \n\nThen set CPU to 24 and memory to 96 GB:\nprocess {\n  withName: '.*:RNASEQ:ALIGN_STAR:STAR_ALIGN' {\n    cpus = 24\n    memory = 96.GB\n  }\n} \n\n\n\n\n\n\nCompleted config file\n\n\n\n\n\nprocess {\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 2\n        memory = 6.GB\n    } \n    withLabel: process_high {\n        cpus = 2\n        memory = 6.GB\n    }\n    withName: '.*:RNASEQ:ALIGN_STAR:STAR_ALIGN' {\n        cpus = 24\n        memory = 96.GB\n    }\n}\n\n\n\n\n\n\n\n\n\nWhat if the parameter I want to apply isn’t available?\n\n\n\n\n\nRecall from earlier that nf-core modules use ext.args to pass additional arguments to a module. This uses a special Nextflow directive ext. If an nf-core pipeline does not have a pre-defined parameter for a process, you may be able to implement ext.args.\nThe inclusion of ext.args is currently best practice for all DSL2 nf-core modules where additional parameters may be required to run a process. However, this may not be implemented for all modules in all nf-core pipelines. Depending on the pipeline, these process modules may not have defined the ext.args variable in the script blocks and is thus not available for applying customisation. If that is the case consider submitting a feature request or a making pull request on the pipeline’s GitHub repository to implement this!\n\n\n\nSave the config then resume your run, setting outdir to lesson2.1.8, applying your custom resources from my_resources.config:\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n  -profile singularity \\\n  -c my_resources.config \\\n  -params-file workshop-params.yaml \\\n  --outdir lesson2.1.8 \\\n  -resume \nIf your execution path for the STAR_ALIGN module was specified correctly, your run should have died with the error shown below because Nextflow checks that the resources requested are available before executing a workflow:\n\n\n\n\n\n\n\nThoughts? 💭\n\n\n\nWhat do you expect would happen to your run if your execution path for the STAR_ALIGN module was not specified correctly?\n\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIn this case, our pipeline would complete OK, because the resources for the STAR_ALIGN process have been appropriately set for our interactive session using the process_high label within our my_resources.config.\nThe directives set within the withName scope would not be applicable, and a warning would be printed, eg\nWARN: There's no process matching config selector: .*:RNASEQ:ALIGN_STAR:STARALIGN\n\n\n\n\n\n\n\n\n\nChallenge\n\n\n\nChange the container used by multiqc to quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0 using the withName scope in your my_resources.config\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nmy_resources.config\nprocess {\n    withLabel: process_low {\n        cpus = 2\n        memory = 6.GB\n    }\n    withLabel: process_medium {\n        cpus = 2\n        memory = 6.GB\n    } \n    withLabel: process_high {\n        cpus = 2\n        memory = 6.GB\n    }\n    withName: '.*:MULTIQC' {\n        container = \"quay.io/biocontainers/multiqc:1.14--pyhdfd78af_0\"\n    }\n}\nRun the pipeline\nnextflow run nf-core/rnaseq -r 3.11.1 \\\n  -profile singularity \\\n  -c my_resources.config \\\n  -params-file workshop-params.yaml \\\n  --outdir lesson2.1.8 \\\n  -resume \n\n\n\n\n\n\n\n\n\nKey points\n\n\n\n\nnf-core workflows work ‘out of the box’ but there are compute and software configurations we should customise for our runs to work well on our compute infrastructure\nnf-core executes by default with workflow/nextflow.config and workflow/conf/base.config and has a repository of community-contributed institutional config that ship with the workflow\ncustom config can be applied to a run with -c &lt;config_name&gt;, and will over-ride settings in the default config\ncustomisations can be targeted to specific processes using withLabel or withName\nworkflow parameters belong in -params-file &lt;params_file&gt; and not -c &lt;custom_config&gt;\n\n\n\n\nThese materials are adapted from Customising Nf-Core Workshop by Sydney Informatics Hub"
  },
  {
    "objectID": "sessions/1_intro_run_nf.html",
    "href": "sessions/1_intro_run_nf.html",
    "title": "Introduction to Nextflow and running nf-core workflows",
    "section": "",
    "text": "This workshop is designed to provide participants with a foundational understanding of Nextflow and nf-core pipelines, with a focus on running existing pipelines efficiently. Participants are expected to have prior experience with the command-line interface and working with cluster systems like Slurm. The primary goal of the workshop is to equip researchers with the skills needed to use nf-core pipelines for their research data.\n\nCourse Presenters\n\nSong Li, Bioinformatics Core Facility, Peter Mac\nRichard Lupat, Bioinformatics Core Facility, Peter Mac\n\n\n\nCourse Helpers\n\nTBC\n\n\n\nPrerequisites\n\nExperience with command line interface and cluster/slurm\nFamiliarity with the basic concept of workflows\nAccess to a slurm cluster\n\n\n\nLearning Objectives:\nBy the end of this workshop, participants should be able to:\n\nGain exposure to key concepts and terminology in Nextflow and nf-core pipelines.\nUnderstand the foundational knowledge required to navigate and customize the code base of nf-core pipelines.\nDevelop basic troubleshooting and customization skills necessary for responsibly applying nf-core pipelines to your own research data.\n\n\n\nSet up requirements\nPlease complete the Setup Instructions before the course.\nIf you have any trouble, please get in contact with us ASAP via Slack\n\n\nWorkshop schedule\n\n\n\nLesson\nOverview\nTime\n\n\n\n\nSetup\nFollow these instructions to install VS Code and setup your workspace\nPrior to workshop\n\n\nSession kick off\nSession kick off: Discuss learning outcomes and finalising workspace setup\n10:00 - 10.10\n\n\nIntroduction to Nextflow\nIntroduction to Nextflow: Introduce nextflow’s core features and concepts; including CLI and how to run it on cluster\n10:10 - 10:20\n\n\nBasic to Create a Nextflow Workflow\nIntroduction to nextflow channels, processes, data types and workflows\n10:20 - 10.50\n\n\nBreak\nBreak\n10:50 - 11:00\n\n\nIntroduction to nf-core\nIntroduction to nf-core: Introduce nf-core features and concepts, structures, tools, and example nf-core pipelines\n11:00 - 11:30\n\n\nCustomising & running nf-core pipelines\nCustomising & running nf-core pipelines: Discuss pipelines’ required inputs, optional inputs, outputs, parameters file and configurations files\n11:30 - 12:15\n\n\nLunch Break\nBreak\n12:15 - 13:00\n\n\nTroubleshooting nextflow run\nTroubleshooting nextflow run: Discuss Nextflow logging, caching, task execution directory, dependencies, and manual troubleshooting\n13:00 - 13:30\n\n\nBest practise and Q&A\nBest practise, tips & tricks for running nextflow pipelines\n13.30 - 14:00\n\n\n\n\n\nCredits and acknowledgement\nThis workshop is adapted from Customising Nf-Core Workshop materials from Sydney Informatics Hub"
  }
]